<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 56]
- [cs.CL](#cs.CL) [Total: 28]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.AI](#cs.AI) [Total: 25]
- [cs.LG](#cs.LG) [Total: 48]
- [cs.HC](#cs.HC) [Total: 5]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [NeRV360: Neural Representation for 360-Degree Videos with a Viewport Decoder](https://arxiv.org/abs/2512.20871)
*Daichi Arai,Kyohei Unno,Yasuko Sugito,Yuichi Kusakabe*

Main category: cs.CV

TL;DR: NeRV360针对高分辨率360度视频的压缩问题，提出只解码用户选定视口的方案，相比现有方法大幅降低内存消耗并提升解码速度。


<details>
  <summary>Details</summary>
Motivation: 现有基于隐式神经表示的视频压缩方法在处理高分辨率360度视频时面临内存占用高和解码速度慢的问题，难以实现实时应用。

Method: NeRV360采用端到端框架，将视口提取集成到解码过程中，引入时空仿射变换模块根据视角和时间进行条件解码。

Result: 在6K分辨率视频上的实验表明，相比先前的HNeRV方法，NeRV360内存消耗降低7倍，解码速度提升2.5倍，同时获得了更好的图像质量客观指标。

Conclusion: NeRV360为解决360度视频的高效压缩和解码提供了有效方案，通过只解码用户选定视口的方式克服了传统方法的内存和速度瓶颈。

Abstract: Implicit neural representations for videos (NeRV) have shown strong potential for video compression. However, applying NeRV to high-resolution 360-degree videos causes high memory usage and slow decoding, making real-time applications impractical. We propose NeRV360, an end-to-end framework that decodes only the user-selected viewport instead of reconstructing the entire panoramic frame. Unlike conventional pipelines, NeRV360 integrates viewport extraction into decoding and introduces a spatial-temporal affine transform module for conditional decoding based on viewpoint and time. Experiments on 6K-resolution videos show that NeRV360 achieves a 7-fold reduction in memory consumption and a 2.5-fold increase in decoding speed compared to HNeRV, a representative prior work, while delivering better image quality in terms of objective metrics.

</details>


### [2] [VL4Gaze: Unleashing Vision-Language Models for Gaze Following](https://arxiv.org/abs/2512.20735)
*Shijing Wang,Chaoqun Cui,Yaping Huang,Hyung Jin Chang,Yihua Cheng*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Human gaze provides essential cues for interpreting attention, intention, and social interaction in visual scenes, yet gaze understanding remains largely unexplored in current vision-language models (VLMs). While recent VLMs achieve strong scene-level reasoning across a range of visual tasks, there exists no benchmark that systematically evaluates or trains them for gaze interpretation, leaving open the question of whether gaze understanding can emerge from general-purpose vision-language pre-training. To address this gap, we introduce VL4Gaze, the first large-scale benchmark designed to investigate, evaluate, and unlock the potential of VLMs for gaze understanding. VL4Gaze contains 489K automatically generated question-answer pairs across 124K images and formulates gaze understanding as a unified VQA problem through four complementary tasks: (1) gaze object description, (2) gaze direction description, (3) gaze point location, and (4) ambiguous question recognition. We comprehensively evaluate both commercial and open-source VLMs under in-context learning and fine-tuning settings. The results show that even large-scale VLMs struggle to reliably infer gaze semantics and spatial localization without task-specific supervision. In contrast, training on VL4Gaze brings substantial and consistent improvements across all tasks, highlighting the importance of targeted multi-task supervision for developing gaze understanding capabilities in VLMs. We will release the dataset and code to support further research and development in this direction.

</details>


### [3] [Transductive Visual Programming: Evolving Tool Libraries from Experience for Spatial Reasoning](https://arxiv.org/abs/2512.20934)
*Shengguang Wu,Xiaohan Wang,Yuhui Zhang,Hao Zhu,Serena Yeung-Levy*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Spatial reasoning in 3D scenes requires precise geometric calculations that challenge vision-language models. Visual programming addresses this by decomposing problems into steps calling specialized tools, yet existing methods rely on either fixed toolsets or speculative tool induction before solving problems, resulting in suboptimal programs and poor utilization of induced tools. We present Transductive Visual Programming (TVP), a novel framework that builds new tools from its own experience rather than speculation. TVP first solves problems using basic tools while accumulating experiential solutions into an Example Library, then abstracts recurring patterns from these programs into reusable higher-level tools for an evolving Tool Library. This allows TVP to tackle new problems with increasingly powerful tools learned from experience. On Omni3D-Bench, TVP achieves state-of-the-art performance, outperforming GPT-4o by 22% and the previous best visual programming system by 11%. Our transductively learned tools are used 5x more frequently as core program dependency than inductively created ones, demonstrating more effective tool discovery and reuse. The evolved tools also show strong generalization to unseen spatial tasks, achieving superior performance on benchmarks from SpatialScore-Hard collection without any testset-specific modification. Our work establishes experience-driven transductive tool creation as a powerful paradigm for building self-evolving visual programming agents that effectively tackle challenging spatial reasoning tasks. We release our code at https://transductive-visualprogram.github.io/.

</details>


### [4] [OccuFly: A 3D Vision Benchmark for Semantic Scene Completion from the Aerial Perspective](https://arxiv.org/abs/2512.20770)
*Markus Gross,Sai B. Matha,Aya Fahmy,Rui Song,Daniel Cremers,Henri Meess*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Semantic Scene Completion (SSC) is crucial for 3D perception in mobile robotics, as it enables holistic scene understanding by jointly estimating dense volumetric occupancy and per-voxel semantics. Although SSC has been widely studied in terrestrial domains such as autonomous driving, aerial scenarios like autonomous flying remain largely unexplored, thereby limiting progress on downstream applications. Furthermore, LiDAR sensors represent the primary modality for SSC data generation, which poses challenges for most uncrewed aerial vehicles (UAVs) due to flight regulations, mass and energy constraints, and the sparsity of LiDAR-based point clouds from elevated viewpoints. To address these limitations, we introduce OccuFly, the first real-world, camera-based aerial SSC benchmark, captured at altitudes of 50m, 40m, and 30m during spring, summer, fall, and winter. OccuFly covers urban, industrial, and rural scenarios, provides 22 semantic classes, and the data format adheres to established conventions to facilitate seamless integration with existing research. Crucially, we propose a LiDAR-free data generation framework based on camera modality, which is ubiquitous on modern UAVs. By utilizing traditional 3D reconstruction, our framework automates label transfer by lifting a subset of annotated 2D masks into the reconstructed point cloud, thereby substantially minimizing manual 3D annotation effort. Finally, we benchmark the state-of-the-art on OccuFly and highlight challenges specific to elevated viewpoints, yielding a comprehensive vision benchmark for holistic aerial 3D scene understanding.

</details>


### [5] [NULLBUS: Multimodal Mixed-Supervision for Breast Ultrasound Segmentation via Nullable Global-Local Prompts](https://arxiv.org/abs/2512.20783)
*Raja Mallina,Bryar Shareef*

Main category: cs.CV

TL;DR: 该研究提出了NullBUS框架，用于超声图像分割，能够同时处理有无文本提示的图像，提升在缺乏可靠元数据情况下的分割性能


<details>
  <summary>Details</summary>
Motivation: 传统可提示方法依赖文本或空间提示来提升超声分割性能，但许多公共乳腺超声数据集缺乏可靠元数据或报告，限制模型训练只能在小规模多模态子集上进行，降低了模型的鲁棒性。

Method: 提出NullBUS多模态混合监督框架，在单个模型中同时学习有无提示的图像。引入可空提示（nullable prompts），使用可学习的空嵌入和存在掩码实现，当元数据缺失时回退到仅图像证据，当文本存在时则利用文本信息。

Result: 在包含3个公共BUS数据集的统一池上评估，NullBUS达到平均IoU 0.8568和平均Dice 0.9103，在混合提示可用性下展现出最优性能。

Conclusion: NullBUS框架能够在有无文本提示的情况下都实现有效的超声图像分割，有效解决了公共数据集中元数据缺失的问题，为计算机辅助诊断和治疗规划提供了更鲁棒的解决方案。

Abstract: Breast ultrasound (BUS) segmentation provides lesion boundaries essential for computer-aided diagnosis and treatment planning. While promptable methods can improve segmentation performance and tumor delineation when text or spatial prompts are available, many public BUS datasets lack reliable metadata or reports, constraining training to small multimodal subsets and reducing robustness. We propose NullBUS, a multimodal mixed-supervision framework that learns from images with and without prompts in a single model. To handle missing text, we introduce nullable prompts, implemented as learnable null embeddings with presence masks, enabling fallback to image-only evidence when metadata are absent and the use of text when present. Evaluated on a unified pool of three public BUS datasets, NullBUS achieves a mean IoU of 0.8568 and a mean Dice of 0.9103, demonstrating state-of-the-art performance under mixed prompt availability.

</details>


### [6] [CHAMMI-75: pre-training multi-channel models with heterogeneous microscopy images](https://arxiv.org/abs/2512.20833)
*Vidit Agrawal,John Peters,Tyler N. Thompson,Mohammad Vali Sanian,Chau Pham,Nikita Moshkov,Arshad Kazi,Aditya Pillai,Jack Freeman,Byunguk Kang,Samouil L. Farhi,Ernest Fraenkel,Ron Stewart,Lassi Paavolainen,Bryan A. Plummer,Juan C. Caicedo*

Main category: cs.CV

TL;DR: CHAMMI-75数据集为75个不同生物学研究的多通道显微图像，旨在训练能适应不同通道和模态的通用细胞形态学模型。


<details>
  <summary>Details</summary>
Motivation: 现有的细胞形态量化模型通常基于单一显微成像类型训练，导致模型无法跨研究重复使用，限制了其应用范围。

Method: 从公开来源收集并整理75个不同生物学研究的异质性多通道显微图像创建CHAMMI-75数据集，用于训练通道自适应的细胞形态学模型。

Result: 实验表明，使用CHAMMI-75数据集训练能提高在多通道生物成像任务中的性能，主要得益于其显微模态的高度多样性。

Conclusion: 该工作为创建下一代适用于生物学研究的通用细胞形态学模型铺平了道路，解决了现有模型难以跨研究复用的局限性。

Abstract: Quantifying cell morphology using images and machine learning has proven to be a powerful tool to study the response of cells to treatments. However, models used to quantify cellular morphology are typically trained with a single microscopy imaging type. This results in specialized models that cannot be reused across biological studies because the technical specifications do not match (e.g., different number of channels), or because the target experimental conditions are out of distribution. Here, we present CHAMMI-75, an open access dataset of heterogeneous, multi-channel microscopy images from 75 diverse biological studies. We curated this resource from publicly available sources to investigate cellular morphology models that are channel-adaptive and can process any microscopy image type. Our experiments show that training with CHAMMI-75 can improve performance in multi-channel bioimaging tasks primarily because of its high diversity in microscopy modalities. This work paves the way to create the next generation of cellular morphology models for biological studies.

</details>


### [7] [Input-Adaptive Visual Preprocessing for Efficient Fast Vision-Language Model Inference](https://arxiv.org/abs/2512.20839)
*Putu Indah Githa Cahyani,Komang David Dananjaya Suartana,Novanto Yudistira*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Vision-Language Models (VLMs) have demonstrated strong performance on multimodal reasoning tasks, but their deployment remains challenging due to high inference latency and computational cost, particularly when processing high-resolution visual inputs. While recent architectures such as FastVLM improve efficiency through optimized vision encoders, existing pipelines still rely on static visual preprocessing, leading to redundant computation for visually simple inputs. In this work, we propose an adaptive visual preprocessing method that dynamically adjusts input resolution and spatial coverage based on image content characteristics. The proposed approach combines content-aware image analysis, adaptive resolution selection, and content-aware cropping to reduce visual redundancy prior to vision encoding. Importantly, the method is integrated with FastVLM without modifying its architecture or requiring retraining. We evaluate the proposed method on a subset of the DocVQA dataset in an inference-only setting, focusing on efficiency-oriented metrics. Experimental results show that adaptive preprocessing reduces per-image inference time by over 50\%, lowers mean full generation time, and achieves a consistent reduction of more than 55\% in visual token count compared to the baseline pipeline. These findings demonstrate that input-aware preprocessing is an effective and lightweight strategy for improving deployment-oriented efficiency of vision-language models. To facilitate reproducibility, our implementation is provided as a fork of the FastVLM repository, incorporating the files for the proposed method, and is available at https://github.com/kmdavidds/mlfastlm.

</details>


### [8] [ALIVE: An Avatar-Lecture Interactive Video Engine with Content-Aware Retrieval for Real-Time Interaction](https://arxiv.org/abs/2512.20858)
*Md Zabirul Islam,Md Motaleb Hossen Manik,Ge Wang*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Traditional lecture videos offer flexibility but lack mechanisms for real-time clarification, forcing learners to search externally when confusion arises. Recent advances in large language models and neural avatars provide new opportunities for interactive learning, yet existing systems typically lack lecture awareness, rely on cloud-based services, or fail to integrate retrieval and avatar-delivered explanations in a unified, privacy-preserving pipeline.
  We present ALIVE, an Avatar-Lecture Interactive Video Engine that transforms passive lecture viewing into a dynamic, real-time learning experience. ALIVE operates fully on local hardware and integrates (1) Avatar-delivered lecture generated through ASR transcription, LLM refinement, and neural talking-head synthesis; (2) A content-aware retrieval mechanism that combines semantic similarity with timestamp alignment to surface contextually relevant lecture segments; and (3) Real-time multimodal interaction, enabling students to pause the lecture, ask questions through text or voice, and receive grounded explanations either as text or as avatar-delivered responses.
  To maintain responsiveness, ALIVE employs lightweight embedding models, FAISS-based retrieval, and segmented avatar synthesis with progressive preloading. We demonstrate the system on a complete medical imaging course, evaluate its retrieval accuracy, latency characteristics, and user experience, and show that ALIVE provides accurate, content-aware, and engaging real-time support.
  ALIVE illustrates how multimodal AI-when combined with content-aware retrieval and local deployment-can significantly enhance the pedagogical value of recorded lectures, offering an extensible pathway toward next-generation interactive learning environments.

</details>


### [9] [DexAvatar: 3D Sign Language Reconstruction with Hand and Body Pose Priors](https://arxiv.org/abs/2512.21054)
*Kaustubh Kundu,Hrishav Bakul Barua,Lucy Robertson-Bell,Zhixi Cai,Kalin Stefanov*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The trend in sign language generation is centered around data-driven generative methods that require vast amounts of precise 2D and 3D human pose data to achieve an acceptable generation quality. However, currently, most sign language datasets are video-based and limited to automatically reconstructed 2D human poses (i.e., keypoints) and lack accurate 3D information. Furthermore, existing state-of-the-art for automatic 3D human pose estimation from sign language videos is prone to self-occlusion, noise, and motion blur effects, resulting in poor reconstruction quality. In response to this, we introduce DexAvatar, a novel framework to reconstruct bio-mechanically accurate fine-grained hand articulations and body movements from in-the-wild monocular sign language videos, guided by learned 3D hand and body priors. DexAvatar achieves strong performance in the SGNify motion capture dataset, the only benchmark available for this task, reaching an improvement of 35.11% in the estimation of body and hand poses compared to the state-of-the-art. The official website of this work is: https://github.com/kaustesseract/DexAvatar.

</details>


### [10] [Lightweight framework for underground pipeline recognition and spatial localization based on multi-view 2D GPR images](https://arxiv.org/abs/2512.20866)
*Haotian Lv,Chao Li,Jiangbo Dai,Yuhui Zhang,Zepeng Fan,Yiqiu Tan,Dawei Wang,Binglei Xie*

Main category: cs.CV

TL;DR: 为解决3D探地雷达地下管线检测中的多视图特征相关性弱、小目标识别精度低、复杂场景下鲁棒性不足等问题，本文提出了一种3D管线智能检测框架。


<details>
  <summary>Details</summary>
Motivation: 3D探地雷达地下管线检测存在多视图特征相关性弱、小尺寸目标识别精度低、复杂场景下鲁棒性不足等问题，需要一种更有效的检测方法。

Method: 提出3D管线智能检测框架：1)基于B/C/D-Scan三视图联合分析策略，通过FDTD方法的前向仿真结果与实际测量数据交叉验证，建立三维管线三视图特征评估方法；2)提出DCO-YOLO框架，集成了DySample、CGLU和OutlookAttention跨维度相关机制到YOLOv11算法中；3)提出3D-DIoU空间特征匹配算法，集成三维几何约束和中心距离惩罚项。

Result: 基于城市地下管线真实数据的实验表明，在复杂多管线场景下，该方法准确率、召回率和平均精度分别为96.2%、93.3%和96.7%，比基线模型分别提高了2.0%、2.1%和0.9%。消融实验验证了动态特征增强模块的协同优化效果。

Conclusion: 本研究将深度学习优化策略与3D探地雷达物理特性相结合，为地下管线的智能识别与定位提供了一种高效可靠的新型技术框架。

Abstract: To address the issues of weak correlation between multi-view features, low recognition accuracy of small-scale targets, and insufficient robustness in complex scenarios in underground pipeline detection using 3D GPR, this paper proposes a 3D pipeline intelligent detection framework. First, based on a B/C/D-Scan three-view joint analysis strategy, a three-dimensional pipeline three-view feature evaluation method is established by cross-validating forward simulation results obtained using FDTD methods with actual measurement data. Second, the DCO-YOLO framework is proposed, which integrates DySample, CGLU, and OutlookAttention cross-dimensional correlation mechanisms into the original YOLOv11 algorithm, significantly improving the small-scale pipeline edge feature extraction capability. Furthermore, a 3D-DIoU spatial feature matching algorithm is proposed, which integrates three-dimensional geometric constraints and center distance penalty terms to achieve automated association of multi-view annotations. The three-view fusion strategy resolves inherent ambiguities in single-view detection. Experiments based on real urban underground pipeline data show that the proposed method achieves accuracy, recall, and mean average precision of 96.2%, 93.3%, and 96.7%, respectively, in complex multi-pipeline scenarios, which are 2.0%, 2.1%, and 0.9% higher than the baseline model. Ablation experiments validated the synergistic optimization effect of the dynamic feature enhancement module and Grad-CAM++ heatmap visualization demonstrated that the improved model significantly enhanced its ability to focus on pipeline geometric features. This study integrates deep learning optimization strategies with the physical characteristics of 3D GPR, offering an efficient and reliable novel technical framework for the intelligent recognition and localization of underground pipelines.

</details>


### [11] [Beyond Weight Adaptation: Feature-Space Domain Injection for Cross-Modal Ship Re-Identification](https://arxiv.org/abs/2512.20892)
*Tingfeng Xian,Wenlve Zhou,Zhiheng Zhou,Zhelin Li*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Cross-Modality Ship Re-Identification (CMS Re-ID) is critical for achieving all-day and all-weather maritime target tracking, yet it is fundamentally challenged by significant modality discrepancies. Mainstream solutions typically rely on explicit modality alignment strategies; however, this paradigm heavily depends on constructing large-scale paired datasets for pre-training. To address this, grounded in the Platonic Representation Hypothesis, we explore the potential of Vision Foundation Models (VFMs) in bridging modality gaps. Recognizing the suboptimal performance of existing generic Parameter-Efficient Fine-Tuning (PEFT) methods that operate within the weight space, particularly on limited-capacity models, we shift the optimization perspective to the feature space and propose a novel PEFT strategy termed Domain Representation Injection (DRI). Specifically, while keeping the VFM fully frozen to maximize the preservation of general knowledge, we design a lightweight, learnable Offset Encoder to extract domain-specific representations rich in modality and identity attributes from raw inputs. Guided by the contextual information of intermediate features at different layers, a Modulator adaptively transforms these representations. Subsequently, they are injected into the intermediate layers via additive fusion, dynamically reshaping the feature distribution to adapt to the downstream task without altering the VFM's pre-trained weights. Extensive experimental results demonstrate the superiority of our method, achieving State-of-the-Art (SOTA) performance with minimal trainable parameters. For instance, on the HOSS-ReID dataset, we attain 57.9\% and 60.5\% mAP using only 1.54M and 7.05M parameters, respectively. The code is available at https://github.com/TingfengXian/DRI.

</details>


### [12] [DGSAN: Dual-Graph Spatiotemporal Attention Network for Pulmonary Nodule Malignancy Prediction](https://arxiv.org/abs/2512.20898)
*Xiao Yu,Zhaojie Fang,Guanyu Zhou,Yin Shen,Huoling Luo,Ye Li,Ahmed Elazab,Xiang Wan,Ruiquan Ge,Changmiao Wang*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Lung cancer continues to be the leading cause of cancer-related deaths globally. Early detection and diagnosis of pulmonary nodules are essential for improving patient survival rates. Although previous research has integrated multimodal and multi-temporal information, outperforming single modality and single time point, the fusion methods are limited to inefficient vector concatenation and simple mutual attention, highlighting the need for more effective multimodal information fusion. To address these challenges, we introduce a Dual-Graph Spatiotemporal Attention Network, which leverages temporal variations and multimodal data to enhance the accuracy of predictions. Our methodology involves developing a Global-Local Feature Encoder to better capture the local, global, and fused characteristics of pulmonary nodules. Additionally, a Dual-Graph Construction method organizes multimodal features into inter-modal and intra-modal graphs. Furthermore, a Hierarchical Cross-Modal Graph Fusion Module is introduced to refine feature integration. We also compiled a novel multimodal dataset named the NLST-cmst dataset as a comprehensive source of support for related research. Our extensive experiments, conducted on both the NLST-cmst and curated CSTL-derived datasets, demonstrate that our DGSAN significantly outperforms state-of-the-art methods in classifying pulmonary nodules with exceptional computational efficiency.

</details>


### [13] [Benchmarking and Enhancing VLM for Compressed Image Understanding](https://arxiv.org/abs/2512.20901)
*Zifu Zhang,Tongda Xu,Siqi Li,Shengxi Li,Yue Zhang,Mai Xu,Yan Wang*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: With the rapid development of Vision-Language Models (VLMs) and the growing demand for their applications, efficient compression of the image inputs has become increasingly important. Existing VLMs predominantly digest and understand high-bitrate compressed images, while their ability to interpret low-bitrate compressed images has yet to be explored by far. In this paper, we introduce the first comprehensive benchmark to evaluate the ability of VLM against compressed images, varying existing widely used image codecs and diverse set of tasks, encompassing over one million compressed images in our benchmark. Next, we analyse the source of performance gap, by categorising the gap from a) the information loss during compression and b) generalisation failure of VLM. We visualize these gaps with concrete examples and identify that for compressed images, only the generalization gap can be mitigated. Finally, we propose a universal VLM adaptor to enhance model performance on images compressed by existing codecs. Consequently, we demonstrate that a single adaptor can improve VLM performance across images with varying codecs and bitrates by 10%-30%. We believe that our benchmark and enhancement method provide valuable insights and contribute toward bridging the gap between VLMs and compressed images.

</details>


### [14] [PanoGrounder: Bridging 2D and 3D with Panoramic Scene Representations for VLM-based 3D Visual Grounding](https://arxiv.org/abs/2512.20907)
*Seongmin Jung,Seongho Choi,Gunwoo Jeon,Minsu Cho,Jongwoo Lim*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: 3D Visual Grounding (3DVG) is a critical bridge from vision-language perception to robotics, requiring both language understanding and 3D scene reasoning. Traditional supervised models leverage explicit 3D geometry but exhibit limited generalization, owing to the scarcity of 3D vision-language datasets and the limited reasoning capabilities compared to modern vision-language models (VLMs). We propose PanoGrounder, a generalizable 3DVG framework that couples multi-modal panoramic representation with pretrained 2D VLMs for strong vision-language reasoning. Panoramic renderings, augmented with 3D semantic and geometric features, serve as an intermediate representation between 2D and 3D, and offer two major benefits: (i) they can be directly fed to VLMs with minimal adaptation and (ii) they retain long-range object-to-object relations thanks to their 360-degree field of view. We devise a three-stage pipeline that places a compact set of panoramic viewpoints considering the scene layout and geometry, grounds a text query on each panoramic rendering with a VLM, and fuses per-view predictions into a single 3D bounding box via lifting. Our approach achieves state-of-the-art results on ScanRefer and Nr3D, and demonstrates superior generalization to unseen 3D datasets and text rephrasings.

</details>


### [15] [Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting](https://arxiv.org/abs/2512.20927)
*Yoonwoo Jeong,Cheng Sun,Frank Wang,Minsu Cho,Jaesung Choe*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent advancements in computer vision have successfully extended Open-vocabulary segmentation (OVS) to the 3D domain by leveraging 3D Gaussian Splatting (3D-GS). Despite this progress, efficiently rendering the high-dimensional features required for open-vocabulary queries poses a significant challenge. Existing methods employ codebooks or feature compression, causing information loss, thereby degrading segmentation quality. To address this limitation, we introduce Quantile Rendering (Q-Render), a novel rendering strategy for 3D Gaussians that efficiently handles high-dimensional features while maintaining high fidelity. Unlike conventional volume rendering, which densely samples all 3D Gaussians intersecting each ray, Q-Render sparsely samples only those with dominant influence along the ray. By integrating Q-Render into a generalizable 3D neural network, we also propose Gaussian Splatting Network (GS-Net), which predicts Gaussian features in a generalizable manner. Extensive experiments on ScanNet and LeRF demonstrate that our framework outperforms state-of-the-art methods, while enabling real-time rendering with an approximate ~43.7x speedup on 512-D feature maps. Code will be made publicly available.

</details>


### [16] [Reasoning-Driven Amodal Completion: Collaborative Agents and Perceptual Evaluation](https://arxiv.org/abs/2512.20936)
*Hongxing Fan,Shuyu Zhao,Jiayang Ao,Lu Sheng*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Amodal completion, the task of inferring invisible object parts, faces significant challenges in maintaining semantic consistency and structural integrity. Prior progressive approaches are inherently limited by inference instability and error accumulation. To tackle these limitations, we present a Collaborative Multi-Agent Reasoning Framework that explicitly decouples Semantic Planning from Visual Synthesis. By employing specialized agents for upfront reasoning, our method generates a structured, explicit plan before pixel generation, enabling visually and semantically coherent single-pass synthesis. We integrate this framework with two critical mechanisms: (1) a self-correcting Verification Agent that employs Chain-of-Thought reasoning to rectify visible region segmentation and identify residual occluders strictly within the Semantic Planning phase, and (2) a Diverse Hypothesis Generator that addresses the ambiguity of invisible regions by offering diverse, plausible semantic interpretations, surpassing the limited pixel-level variations of standard random seed sampling. Furthermore, addressing the limitations of traditional metrics in assessing inferred invisible content, we introduce the MAC-Score (MLLM Amodal Completion Score), a novel human-aligned evaluation metric. Validated against human judgment and ground truth, these metrics establish a robust standard for assessing structural completeness and semantic consistency with visible context. Extensive experiments demonstrate that our framework significantly outperforms state-of-the-art methods across multiple datasets. Our project is available at: https://fanhongxing.github.io/remac-page.

</details>


### [17] [Beyond Artifacts: Real-Centric Envelope Modeling for Reliable AI-Generated Image Detection](https://arxiv.org/abs/2512.20937)
*Ruiqi Liu,Yi Han,Zhengbo Zhang,Liwei Yao,Zhiyuan Yan,Jialiang Shen,ZhiJin Chen,Boyi Sun,Lubin Weng,Jing Dong,Yan Wang,Shu Wu*

Main category: cs.CV

TL;DR: 这篇论文提出了REM方法，将合成图像检测的焦点从学习生成器伪影转移到建模真实图像的鲁棒分布，通过模拟真实世界退化情况下的检测挑战。


<details>
  <summary>Details</summary>
Motivation: 现有检测器往往过度拟合特定生成器的伪影特征，对真实世界图像退化（如跨平台分享、后处理等连锁退化）高度敏感。随着生成架构演进和多轮图像处理，传统伪影线索变得过时且难以检测。

Method: 1. 提出真实中心包络建模（REM），在自重建中引入特征级扰动以生成近真实样本；2. 使用带跨域一致性的包络估计器学习包围真实图像流形的边界；3. 构建RealChain基准，覆盖开源和商业生成器并模拟真实世界退化。

Result: 在8个基准评估中，REM比最先进方法平均提升7.5%；在严重退化的RealChain基准上保持优异的泛化能力，为真实世界条件下的合成图像检测打下坚实基础。

Conclusion: REM通过建模真实图像分布而非特定生成器伪影，显著提升了合成图像检测在真实世界条件下的鲁棒性和泛化能力，解决了现有方法对图像退化敏感的问题。

Abstract: The rapid progress of generative models has intensified the need for reliable and robust detection under real-world conditions. However, existing detectors often overfit to generator-specific artifacts and remain highly sensitive to real-world degradations. As generative architectures evolve and images undergo multi-round cross-platform sharing and post-processing (chain degradations), these artifact cues become obsolete and harder to detect. To address this, we propose Real-centric Envelope Modeling (REM), a new paradigm that shifts detection from learning generator artifacts to modeling the robust distribution of real images. REM introduces feature-level perturbations in self-reconstruction to generate near-real samples, and employs an envelope estimator with cross-domain consistency to learn a boundary enclosing the real image manifold. We further build RealChain, a comprehensive benchmark covering both open-source and commercial generators with simulated real-world degradation. Across eight benchmark evaluations, REM achieves an average improvement of 7.5% over state-of-the-art methods, and notably maintains exceptional generalization on the severely degraded RealChain benchmark, establishing a solid foundation for synthetic image detection under real-world conditions. The code and the RealChain benchmark will be made publicly available upon acceptance of the paper.

</details>


### [18] [SPOT!: Map-Guided LLM Agent for Unsupervised Multi-CCTV Dynamic Object Tracking](https://arxiv.org/abs/2512.20975)
*Yujin Noh,Inho Jake Park,Chigon Hwang*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: CCTV-based vehicle tracking systems face structural limitations in continuously connecting the trajectories of the same vehicle across multiple camera environments. In particular, blind spots occur due to the intervals between CCTVs and limited Fields of View (FOV), which leads to object ID switching and trajectory loss, thereby reducing the reliability of real-time path prediction. This paper proposes SPOT (Spatial Prediction Over Trajectories), a map-guided LLM agent capable of tracking vehicles even in blind spots of multi-CCTV environments without prior training. The proposed method represents road structures (Waypoints) and CCTV placement information as documents based on 2D spatial coordinates and organizes them through chunking techniques to enable real-time querying and inference. Furthermore, it transforms the vehicle's position into the actual world coordinate system using the relative position and FOV information of objects observed in CCTV images. By combining map spatial information with the vehicle's moving direction, speed, and driving patterns, a beam search is performed at the intersection level to derive candidate CCTV locations where the vehicle is most likely to enter after the blind spot. Experimental results based on the CARLA simulator in a virtual city environment confirmed that the proposed method accurately predicts the next appearing CCTV even in blind spot sections, maintaining continuous vehicle trajectories more effectively than existing techniques.

</details>


### [19] [XGrid-Mapping: Explicit Implicit Hybrid Grid Submaps for Efficient Incremental Neural LiDAR Mapping](https://arxiv.org/abs/2512.20976)
*Zeqing Song,Zhongmiao Yan,Junyuan Deng,Songpengcheng Xia,Xiang Mu,Jingyi Xu,Qi Wu,Ling Pei*

Main category: cs.CV

TL;DR: 该论文提出了XGrid-Mapping，一种用于高效神经激光雷达建图的混合网格框架


<details>
  <summary>Details</summary>
Motivation: 大规模增量建图对于自主系统至关重要，但现有神经激光雷达建图方法大多依赖密集隐式表示，而体素引导方法难以实现实时性能

Method: 结合稀疏网格（提供几何先验和结构指导）与隐式密集网格的混合框架，使用VDB结构与基于子图的组织，引入基于蒸馏的重叠对齐策略和动态移除模块

Result: 在大规模实验中显示出优越的建图质量，克服了体素引导方法的效率限制，性能优于现有最先进方法

Conclusion: XGrid-Mapping通过有效结合显式和隐式表示，实现了大规模增量神经激光雷达建图，在质量和效率上都超越了现有方法

Abstract: Large-scale incremental mapping is fundamental to the development of robust and reliable autonomous systems, as it underpins incremental environmental understanding with sequential inputs for navigation and decision-making. LiDAR is widely used for this purpose due to its accuracy and robustness. Recently, neural LiDAR mapping has shown impressive performance; however, most approaches rely on dense implicit representations and underutilize geometric structure, while existing voxel-guided methods struggle to achieve real-time performance. To address these challenges, we propose XGrid-Mapping, a hybrid grid framework that jointly exploits explicit and implicit representations for efficient neural LiDAR mapping. Specifically, the strategy combines a sparse grid, providing geometric priors and structural guidance, with an implicit dense grid that enriches scene representation. By coupling the VDB structure with a submap-based organization, the framework reduces computational load and enables efficient incremental mapping on a large scale. To mitigate discontinuities across submaps, we introduce a distillation-based overlap alignment strategy, in which preceding submaps supervise subsequent ones to ensure consistency in overlapping regions. To further enhance robustness and sampling efficiency, we incorporate a dynamic removal module. Extensive experiments show that our approach delivers superior mapping quality while overcoming the efficiency limitations of voxel-guided methods, thereby outperforming existing state-of-the-art mapping methods.

</details>


### [20] [X-ray Insights Unleashed: Pioneering the Enhancement of Multi-Label Long-Tail Data](https://arxiv.org/abs/2512.20980)
*Xinquan Yang,Jinheng Xie,Yawen Huang,Yuexiang Li,Huimin Huang,Hao Zheng,Xian Wu,Yefeng Zheng,Linlin Shen*

Main category: cs.CV

TL;DR: 提出一个新的数据合成框架解决胸部X光图像中长尾病变检测问题，通过利用大量正常X光片训练扩散模型，然后通过修复技术生成尾部病变数据，并结合LLM知识引导和渐进式增量学习策略提升效果


<details>
  <summary>Details</summary>
Motivation: 长尾肺部异常在胸部X光中诊断困难，现有基于扩散的方法由于罕见病变样本稀缺，生成能力有限，导致诊断精度不理想

Method: 1. 收集大量正常X光片训练扩散模型生成正常图像；2. 利用预训练扩散模型修复病变X光片中的头部病变，将尾部类别作为增强训练数据；3. 引入大语言模型知识引导模块和渐进式增量学习策略以稳定修复微调过程

Result: 在公开肺部数据集MIMIC和CheXpert上的综合评估表明，该方法实现了新的性能基准

Conclusion: 提出的数据合成框架有效解决了长尾肺部异常检测问题，通过结合扩散模型、LLM知识引导和渐进学习策略，显著提升了诊断精度

Abstract: Long-tailed pulmonary anomalies in chest radiography present formidable diagnostic challenges. Despite the recent strides in diffusion-based methods for enhancing the representation of tailed lesions, the paucity of rare lesion exemplars curtails the generative capabilities of these approaches, thereby leaving the diagnostic precision less than optimal. In this paper, we propose a novel data synthesis pipeline designed to augment tail lesions utilizing a copious supply of conventional normal X-rays. Specifically, a sufficient quantity of normal samples is amassed to train a diffusion model capable of generating normal X-ray images. This pre-trained diffusion model is subsequently utilized to inpaint the head lesions present in the diseased X-rays, thereby preserving the tail classes as augmented training data. Additionally, we propose the integration of a Large Language Model Knowledge Guidance (LKG) module alongside a Progressive Incremental Learning (PIL) strategy to stabilize the inpainting fine-tuning process. Comprehensive evaluations conducted on the public lung datasets MIMIC and CheXpert demonstrate that the proposed method sets a new benchmark in performance.

</details>


### [21] [MVInverse: Feed-forward Multi-view Inverse Rendering in Seconds](https://arxiv.org/abs/2512.21003)
*Xiangzuo Wu,Chengwei Ren,Jun Zhou,Xiu Li,Yuan Liu*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Multi-view inverse rendering aims to recover geometry, materials, and illumination consistently across multiple viewpoints. When applied to multi-view images, existing single-view approaches often ignore cross-view relationships, leading to inconsistent results. In contrast, multi-view optimization methods rely on slow differentiable rendering and per-scene refinement, making them computationally expensive and hard to scale. To address these limitations, we introduce a feed-forward multi-view inverse rendering framework that directly predicts spatially varying albedo, metallic, roughness, diffuse shading, and surface normals from sequences of RGB images. By alternating attention across views, our model captures both intra-view long-range lighting interactions and inter-view material consistency, enabling coherent scene-level reasoning within a single forward pass. Due to the scarcity of real-world training data, models trained on existing synthetic datasets often struggle to generalize to real-world scenes. To overcome this limitation, we propose a consistency-based finetuning strategy that leverages unlabeled real-world videos to enhance both multi-view coherence and robustness under in-the-wild conditions. Extensive experiments on benchmark datasets demonstrate that our method achieves state-of-the-art performance in terms of multi-view consistency, material and normal estimation quality, and generalization to real-world imagery.

</details>


### [22] [Learning from Next-Frame Prediction: Autoregressive Video Modeling Encodes Effective Representations](https://arxiv.org/abs/2512.21004)
*Jinghan Li,Yang Jin,Hao Jiang,Yadong Mu,Yang Song,Kun Xu*

Main category: cs.CV

TL;DR: NExT-Vid是一个新颖的自回归视觉生成预训练框架，通过掩码下一帧预测联合建模图像和视频，在视觉表示学习上优于现有生成预训练方法。


<details>
  <summary>Details</summary>
Motivation: 当前大多数视觉生成预训练仍依赖BERT式掩码建模，忽视了视频分析中关键的时序信息；而现有的自回归视觉预训练方法存在语义定位不准、生成质量差等问题。

Method: 提出NExT-Vid框架：1）使用掩码下一帧预测进行自回归预训练；2）引入上下文隔离的自回归预测器，将语义表示与目标解码解耦；3）采用条件流匹配解码器提升生成质量和多样性。

Result: 在大规模预训练模型上的广泛实验表明，该方法在下游分类任务的注意力探测中，持续超越以往的生成式预训练方法。

Conclusion: NExT-Vid通过上下文隔离的流匹配预训练获得了强大的视觉表示能力，为视觉生成预训练提供了有效的新范式。

Abstract: Recent advances in pretraining general foundation models have significantly improved performance across diverse downstream tasks. While autoregressive (AR) generative models like GPT have revolutionized NLP, most visual generative pretraining methods still rely on BERT-style masked modeling, which often disregards the temporal information essential for video analysis. The few existing autoregressive visual pretraining methods suffer from issues such as inaccurate semantic localization and poor generation quality, leading to poor semantics. In this work, we propose NExT-Vid, a novel autoregressive visual generative pretraining framework that utilizes masked next-frame prediction to jointly model images and videos. NExT-Vid introduces a context-isolated autoregressive predictor to decouple semantic representation from target decoding, and a conditioned flow-matching decoder to enhance generation quality and diversity. Through context-isolated flow-matching pretraining, our approach achieves strong representations. Extensive experiments on large-scale pretrained models demonstrate that our proposed method consistently outperforms previous generative pretraining methods for visual representation learning via attentive probing in downstream classification.

</details>


### [23] [Granular-ball Guided Masking: Structure-aware Data Augmentation](https://arxiv.org/abs/2512.21011)
*Shuyin Xia,Fan Chen,Dawei Dai,Meng Yang,Junwei Han,Xinbo Gao,Guoyin Wang*

Main category: cs.CV

TL;DR: 提出Granular-ball Guided Masking (GBGM)，一种结构感知的数据增强方法，利用Granular-ball Computing指导掩码过程，在保持语义丰富区域的同时抑制冗余区域


<details>
  <summary>Details</summary>
Motivation: 深度学习依赖大规模标注数据，在有限数据或分布变化时易过拟合；现有掩码增强方法缺乏结构感知，可能丢弃重要语义

Method: 基于Granular-ball Computing的粗到细分层掩码策略，自适应保留语义丰富、结构重要区域，抑制冗余区域

Result: 多基准测试显示在分类精度和掩码图像重建方面持续改进，证实方法有效性

Conclusion: GBGM为结构感知数据增强提供新范式，简单且模型无关，可无缝集成到CNN和Vision Transformer

Abstract: Deep learning models have achieved remarkable success in computer vision, but they still rely heavily on large-scale labeled data and tend to overfit when data are limited or distributions shift. Data augmentation, particularly mask-based information dropping, can enhance robustness by forcing models to explore complementary cues; however, existing approaches often lack structural awareness and may discard essential semantics. We propose Granular-ball Guided Masking (GBGM), a structure-aware augmentation strategy guided by Granular-ball Computing (GBC). GBGM adaptively preserves semantically rich, structurally important regions while suppressing redundant areas through a coarse-to-fine hierarchical masking process, producing augmentations that are both representative and discriminative. Extensive experiments on multiple benchmarks demonstrate consistent improvements in classification accuracy and masked image reconstruction, confirming the effectiveness and broad applicability of the proposed method. Simple and model-agnostic, it integrates seamlessly into CNNs and Vision Transformers and provides a new paradigm for structure-aware data augmentation.

</details>


### [24] [FluencyVE: Marrying Temporal-Aware Mamba with Bypass Attention for Video Editing](https://arxiv.org/abs/2512.21015)
*Mingshu Cai,Yixuan Li,Osamu Yoshie,Yuya Ieiri*

Main category: cs.CV

TL;DR: FluencyVE用Mamba替代时空注意力层，实现高效单镜头视频编辑


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑方法在扩展文本到图像扩散模型时存在时序不一致和高计算成本问题

Method: 将线性时间序列模块Mamba集成到基于Stable Diffusion的视频编辑模型中，替换时空注意力层，并使用低秩近似矩阵替代因果注意力的查询和键权重矩阵，结合加权平均技术更新注意力分数

Result: 实验显示该方法在编辑真实视频中各种属性、主体和场景方面取得良好效果

Conclusion: FluencyVE在保持文本到图像模型生成能力的同时，显著降低了计算负担，实现了高效的视频编辑

Abstract: Large-scale text-to-image diffusion models have achieved unprecedented success in image generation and editing. However, extending this success to video editing remains challenging. Recent video editing efforts have adapted pretrained text-to-image models by adding temporal attention mechanisms to handle video tasks. Unfortunately, these methods continue to suffer from temporal inconsistency issues and high computational overheads. In this study, we propose FluencyVE, which is a simple yet effective one-shot video editing approach. FluencyVE integrates the linear time-series module, Mamba, into a video editing model based on pretrained Stable Diffusion models, replacing the temporal attention layer. This enables global frame-level attention while reducing the computational costs. In addition, we employ low-rank approximation matrices to replace the query and key weight matrices in the causal attention, and use a weighted averaging technique during training to update the attention scores. This approach significantly preserves the generative power of the text-to-image model while effectively reducing the computational burden. Experiments and analyses demonstrate promising results in editing various attributes, subjects, and locations in real-world videos.

</details>


### [25] [Efficient and Robust Video Defense Framework against 3D-field Personalized Talking Face](https://arxiv.org/abs/2512.21019)
*Rui-qing Sun,Xingshan Yao,Tian Lan,Hui-Yang Zhao,Jia-Ling Shi,Chen-Hao Cui,Zhijing Wu,Chen Yang,Xian-Ling Mao*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: State-of-the-art 3D-field video-referenced Talking Face Generation (TFG) methods synthesize high-fidelity personalized talking-face videos in real time by modeling 3D geometry and appearance from reference portrait video. This capability raises significant privacy concerns regarding malicious misuse of personal portraits. However, no efficient defense framework exists to protect such videos against 3D-field TFG methods. While image-based defenses could apply per-frame 2D perturbations, they incur prohibitive computational costs, severe video quality degradation, failing to disrupt 3D information for video protection. To address this, we propose a novel and efficient video defense framework against 3D-field TFG methods, which protects portrait video by perturbing the 3D information acquisition process while maintain high-fidelity video quality. Specifically, our method introduces: (1) a similarity-guided parameter sharing mechanism for computational efficiency, and (2) a multi-scale dual-domain attention module to jointly optimize spatial-frequency perturbations. Extensive experiments demonstrate that our proposed framework exhibits strong defense capability and achieves a 47x acceleration over the fastest baseline while maintaining high fidelity. Moreover, it remains robust against scaling operations and state-of-the-art purification attacks, and the effectiveness of our design choices is further validated through ablation studies. Our project is available at https://github.com/Richen7418/VDF.

</details>


### [26] [Multi-Attribute guided Thermal Face Image Translation based on Latent Diffusion Model](https://arxiv.org/abs/2512.21032)
*Mingshu Cai,Osamu Yoshie,Yuya Ieiri*

Main category: cs.CV

TL;DR: 本文针对红外光到可见光人脸识别的跨模态匹配问题，提出了基于潜在扩散模型的高质量图像生成方案，结合多属性分类器和Self-attn Mamba模块，在提升图像质量的同时有效保留了身份信息。


<details>
  <summary>Details</summary>
Motivation: 传统人脸识别模型在可见光数据集上训练，对红外图像性能显著下降；生成式方法虽能缓解模态差异，但仍存在图像失真和特征损失问题。

Method: 提出潜在扩散模型生成高质量可见光人脸图像；引入多属性分类器提取关键面部属性以减少特征损失；设计Self-attn Mamba模块增强全局跨模态建模能力并提升推理速度。

Result: 在两个基准数据集上实现最优性能，图像质量和身份保持均达到SOTA水平。

Conclusion: 该方法成功解决了红外-可见光人脸识别中的模态差异问题，生成图像质量高且身份特征保持良好，为跨模态人脸识别提供了有效解决方案。

Abstract: Modern surveillance systems increasingly rely on multi-wavelength sensors and deep neural networks to recognize faces in infrared images captured at night. However, most facial recognition models are trained on visible light datasets, leading to substantial performance degradation on infrared inputs due to significant domain shifts. Early feature-based methods for infrared face recognition proved ineffective, prompting researchers to adopt generative approaches that convert infrared images into visible light images for improved recognition. This paradigm, known as Heterogeneous Face Recognition (HFR), faces challenges such as model and modality discrepancies, leading to distortion and feature loss in generated images. To address these limitations, this paper introduces a novel latent diffusion-based model designed to generate high-quality visible face images from thermal inputs while preserving critical identity features. A multi-attribute classifier is incorporated to extract key facial attributes from visible images, mitigating feature loss during infrared-to-visible image restoration. Additionally, we propose the Self-attn Mamba module, which enhances global modeling of cross-modal features and significantly improves inference speed. Experimental results on two benchmark datasets demonstrate the superiority of our approach, achieving state-of-the-art performance in both image quality and identity preservation.

</details>


### [27] [A Large-Depth-Range Layer-Based Hologram Dataset for Machine Learning-Based 3D Computer-Generated Holography](https://arxiv.org/abs/2512.21040)
*Jaehong Lee,You Chan No,YoungWoo Kim,Duksu Kim*

Main category: cs.CV

TL;DR: 提出了KOREATECH-CGH公开数据集，包含6000对RGB-D图像和复杂全息图，并引入了振幅投影后处理技术以提高大深度范围的全息图质量。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习全息术（ML-CGH）面临高质量大规模全息图数据集不足的制约，阻碍了该领域的发展。为解决这一数据瓶颈，需要创建公开可用的大规模高质量全息图数据集。

Method: 1) 构建KOREATECH-CGH数据集：包含6,000对RGB-D图像和复杂全息图，分辨率从256*256到2048*2048，深度范围涵盖角谱方法理论极限；2) 提出振幅投影技术：一种后处理方法，在保持相位不变的同时替换全息波场在每个深度层的振幅分量。

Result: 振幅投影方法显著提高了重建质量，达到27.01 dB PSNR和0.87 SSIM，相比最近基于优化轮廓掩模层的方法分别提升了2.03 dB和0.04 SSIM。使用最先进的ML模型在全息生成和超分辨率任务上验证了数据集的实用性。

Conclusion: KOREATECH-CGH数据集解决了ML-CGH领域的数据稀缺问题，其高质量特征和引入的振幅投影技术为训练和评估下一代ML-CGH系统提供了重要资源，推动了该领域的研究进展。

Abstract: Machine learning-based computer-generated holography (ML-CGH) has advanced rapidly in recent years, yet progress is constrained by the limited availability of high-quality, large-scale hologram datasets. To address this, we present KOREATECH-CGH, a publicly available dataset comprising 6,000 pairs of RGB-D images and complex holograms across resolutions ranging from 256*256 to 2048*2048, with depth ranges extending to the theoretical limits of the angular spectrum method for wide 3D scene coverage. To improve hologram quality at large depth ranges, we introduce amplitude projection, a post-processing technique that replaces amplitude components of hologram wavefields at each depth layer while preserving phase. This approach enhances reconstruction fidelity, achieving 27.01 dB PSNR and 0.87 SSIM, surpassing a recent optimized silhouette-masking layer-based method by 2.03 dB and 0.04 SSIM, respectively. We further validate the utility of KOREATECH-CGH through experiments on hologram generation and super-resolution using state-of-the-art ML models, confirming its applicability for training and evaluating next-generation ML-CGH systems.

</details>


### [28] [Optical Flow-Guided 6DoF Object Pose Tracking with an Event Camera](https://arxiv.org/abs/2512.21053)
*Zibin Liu,Banglei Guan,Yang Shang,Shunkun Liang,Zhenbao Yu,Qifeng Yu*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Object pose tracking is one of the pivotal technologies in multimedia, attracting ever-growing attention in recent years. Existing methods employing traditional cameras encounter numerous challenges such as motion blur, sensor noise, partial occlusion, and changing lighting conditions. The emerging bio-inspired sensors, particularly event cameras, possess advantages such as high dynamic range and low latency, which hold the potential to address the aforementioned challenges. In this work, we present an optical flow-guided 6DoF object pose tracking method with an event camera. A 2D-3D hybrid feature extraction strategy is firstly utilized to detect corners and edges from events and object models, which characterizes object motion precisely. Then, we search for the optical flow of corners by maximizing the event-associated probability within a spatio-temporal window, and establish the correlation between corners and edges guided by optical flow. Furthermore, by minimizing the distances between corners and edges, the 6DoF object pose is iteratively optimized to achieve continuous pose tracking. Experimental results of both simulated and real events demonstrate that our methods outperform event-based state-of-the-art methods in terms of both accuracy and robustness.

</details>


### [29] [Beyond Pixel Simulation: Pathology Image Generation via Diagnostic Semantic Tokens and Prototype Control](https://arxiv.org/abs/2512.21058)
*Minghao Han,YiChen Liu,Yizhou Liu,Zizhi Chen,Jingqun Tang,Xuecheng Wu,Dingkang Yang,Lihua Zhang*

Main category: cs.CV

TL;DR: UniPath是一个语义驱动的病理图像生成框架，利用成熟的诊断理解能力实现可控生成，解决了病理计算中生成模型的三个关键限制：数据稀缺、缺乏细粒度语义控制以及术语异质性。


<details>
  <summary>Details</summary>
Motivation: 病理图像理解与生成的发展不平衡，高端理解模型已达诊断水平，但生成模型仍停留在像素模拟。主要阻碍包括：大规模高质量图文数据稀缺；缺乏精确的细粒度语义控制依赖非语义线索；术语异质性导致相同诊断概念的多样化表述影响文本条件控制可靠性。

Method: UniPath采用多流控制方法：1) 原始文本流；2) 高层次语义流，使用可学习查询到冻结果理多模态大语言模型，提取抗释义的诊断语义标记，并将提示扩展为诊断感知的属性包；3) 原型流，通过原型库提供组件级的形态学控制。同时构建了265万图文语料库和68K精细标注的高质量子集缓解数据稀缺问题。

Result: 建立了四层次病理评估体系，实验显示UniPath达到SOTA性能：Patho-FID为80.9（比第二名提高51%），细粒度语义控制达到真实图像的98.7%效果。精心构建的数据集、完整源代码和预训练模型权重将公开提供。

Conclusion: UniPath通过语义驱动的生成框架和多流控制方法，显著提升了病理图像生成的质量和控制能力，解决了当前病理计算中生成模型的主要挑战，为病理研究提供了开源工具和数据集。

Abstract: In computational pathology, understanding and generation have evolved along disparate paths: advanced understanding models already exhibit diagnostic-level competence, whereas generative models largely simulate pixels. Progress remains hindered by three coupled factors: the scarcity of large, high-quality image-text corpora; the lack of precise, fine-grained semantic control, which forces reliance on non-semantic cues; and terminological heterogeneity, where diverse phrasings for the same diagnostic concept impede reliable text conditioning. We introduce UniPath, a semantics-driven pathology image generation framework that leverages mature diagnostic understanding to enable controllable generation. UniPath implements Multi-Stream Control: a Raw-Text stream; a High-Level Semantics stream that uses learnable queries to a frozen pathology MLLM to distill paraphrase-robust Diagnostic Semantic Tokens and to expand prompts into diagnosis-aware attribute bundles; and a Prototype stream that affords component-level morphological control via a prototype bank. On the data front, we curate a 2.65M image-text corpus and a finely annotated, high-quality 68K subset to alleviate data scarcity. For a comprehensive assessment, we establish a four-tier evaluation hierarchy tailored to pathology. Extensive experiments demonstrate UniPath's SOTA performance, including a Patho-FID of 80.9 (51% better than the second-best) and fine-grained semantic control achieving 98.7% of the real-image. The meticulously curated datasets, complete source code, and pre-trained model weights developed in this study will be made openly accessible to the public.

</details>


### [30] [Multimodal Skeleton-Based Action Representation Learning via Decomposition and Composition](https://arxiv.org/abs/2512.21064)
*Hongsong Wang,Heng Fei,Bingxuan Dai,Jie Gui*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Multimodal human action understanding is a significant problem in computer vision, with the central challenge being the effective utilization of the complementarity among diverse modalities while maintaining model efficiency. However, most existing methods rely on simple late fusion to enhance performance, which results in substantial computational overhead. Although early fusion with a shared backbone for all modalities is efficient, it struggles to achieve excellent performance. To address the dilemma of balancing efficiency and effectiveness, we introduce a self-supervised multimodal skeleton-based action representation learning framework, named Decomposition and Composition. The Decomposition strategy meticulously decomposes the fused multimodal features into distinct unimodal features, subsequently aligning them with their respective ground truth unimodal counterparts. On the other hand, the Composition strategy integrates multiple unimodal features, leveraging them as self-supervised guidance to enhance the learning of multimodal representations. Extensive experiments on the NTU RGB+D 60, NTU RGB+D 120, and PKU-MMD II datasets demonstrate that the proposed method strikes an excellent balance between computational cost and model performance.

</details>


### [31] [UniPR-3D: Towards Universal Visual Place Recognition with Visual Geometry Grounded Transformer](https://arxiv.org/abs/2512.21078)
*Tianchen Deng,Xun Chen,Ziming Li,Hongming Shen,Danwei Wang,Javier Civera,Hesheng Wang*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Visual Place Recognition (VPR) has been traditionally formulated as a single-image retrieval task. Using multiple views offers clear advantages, yet this setting remains relatively underexplored and existing methods often struggle to generalize across diverse environments. In this work we introduce UniPR-3D, the first VPR architecture that effectively integrates information from multiple views. UniPR-3D builds on a VGGT backbone capable of encoding multi-view 3D representations, which we adapt by designing feature aggregators and fine-tune for the place recognition task. To construct our descriptor, we jointly leverage the 3D tokens and intermediate 2D tokens produced by VGGT. Based on their distinct characteristics, we design dedicated aggregation modules for 2D and 3D features, allowing our descriptor to capture fine-grained texture cues while also reasoning across viewpoints. To further enhance generalization, we incorporate both single- and multi-frame aggregation schemes, along with a variable-length sequence retrieval strategy. Our experiments show that UniPR-3D sets a new state of the art, outperforming both single- and multi-view baselines and highlighting the effectiveness of geometry-grounded tokens for VPR. Our code and models will be made publicly available on Github https://github.com/dtc111111/UniPR-3D.

</details>


### [32] [Hierarchical Modeling Approach to Fast and Accurate Table Recognition](https://arxiv.org/abs/2512.21083)
*Takaya Kawakatsu*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The extraction and use of diverse knowledge from numerous documents is a pressing challenge in intelligent information retrieval. Documents contain elements that require different recognition methods. Table recognition typically consists of three subtasks, namely table structure, cell position and cell content recognition. Recent models have achieved excellent recognition with a combination of multi-task learning, local attention, and mutual learning. However, their effectiveness has not been fully explained, and they require a long period of time for inference. This paper presents a novel multi-task model that utilizes non-causal attention to capture the entire table structure, and a parallel inference algorithm for faster cell content inference. The superiority is demonstrated both visually and statistically on two large public datasets.

</details>


### [33] [T2AV-Compass: Towards Unified Evaluation for Text-to-Audio-Video Generation](https://arxiv.org/abs/2512.21094)
*Zhe Cao,Tao Wang,Jiaming Wang,Yanghai Wang,Yuanxing Zhang,Jialu Chen,Miao Deng,Jiahao Wang,Yubin Guo,Chenxi Liao,Yize Zhang,Zhaoxiang Zhang,Jiaheng Liu*

Main category: cs.CV

TL;DR: T2AV-Compass：一个统一的文本到音视频生成系统评测基准，包含500个多样化复杂提示和双层级评估框架


<details>
  <summary>Details</summary>
Motivation: 当前T2AV生成任务的评测分散且不全面，通常依赖单模态指标或限定范围的基准，无法充分评估跨模态对齐、指令跟随和感知真实性等关键维度。

Method: 1）构建包含500个多样复杂提示的基准集；2）提出双层级评估框架：客观信号级指标（视频质量、音频质量、跨模态对齐）+主观MLLM-as-a-Judge协议；3）对11个代表性T2AV系统进行全面评估。

Result: 评测发现即使最强模型也与人类级真实感和跨模态一致性存在显著差距，在音频真实感、细粒度同步、指令跟随等方面存在持续性缺陷。

Conclusion: T2AV生成模型仍有巨大改进空间，T2AV-Compass作为一个具有挑战性和诊断性的测试平台，对推动领域发展具有重要价值。

Abstract: Text-to-Audio-Video (T2AV) generation aims to synthesize temporally coherent video and semantically synchronized audio from natural language, yet its evaluation remains fragmented, often relying on unimodal metrics or narrowly scoped benchmarks that fail to capture cross-modal alignment, instruction following, and perceptual realism under complex prompts. To address this limitation, we present T2AV-Compass, a unified benchmark for comprehensive evaluation of T2AV systems, consisting of 500 diverse and complex prompts constructed via a taxonomy-driven pipeline to ensure semantic richness and physical plausibility. Besides, T2AV-Compass introduces a dual-level evaluation framework that integrates objective signal-level metrics for video quality, audio quality, and cross-modal alignment with a subjective MLLM-as-a-Judge protocol for instruction following and realism assessment. Extensive evaluation of 11 representative T2AVsystems reveals that even the strongest models fall substantially short of human-level realism and cross-modal consistency, with persistent failures in audio realism, fine-grained synchronization, instruction following, etc. These results indicate significant improvement room for future models and highlight the value of T2AV-Compass as a challenging and diagnostic testbed for advancing text-to-audio-video generation.

</details>


### [34] [UniRec-0.1B: Unified Text and Formula Recognition with 0.1B Parameters](https://arxiv.org/abs/2512.21095)
*Yongkun Du,Zhineng Chen,Yazhen Xie,Weikang Baiand Hao Feng,Wei Shi,Yuchen Su,Can Huang,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 本文提出 UniRec-0.1B，一个仅 0.1B 参数的轻量级统一文档识别模型，可在字符、单词、行、段落和文档等多个层次同时识别文本和公式，解决了现有视觉语言模型参数大、计算需求高的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型虽然在统一识别文本和公式方面表现出色，但其参数量大、计算需求高，限制了在实际应用中的部署。因此需要开发轻量级但功能强大的统一识别模型，以满足实际应用的需求。

Method: 首先构建了包含 4000 万样本的 UniRec40M 数据集；其次针对层级结构多变和文本与公式语义纠缠两大挑战，提出了分层监督训练来显式指导结构理解，以及语义解耦分词器来分离文本和公式的表示。

Result: 实验结果表明，UniRec-0.1B 在中文和英文文档的多领域、多层次评估基准上，性能优于通用视觉语言模型和领先的文档解析专家模型，同时实现了 2-9 倍的加速。

Conclusion: UniRec-0.1B 作为一个轻量级但功能强大的统一识别模型，在保持高效计算的同时，提供了出色的文本和公式识别能力，验证了其在实际应用中的有效性和效率。

Abstract: Text and formulas constitute the core informational components of many documents. Accurately and efficiently recognizing both is crucial for developing robust and generalizable document parsing systems. Recently, vision-language models (VLMs) have achieved impressive unified recognition of text and formulas. However, they are large-sized and computationally demanding, restricting their usage in many applications. In this paper, we propose UniRec-0.1B, a unified recognition model with only 0.1B parameters. It is capable of performing text and formula recognition at multiple levels, including characters, words, lines, paragraphs, and documents. To implement this task, we first establish UniRec40M, a large-scale dataset comprises 40 million text, formula and their mix samples, enabling the training of a powerful yet lightweight model. Secondly, we identify two challenges when building such a lightweight but unified expert model. They are: structural variability across hierarchies and semantic entanglement between textual and formulaic content. To tackle these, we introduce a hierarchical supervision training that explicitly guides structural comprehension, and a semantic-decoupled tokenizer that separates text and formula representations. Finally, we develop a comprehensive evaluation benchmark covering Chinese and English documents from multiple domains and with multiple levels. Experimental results on this and public benchmarks demonstrate that UniRec-0.1B outperforms both general-purpose VLMs and leading document parsing expert models, while achieving a 2-9$\times$ speedup, validating its effectiveness and efficiency. Codebase and Dataset: https://github.com/Topdu/OpenOCR.

</details>


### [35] [FreeInpaint: Tuning-free Prompt Alignment and Visual Rationality Enhancement in Image Inpainting](https://arxiv.org/abs/2512.21104)
*Chao Gong,Dong Li,Yingwei Pan,Jingjing Chen,Ting Yao,Tao Mei*

Main category: cs.CV

TL;DR: FreeInpaint是一种即插即用的免调优图像修复方法，通过直接优化扩散过程中的潜变量来提升生成图像与文本提示的匹配度和视觉合理性。


<details>
  <summary>Details</summary>
Motivation: 现有基于预训练文本到图像扩散模型的修复方法难以同时保持提示对齐和视觉合理性，需要一种更有效的技术来解决这一挑战。

Method: 提出先验引导的噪声优化方法，通过优化初始噪声引导模型关注有效修复区域；同时设计了专门针对修复任务的复合引导目标，通过逐步优化中间潜变量来增强提示对齐和视觉合理性。

Result: 在多种修复扩散模型和评估指标上的广泛实验表明，FreeInpaint具有有效性和鲁棒性。

Conclusion: FreeInpaint作为即插即用的免调优方法，能够直接优化推理过程中的扩散潜变量，显著提升了文本引导图像修复的准确性和视觉质量。

Abstract: Text-guided image inpainting endeavors to generate new content within specified regions of images using textual prompts from users. The primary challenge is to accurately align the inpainted areas with the user-provided prompts while maintaining a high degree of visual fidelity. While existing inpainting methods have produced visually convincing results by leveraging the pre-trained text-to-image diffusion models, they still struggle to uphold both prompt alignment and visual rationality simultaneously. In this work, we introduce FreeInpaint, a plug-and-play tuning-free approach that directly optimizes the diffusion latents on the fly during inference to improve the faithfulness of the generated images. Technically, we introduce a prior-guided noise optimization method that steers model attention towards valid inpainting regions by optimizing the initial noise. Furthermore, we meticulously design a composite guidance objective tailored specifically for the inpainting task. This objective efficiently directs the denoising process, enhancing prompt alignment and visual rationality by optimizing intermediate latents at each step. Through extensive experiments involving various inpainting diffusion models and evaluation metrics, we demonstrate the effectiveness and robustness of our proposed FreeInpaint.

</details>


### [36] [MarineEval: Assessing the Marine Intelligence of Vision-Language Models](https://arxiv.org/abs/2512.21126)
*YuK-Kwan Wong,Tuan-An To,Jipeng Zhang,Ziqiang Zheng,Sai-Kit Yeung*

Main category: cs.CV

TL;DR: 构建首个大规模海洋视觉语言模型评估基准MarineEval，包含2000个图像问答对，评估现有VLMs在海洋专业领域的表现


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型在通用任务上表现出色，但缺乏对海洋等专业领域专家能力的评估。海洋问题需要专业知识和应对特殊领域挑战，作者希望探明现有VLMs的专业边界。

Method: 1) 构建包含7个任务维度、20个能力维度的海洋VLM数据集MarineEval，确保数据多样性和覆盖范围；2) 由海洋领域专家验证数据构建的专业性；3) 在17个现有VLM模型上进行全面基准测试；4) 分析模型在海洋研究问题上的局限性。

Result: 实验结果显示，现有VLM模型无法有效回答领域特定问题，性能仍有很大提升空间。模型在海洋专业知识理解方面存在明显不足。

Conclusion: 研究揭示了现有视觉语言模型在专业领域（特别是海洋科学）的知识局限性，提出的MarineEval基准将为未来研究提供重要评估工具，促进领域专业化模型的发展。

Abstract: We have witnessed promising progress led by large language models (LLMs) and further vision language models (VLMs) in handling various queries as a general-purpose assistant. VLMs, as a bridge to connect the visual world and language corpus, receive both visual content and various text-only user instructions to generate corresponding responses. Though great success has been achieved by VLMs in various fields, in this work, we ask whether the existing VLMs can act as domain experts, accurately answering marine questions, which require significant domain expertise and address special domain challenges/requirements. To comprehensively evaluate the effectiveness and explore the boundary of existing VLMs, we construct the first large-scale marine VLM dataset and benchmark called MarineEval, with 2,000 image-based question-answering pairs. During our dataset construction, we ensure the diversity and coverage of the constructed data: 7 task dimensions and 20 capacity dimensions. The domain requirements are specially integrated into the data construction and further verified by the corresponding marine domain experts. We comprehensively benchmark 17 existing VLMs on our MarineEval and also investigate the limitations of existing models in answering marine research questions. The experimental results reveal that existing VLMs cannot effectively answer the domain-specific questions, and there is still a large room for further performance improvements. We hope our new benchmark and observations will facilitate future research. Project Page: http://marineeval.hkustvgd.com/

</details>


### [37] [TGC-Net: A Structure-Aware and Semantically-Aligned Framework for Text-Guided Medical Image Segmentation](https://arxiv.org/abs/2512.21135)
*Gaoren Lin,Huangxuan Zhao,Yuan Xiong,Lefei Zhang,Bo Du,Wentao Zhu*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Text-guided medical segmentation enhances segmentation accuracy by utilizing clinical reports as auxiliary information. However, existing methods typically rely on unaligned image and text encoders, which necessitate complex interaction modules for multimodal fusion. While CLIP provides a pre-aligned multimodal feature space, its direct application to medical imaging is limited by three main issues: insufficient preservation of fine-grained anatomical structures, inadequate modeling of complex clinical descriptions, and domain-specific semantic misalignment. To tackle these challenges, we propose TGC-Net, a CLIP-based framework focusing on parameter-efficient, task-specific adaptations. Specifically, it incorporates a Semantic-Structural Synergy Encoder (SSE) that augments CLIP's ViT with a CNN branch for multi-scale structural refinement, a Domain-Augmented Text Encoder (DATE) that injects large-language-model-derived medical knowledge, and a Vision-Language Calibration Module (VLCM) that refines cross-modal correspondence in a unified feature space. Experiments on five datasets across chest X-ray and thoracic CT modalities demonstrate that TGC-Net achieves state-of-the-art performance with substantially fewer trainable parameters, including notable Dice gains on challenging benchmarks.

</details>


### [38] [ORCA: Object Recognition and Comprehension for Archiving Marine Species](https://arxiv.org/abs/2512.21150)
*Yuk-Kwan Wong,Haixin Liang,Zeyu Ma,Yiwei Chen,Ziqiang Zheng,Rinaldi Gotama,Pascal Sebastian,Lauren D. Sparks,Sai-Kit Yeung*

Main category: cs.CV

TL;DR: 这篇论文提出了ORCA基准，一个多模态海洋研究数据库，旨在解决海洋视觉理解中训练数据有限和任务定义不系统的问题，通过评估18个先进模型在物体检测、实例描述和视觉定位等任务上的表现，揭示了该领域的挑战。


<details>
  <summary>Details</summary>
Motivation: 海洋视觉理解对生态监测和保护至关重要，但当前研究受限于训练数据不足和缺乏系统性的任务定义，难以将海洋特定挑战与计算机视觉任务对齐，阻碍了模型的有效应用。

Method: 构建ORCA基准数据集，包含14,647张图像、478个物种的42,217个边界框标注和22,321个专家验证的实例描述，提供细粒度的视觉和文本标注，涵盖形态学属性。在物体检测（封闭集和开放词汇）、实例描述和视觉定位三个任务上评估了18个先进模型。

Result: 评估结果突显了海洋视觉理解中的关键挑战，包括物种多样性、形态重叠和领域特殊性，表明现有模型在海洋领域的表现仍面临困难。

Conclusion: ORCA为海洋视觉理解研究提供了一个全面的基准，有助于推动该领域的方法进步，并强调了解决海洋特有挑战的重要性。

Abstract: Marine visual understanding is essential for monitoring and protecting marine ecosystems, enabling automatic and scalable biological surveys. However, progress is hindered by limited training data and the lack of a systematic task formulation that aligns domain-specific marine challenges with well-defined computer vision tasks, thereby limiting effective model application. To address this gap, we present ORCA, a multi-modal benchmark for marine research comprising 14,647 images from 478 species, with 42,217 bounding box annotations and 22,321 expert-verified instance captions. The dataset provides fine-grained visual and textual annotations that capture morphology-oriented attributes across diverse marine species. To catalyze methodological advances, we evaluate 18 state-of-the-art models on three tasks: object detection (closed-set and open-vocabulary), instance captioning, and visual grounding. Results highlight key challenges, including species diversity, morphological overlap, and specialized domain demands, underscoring the difficulty of marine understanding. ORCA thus establishes a comprehensive benchmark to advance research in marine domain. Project Page: http://orca.hkustvgd.com/.

</details>


### [39] [Towards Arbitrary Motion Completing via Hierarchical Continuous Representation](https://arxiv.org/abs/2512.21183)
*Chenghao Xu,Guangtao Lyu,Qi Liu,Jiexi Yan,Muli Yang,Cheng Deng*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Physical motions are inherently continuous, and higher camera frame rates typically contribute to improved smoothness and temporal coherence. For the first time, we explore continuous representations of human motion sequences, featuring the ability to interpolate, inbetween, and even extrapolate any input motion sequences at arbitrary frame rates. To achieve this, we propose a novel parametric activation-induced hierarchical implicit representation framework, referred to as NAME, based on Implicit Neural Representations (INRs). Our method introduces a hierarchical temporal encoding mechanism that extracts features from motion sequences at multiple temporal scales, enabling effective capture of intricate temporal patterns. Additionally, we integrate a custom parametric activation function, powered by Fourier transformations, into the MLP-based decoder to enhance the expressiveness of the continuous representation. This parametric formulation significantly augments the model's ability to represent complex motion behaviors with high accuracy. Extensive evaluations across several benchmark datasets demonstrate the effectiveness and robustness of our proposed approach.

</details>


### [40] [UltraShape 1.0: High-Fidelity 3D Shape Generation via Scalable Geometric Refinement](https://arxiv.org/abs/2512.21185)
*Tanghui Jia,Dongyu Yan,Dehao Hao,Yang Li,Kaiyi Zhang,Xianyi He,Lanjiong Li,Jinnan Chen,Lutao Jiang,Qishen Yin,Long Quan,Ying-Cong Chen,Li Yuan*

Main category: cs.CV

TL;DR: UltraShape 1.0是一个可扩展的3D扩散框架，使用两阶段生成流程：首先生成粗略全局结构，然后进行细化，能够生成高保真度的3D几何体。


<details>
  <summary>Details</summary>
Motivation: 现有的3D生成方法在几何细节和质量上往往受限，且缺乏足够的数据处理流程。作者致力于开发一个能在有限训练资源下生成高质量3D几何的框架。

Method: 1. 开发全面的数据处理流程：包括新颖的密封处理方法和高质量数据过滤，改善公开3D数据集的几何质量；2. 两阶段生成：首先生成粗略全局结构，然后通过分离空间定位与几何细节合成的扩散过程进行细化；3. 采用基于体素的细化：在固定空间位置进行，利用粗略几何的体素查询提供通过RoPE编码的位置锚点。

Result: 模型在公开3D数据集上训练，几何质量表现强劲。广泛评估表明，UltraShape 1.0在数据处理质量和几何生成方面与现有开源方法具有竞争力。

Conclusion: UltraShape 1.0框架能够在有限条件下生成高质量的3D几何体，为未来研究提供了支持，所有代码和训练模型将公开发布。

Abstract: In this report, we introduce UltraShape 1.0, a scalable 3D diffusion framework for high-fidelity 3D geometry generation. The proposed approach adopts a two-stage generation pipeline: a coarse global structure is first synthesized and then refined to produce detailed, high-quality geometry. To support reliable 3D generation, we develop a comprehensive data processing pipeline that includes a novel watertight processing method and high-quality data filtering. This pipeline improves the geometric quality of publicly available 3D datasets by removing low-quality samples, filling holes, and thickening thin structures, while preserving fine-grained geometric details. To enable fine-grained geometry refinement, we decouple spatial localization from geometric detail synthesis in the diffusion process. We achieve this by performing voxel-based refinement at fixed spatial locations, where voxel queries derived from coarse geometry provide explicit positional anchors encoded via RoPE, allowing the diffusion model to focus on synthesizing local geometric details within a reduced, structured solution space. Our model is trained exclusively on publicly available 3D datasets, achieving strong geometric quality despite limited training resources. Extensive evaluations demonstrate that UltraShape 1.0 performs competitively with existing open-source methods in both data processing quality and geometry generation. All code and trained models will be released to support future research.

</details>


### [41] [VisRes Bench: On Evaluating the Visual Reasoning Capabilities of VLMs](https://arxiv.org/abs/2512.21194)
*Brigitta Malagurski Törtei,Yasser Dahou,Ngoc Dung Huynh,Wamiq Reyaz Para,Phúc H. Lê Khac,Ankit Singh,Sofian Chaybouti,Sanath Narayan*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Vision-Language Models (VLMs) have achieved remarkable progress across tasks such as visual question answering and image captioning. Yet, the extent to which these models perform visual reasoning as opposed to relying on linguistic priors remains unclear. To address this, we introduce VisRes Bench, a benchmark designed to study visual reasoning in naturalistic settings without contextual language supervision. Analyzing model behavior across three levels of complexity, we uncover clear limitations in perceptual and relational visual reasoning capacities. VisRes isolates distinct reasoning abilities across its levels. Level 1 probes perceptual completion and global image matching under perturbations such as blur, texture changes, occlusion, and rotation; Level 2 tests rule-based inference over a single attribute (e.g., color, count, orientation); and Level 3 targets compositional reasoning that requires integrating multiple visual attributes. Across more than 19,000 controlled task images, we find that state-of-the-art VLMs perform near random under subtle perceptual perturbations, revealing limited abstraction beyond pattern recognition. We conclude by discussing how VisRes provides a unified framework for advancing abstract visual reasoning in multimodal research.

</details>


### [42] [Human Motion Estimation with Everyday Wearables](https://arxiv.org/abs/2512.21209)
*Siqi Zhu,Yixuan Li,Junfu Li,Qi Wu,Zan Wang,Haozhe Ma,Wei Liang*

Main category: cs.CV

TL;DR: EveryWear是一种基于日常可穿戴设备的轻量级人体运动捕捉方案，使用智能手机、智能手表、耳机和带摄像头的智能眼镜，无需显式校准，旨在解决现有方法穿戴性差、硬件昂贵和校准繁琐的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于穿戴设备的运动估计算法存在穿戴舒适度差、硬件成本高、校准过程繁琐等缺点，限制了其在日常生活中的应用。

Method: 采用多模态师生框架，将自我中心摄像头的视觉线索与消费设备惯性信号相结合，在真实世界数据集Ego-Elec（9小时数据，56种日常活动，17个多样化环境）上直接训练，避免仿真到实际的差距。

Result: 实验表明该方法优于基准模型，验证了其在实用全身体运动估计中的有效性。

Conclusion: EveryWear通过集成日常可穿戴设备和利用真实世界训练数据，实现了无需校准的实用人体运动捕捉，有望促进XR交互等应用的日常采用。

Abstract: While on-body device-based human motion estimation is crucial for applications such as XR interaction, existing methods often suffer from poor wearability, expensive hardware, and cumbersome calibration, which hinder their adoption in daily life. To address these challenges, we present EveryWear, a lightweight and practical human motion capture approach based entirely on everyday wearables: a smartphone, smartwatch, earbuds, and smart glasses equipped with one forward-facing and two downward-facing cameras, requiring no explicit calibration before use. We introduce Ego-Elec, a 9-hour real-world dataset covering 56 daily activities across 17 diverse indoor and outdoor environments, with ground-truth 3D annotations provided by the motion capture (MoCap), to facilitate robust research and benchmarking in this direction. Our approach employs a multimodal teacher-student framework that integrates visual cues from egocentric cameras with inertial signals from consumer devices. By training directly on real-world data rather than synthetic data, our model effectively eliminates the sim-to-real gap that constrains prior work. Experiments demonstrate that our method outperforms baseline models, validating its effectiveness for practical full-body motion estimation.

</details>


### [43] [Latent Implicit Visual Reasoning](https://arxiv.org/abs/2512.21218)
*Kelvin Li,Chuyi Shang,Leonid Karlinsky,Rogerio Feris,Trevor Darrell,Roei Herzig*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: While Large Multimodal Models (LMMs) have made significant progress, they remain largely text-centric, relying on language as their core reasoning modality. As a result, they are limited in their ability to handle reasoning tasks that are predominantly visual. Recent approaches have sought to address this by supervising intermediate visual steps with helper images, depth maps, or image crops. However, these strategies impose restrictive priors on what "useful" visual abstractions look like, add heavy annotation costs, and struggle to generalize across tasks. To address this critical limitation, we propose a task-agnostic mechanism that trains LMMs to discover and use visual reasoning tokens without explicit supervision. These tokens attend globally and re-encode the image in a task-adaptive way, enabling the model to extract relevant visual information without hand-crafted supervision. Our approach outperforms direct fine-tuning and achieves state-of-the-art results on a diverse range of vision-centric tasks -- including those where intermediate abstractions are hard to specify -- while also generalizing to multi-task instruction tuning.

</details>


### [44] [Leveraging Lightweight Entity Extraction for Scalable Event-Based Image Retrieval](https://arxiv.org/abs/2512.21221)
*Dao Sy Duy Minh,Huynh Trung Kiet,Nguyen Lam Phu Quy,Phu-Hoa Pham,Tran Chi Nguyen*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Retrieving images from natural language descriptions is a core task at the intersection of computer vision and natural language processing, with wide-ranging applications in search engines, media archiving, and digital content management. However, real-world image-text retrieval remains challenging due to vague or context-dependent queries, linguistic variability, and the need for scalable solutions. In this work, we propose a lightweight two-stage retrieval pipeline that leverages event-centric entity extraction to incorporate temporal and contextual signals from real-world captions. The first stage performs efficient candidate filtering using BM25 based on salient entities, while the second stage applies BEiT-3 models to capture deep multimodal semantics and rerank the results. Evaluated on the OpenEvents v1 benchmark, our method achieves a mean average precision of 0.559, substantially outperforming prior baselines. These results highlight the effectiveness of combining event-guided filtering with long-text vision-language modeling for accurate and efficient retrieval in complex, real-world scenarios. Our code is available at https://github.com/PhamPhuHoa-23/Event-Based-Image-Retrieval

</details>


### [45] [SegMo: Segment-aligned Text to 3D Human Motion Generation](https://arxiv.org/abs/2512.21237)
*Bowen Dang,Lin Wu,Xiaohang Yang,Zheng Yuan,Zhixiang Chen*

Main category: cs.CV

TL;DR: SegMo是一个实现细粒度文本-动作对齐的分段对齐文本条件动作生成框架，通过将文本和动作分解为语义连贯的小段单元来提升对齐效果


<details>
  <summary>Details</summary>
Motivation: 现有方法在序列级别对齐文本描述与人体动作，但忽略了模态内部的语义结构。文本描述和动作序列都可以自然分解为更小的语义连贯片段，这些片段能作为原子对齐单元实现更细粒度的对应

Method: SegMo包含三个模块：(1)文本片段提取：将复杂文本描述分解为时序排序的短语，每个短语表示一个简单原子动作；(2)动作片段提取：将完整动作序列分割为对应的动作片段；(3)细粒度文本-动作对齐：通过对比学习对齐文本和动作片段

Result: 在两个广泛使用的数据集上显著提升了基线性能，在HumanML3D测试集上达到了0.553的TOP 1分数。由于学习了文本和动作片段的共享嵌入空间，SegMo还可应用于动作定位和动作-文本检索等检索式任务

Conclusion: 通过细粒度的分段对齐机制，SegMo能够更精确地实现文本条件的人体动作生成，展示了在动作生成和相关检索任务上的有效性

Abstract: Generating 3D human motions from textual descriptions is an important research problem with broad applications in video games, virtual reality, and augmented reality. Recent methods align the textual description with human motion at the sequence level, neglecting the internal semantic structure of modalities. However, both motion descriptions and motion sequences can be naturally decomposed into smaller and semantically coherent segments, which can serve as atomic alignment units to achieve finer-grained correspondence. Motivated by this, we propose SegMo, a novel Segment-aligned text-conditioned human Motion generation framework to achieve fine-grained text-motion alignment. Our framework consists of three modules: (1) Text Segment Extraction, which decomposes complex textual descriptions into temporally ordered phrases, each representing a simple atomic action; (2) Motion Segment Extraction, which partitions complete motion sequences into corresponding motion segments; and (3) Fine-grained Text-Motion Alignment, which aligns text and motion segments with contrastive learning. Extensive experiments demonstrate that SegMo improves the strong baseline on two widely used datasets, achieving an improved TOP 1 score of 0.553 on the HumanML3D test set. Moreover, thanks to the learned shared embedding space for text and motion segments, SegMo can also be applied to retrieval-style tasks such as motion grounding and motion-to-text retrieval.

</details>


### [46] [DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation](https://arxiv.org/abs/2512.21252)
*Jiawei Liu,Junqiao Li,Jiangfan Deng,Gen Li,Siyu Zhou,Zetao Fang,Shanshan Lao,Zengde Deng,Jianing Zhu,Tingting Ma,Jiayi Li,Yunqiu Wang,Qian He,Xinglong Wu*

Main category: cs.CV

TL;DR: DreaMontage：一个从任意帧输入生成无缝长镜头视频的综合框架，通过中间条件机制、视觉表达微调和分段自回归推理实现高质量的虚拟长镜头制作。


<details>
  <summary>Details</summary>
Motivation: 传统长镜头拍摄成本高昂且受现实条件限制，现有视频生成方法多采用简单片段拼接，难以保证视觉流畅性和时间连贯性。

Method: 1. 在DiT架构中加入轻量级中间条件机制，通过自适应调优实现任意帧控制；2. 构建高质量数据集，通过视觉表达SFT阶段和定制DPO方案提升视觉效果；3. 设计内存高效的分段自回归推理策略以生成长序列。

Result: 实验表明该方法能生成视觉惊艳、无缝连贯的长镜头效果，计算效率高，可将碎片化视觉素材转化为生动的长镜头电影体验。

Conclusion: DreaMontage提供了一个完整框架，能够从多样化用户输入生成高质量虚拟长镜头视频，解决了现有方法的局限性，为用户提供了强大的创作工具。

Abstract: The "one-shot" technique represents a distinct and sophisticated aesthetic in filmmaking. However, its practical realization is often hindered by prohibitive costs and complex real-world constraints. Although emerging video generation models offer a virtual alternative, existing approaches typically rely on naive clip concatenation, which frequently fails to maintain visual smoothness and temporal coherence. In this paper, we introduce DreaMontage, a comprehensive framework designed for arbitrary frame-guided generation, capable of synthesizing seamless, expressive, and long-duration one-shot videos from diverse user-provided inputs. To achieve this, we address the challenge through three primary dimensions. (i) We integrate a lightweight intermediate-conditioning mechanism into the DiT architecture. By employing an Adaptive Tuning strategy that effectively leverages base training data, we unlock robust arbitrary-frame control capabilities. (ii) To enhance visual fidelity and cinematic expressiveness, we curate a high-quality dataset and implement a Visual Expression SFT stage. In addressing critical issues such as subject motion rationality and transition smoothness, we apply a Tailored DPO scheme, which significantly improves the success rate and usability of the generated content. (iii) To facilitate the production of extended sequences, we design a Segment-wise Auto-Regressive (SAR) inference strategy that operates in a memory-efficient manner. Extensive experiments demonstrate that our approach achieves visually striking and seamlessly coherent one-shot effects while maintaining computational efficiency, empowering users to transform fragmented visual materials into vivid, cohesive one-shot cinematic experiences.

</details>


### [47] [AnyAD: Unified Any-Modality Anomaly Detection in Incomplete Multi-Sequence MRI](https://arxiv.org/abs/2512.21264)
*Changwei Wu,Yifei Chen,Yuxin Du,Mingxuan Liu,Jinying Zong,Beining Wu,Jie Dong,Feiwei Qin,Yunkang Cao,Qiyuan Tian*

Main category: cs.CV

TL;DR: 该研究提出了一种统一的多模态MRI异常检测框架Any-Modality AD，能够在任意模态可用性下进行稳健的异常检测和定位，无需重复训练即可适应各种模态组合。


<details>
  <summary>Details</summary>
Motivation: 脑MRI异常检测面临标注数据稀缺和临床工作流中模态不全的挑战，现有方法依赖固定模态配置、需要重复训练或无法泛化到未见模态组合，限制了临床可扩展性。

Method: 框架集成了双路径DINOv2编码器和特征分布对齐机制，通过统计对齐不完整模态特征与全模态表示实现稳定推理；引入内在正常原型提取器和INP引导的解码器，仅重建正常解剖模式并自然放大异常偏差；通过随机模态掩码和间接特征完成训练，使模型适应所有模态配置。

Result: 在BraTS2018、MU-Glioma-Post和Pretreat-MetsToBrain-Masks数据集上的实验表明，该方法在7种模态组合上一致超越最先进的工业和医疗AD基线，实现了优越的泛化性能。

Conclusion: 该研究为现实世界中不完美模态条件下的多模态医学异常检测建立了可扩展的范式，能够在临床工作流中处理任意模态可用性场景。

Abstract: Reliable anomaly detection in brain MRI remains challenging due to the scarcity of annotated abnormal cases and the frequent absence of key imaging modalities in real clinical workflows. Existing single-class or multi-class anomaly detection (AD) models typically rely on fixed modality configurations, require repetitive training, or fail to generalize to unseen modality combinations, limiting their clinical scalability. In this work, we present a unified Any-Modality AD framework that performs robust anomaly detection and localization under arbitrary MRI modality availability. The framework integrates a dual-pathway DINOv2 encoder with a feature distribution alignment mechanism that statistically aligns incomplete-modality features with full-modality representations, enabling stable inference even with severe modality dropout. To further enhance semantic consistency, we introduce an Intrinsic Normal Prototypes (INPs) extractor and an INP-guided decoder that reconstruct only normal anatomical patterns while naturally amplifying abnormal deviations. Through randomized modality masking and indirect feature completion during training, the model learns to adapt to all modality configurations without re-training. Extensive experiments on BraTS2018, MU-Glioma-Post, and Pretreat-MetsToBrain-Masks demonstrate that our approach consistently surpasses state-of-the-art industrial and medical AD baselines across 7 modality combinations, achieving superior generalization. This study establishes a scalable paradigm for multimodal medical AD under real-world, imperfect modality conditions. Our source code is available at https://github.com/wuchangw/AnyAD.

</details>


### [48] [ACD: Direct Conditional Control for Video Diffusion Models via Attention Supervision](https://arxiv.org/abs/2512.21268)
*Weiqi Li,Zehao Zhang,Liang Lin,Guangrun Wang*

Main category: cs.CV

TL;DR: 本研究提出了注意力条件扩散（ACD）框架，通过注意力监督直接控制视频扩散模型，解决了现有引导方法中条件对齐的局限性问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频合成中的可控性方法存在局限性，无分类器引导通过联合建模数据与条件分布实现间接控制，可控性有限；基于分类器的引导虽然通过外部分类器强制满足条件，但模型可能通过对抗性手段提高分类器分数而非真正满足条件，导致虚假伪影和有效可控性受限。

Method: 提出注意力条件扩散（ACD）框架，通过将模型注意力图与外部控制信号对齐来实现直接的视频扩散条件控制。为此引入了稀疏3D感知物体布局作为高效条件信号，配备了专门的布局ControlNet以及自动注释流程，实现可扩展的布局集成。

Result: 在基准视频生成数据集上的大量实验表明，ACD在条件输入对齐方面表现优异，同时保持了时间连贯性和视觉保真度，为条件视频合成建立了有效的范式。

Conclusion: 注意力条件扩散（ACD）通过注意力监督机制实现了视频扩散模型的直接条件控制，与现有方法相比提供了更好的条件对齐能力，是条件视频合成的有效解决方案。

Abstract: Controllability is a fundamental requirement in video synthesis, where accurate alignment with conditioning signals is essential. Existing classifier-free guidance methods typically achieve conditioning indirectly by modeling the joint distribution of data and conditions, which often results in limited controllability over the specified conditions. Classifier-based guidance enforces conditions through an external classifier, but the model may exploit this mechanism to raise the classifier score without genuinely satisfying the intended condition, resulting in adversarial artifacts and limited effective controllability. In this paper, we propose Attention-Conditional Diffusion (ACD), a novel framework for direct conditional control in video diffusion models via attention supervision. By aligning the model's attention maps with external control signals, ACD achieves better controllability. To support this, we introduce a sparse 3D-aware object layout as an efficient conditioning signal, along with a dedicated Layout ControlNet and an automated annotation pipeline for scalable layout integration. Extensive experiments on benchmark video generation datasets demonstrate that ACD delivers superior alignment with conditioning inputs while preserving temporal coherence and visual fidelity, establishing an effective paradigm for conditional video synthesis.

</details>


### [49] [GriDiT: Factorized Grid-Based Diffusion for Efficient Long Image Sequence Generation](https://arxiv.org/abs/2512.21276)
*Snehal Singh Tomar,Alexandros Graikos,Arjun Krishna,Dimitris Samaras,Klaus Mueller*

Main category: cs.CV

TL;DR: 论文提出一种新的图像序列生成方法：先生成低分辨率序列，再单独对每帧进行超分辨率处理，避免处理大型图像序列张量，提高生成质量和效率。


<details>
  <summary>Details</summary>
Motivation: 当前基于张量堆叠帧的SOTA图像序列生成方法存在效率瓶颈，作者旨在探索更有效的方法，将序列生成分解为粗粒度序列生成和逐帧精炼两个阶段。

Method: 1. 仅使用子采样帧组成的网格图像训练生成模型；2. 利用DiT的自注意力机制学习帧间相关性，将2D生成器扩展到低分辨率3D序列生成器；3. 对每帧单独进行超分辨率添加高分辨率细节。

Result: 与现有方法相比，该方法在合成质量、序列一致性、任意长度序列生成、推理效率和训练数据使用等方面均有优势，且在多个数据集上质量优于SOTA、推理速度至少快两倍。

Conclusion: 通过分解序列生成过程（先低分辨率序列生成再逐帧超分辨率），该方法克服了当前SOTA的关键限制，实现了更高效、高质量的图像序列生成，并能有效泛化到不同数据领域。

Abstract: Modern deep learning methods typically treat image sequences as large tensors of sequentially stacked frames. However, is this straightforward representation ideal given the current state-of-the-art (SoTA)? In this work, we address this question in the context of generative models and aim to devise a more effective way of modeling image sequence data. Observing the inefficiencies and bottlenecks of current SoTA image sequence generation methods, we showcase that rather than working with large tensors, we can improve the generation process by factorizing it into first generating the coarse sequence at low resolution and then refining the individual frames at high resolution. We train a generative model solely on grid images comprising subsampled frames. Yet, we learn to generate image sequences, using the strong self-attention mechanism of the Diffusion Transformer (DiT) to capture correlations between frames. In effect, our formulation extends a 2D image generator to operate as a low-resolution 3D image-sequence generator without introducing any architectural modifications. Subsequently, we super-resolve each frame individually to add the sequence-independent high-resolution details. This approach offers several advantages and can overcome key limitations of the SoTA in this domain. Compared to existing image sequence generation models, our method achieves superior synthesis quality and improved coherence across sequences. It also delivers high-fidelity generation of arbitrary-length sequences and increased efficiency in inference time and training data usage. Furthermore, our straightforward formulation enables our method to generalize effectively across diverse data domains, which typically require additional priors and supervision to model in a generative context. Our method consistently outperforms SoTA in quality and inference speed (at least twice-as-fast) across datasets.

</details>


### [50] [Post-Processing Mask-Based Table Segmentation for Structural Coordinate Extraction](https://arxiv.org/abs/2512.21287)
*Suren Bandara*

Main category: cs.CV

TL;DR: 提出一种基于多尺度信号处理的新方法，从表格掩码中检测表格边缘，通过高斯卷积和统计阈值在噪声中高效识别行列边界。


<details>
  <summary>Details</summary>
Motivation: 从扫描文档和数字档案中提取表格结构化数据时，行列边界准确检测仍面临挑战，尤其是在低分辨率或噪声图像中。现有Transformer方法对噪声适应性有限，而基于掩码的边缘检测方法易受噪声影响或计算成本高。

Method: 将行列转换建模为一维信号，采用方差逐步增大的高斯卷积进行处理，然后通过统计阈值抑制噪声并保留稳定结构边缘，最后将检测到的信号峰值映射回图像坐标得到准确分割边界。

Result: 在PubLayNet-1M基准测试中，将TableNet与PyTesseract OCR结合的细胞感知分割准确率从67%提升至76%，证明方法对分辨率变化的鲁棒性。

Conclusion: 该方法通过创新的多尺度信号处理技术，能够有效抑制噪声并获得优化的结构化表格输出，适用于各种下游分析任务。

Abstract: Structured data extraction from tables plays a crucial role in document image analysis for scanned documents and digital archives. Although many methods have been proposed to detect table structures and extract cell contents, accurately identifying table segment boundaries (rows and columns) remains challenging, particularly in low-resolution or noisy images. In many real-world scenarios, table data are incomplete or degraded, limiting the adaptability of transformer-based methods to noisy inputs. Mask-based edge detection techniques have shown greater robustness under such conditions, as their sensitivity can be adjusted through threshold tuning; however, existing approaches typically apply masks directly to images, leading to noise sensitivity, resolution loss, or high computational cost. This paper proposes a novel multi-scale signal-processing method for detecting table edges from table masks. Row and column transitions are modeled as one-dimensional signals and processed using Gaussian convolution with progressively increasing variances, followed by statistical thresholding to suppress noise while preserving stable structural edges. Detected signal peaks are mapped back to image coordinates to obtain accurate segment boundaries. Experimental results show that applying the proposed approach to column edge detection improves Cell-Aware Segmentation Accuracy (CASA) a layout-aware metric evaluating both textual correctness and correct cell placement from 67% to 76% on the PubLayNet-1M benchmark when using TableNet with PyTesseract OCR. The method is robust to resolution variations through zero-padding and scaling strategies and produces optimized structured tabular outputs suitable for downstream analysis.

</details>


### [51] [AndroidLens: Long-latency Evaluation with Nested Sub-targets for Android GUI Agents](https://arxiv.org/abs/2512.21302)
*Yue Cao,Yingyao Wang,Pi Bu,Jingxuan Xing,Wei Jiang,Zekun Zhu,Junpeng Ma,Sashuai Zhou,Tong Lu,Jun Song,Yu Cheng,Yuning Jiang,Bo Zheng*

Main category: cs.CV

TL;DR: 介绍AndroidLens，一个针对移动GUI agent的评估框架，包含571个中英文长时延任务，平均需要26步完成，覆盖38个领域，包含多约束、多目标等复杂任务类型


<details>
  <summary>Details</summary>
Motivation: 现有的GUI agent评估基准存在应用有限、任务简单、度量粗糙的问题，需要更贴近真实世界复杂场景的评估框架

Method: AndroidLens包含静态评估（保留真实异常、允许多有效路径）和动态评估（里程碑制，通过平均任务进展ATP进行细粒度度量），任务源自现实用户场景

Result: 评估显示即使最佳模型也只有12.7%的任务成功率，ATP为50.47%。分析指出了环境异常、自适应探索和长时记忆保持等关键挑战

Conclusion: AndroidLens是一个具有挑战性的移动GUI agent评估基准，揭示了当前技术在实际复杂场景中的局限性，为未来研究提供了重要方向

Abstract: Graphical user interface (GUI) agents can substantially improve productivity by automating frequently executed long-latency tasks on mobile devices. However, existing evaluation benchmarks are still constrained to limited applications, simple tasks, and coarse-grained metrics. To address this, we introduce AndroidLens, a challenging evaluation framework for mobile GUI agents, comprising 571 long-latency tasks in both Chinese and English environments, each requiring an average of more than 26 steps to complete. The framework features: (1) tasks derived from real-world user scenarios across 38 domains, covering complex types such as multi-constraint, multi-goal, and domain-specific tasks; (2) static evaluation that preserves real-world anomalies and allows multiple valid paths to reduce bias; and (3) dynamic evaluation that employs a milestone-based scheme for fine-grained progress measurement via Average Task Progress (ATP). Our evaluation indicates that even the best models reach only a 12.7% task success rate and 50.47% ATP. We also underscore key challenges in real-world environments, including environmental anomalies, adaptive exploration, and long-term memory retention.

</details>


### [52] [TICON: A Slide-Level Tile Contextualizer for Histopathology Representation Learning](https://arxiv.org/abs/2512.21331)
*Varun Belagali,Saarthak Kapse,Pierre Marza,Srijan Das,Zilinghan Li,Sofiène Boutaj,Pushpak Pati,Srikar Yellapragada,Tarak Nath Nandi,Ravi K Madduri,Joel Saltz,Prateek Prasanna,Stergios Christodoulidis Maria Vakalopoulou,Dimitris Samaras*

Main category: cs.CV

TL;DR: TICON是一种基于transformer的瓦片表示上下文化模型，可为计算病理学中的各种任务生成丰富的上下文化嵌入，解决了传统瓦片编码器丢失上下文信息的问题。


<details>
  <summary>Details</summary>
Motivation: 传统瓦片编码器在提取全切片图像中瓦片嵌入时剥离了上下文信息，无法建模对局部和全局任务都至关重要的丰富切片级信息，且不同瓦片编码器在不同下游任务中表现各异，需要统一模型来上下文化任何瓦片级基础模型得到的嵌入表示。

Method: 使用基于掩码建模目标的单个共享编码器预训练TICON，以同时统一和上下文化来自多样病理瓦片级基础模型的表示。

Result: TICON上下文化嵌入在多个任务中显著提升性能，在瓦片级和切片级基准测试中均建立了新的最先进结果；预训练的TICON聚合器仅使用11K WSI就超越了使用多达350K WSI预训练的先进切片级基础模型。

Conclusion: TICON为计算病理学提供了一个多功能、高效的上下文化嵌入解决方案，能够显著提升多种任务性能，且资源效率更高。

Abstract: The interpretation of small tiles in large whole slide images (WSI) often needs a larger image context. We introduce TICON, a transformer-based tile representation contextualizer that produces rich, contextualized embeddings for ''any'' application in computational pathology. Standard tile encoder-based pipelines, which extract embeddings of tiles stripped from their context, fail to model the rich slide-level information essential for both local and global tasks. Furthermore, different tile-encoders excel at different downstream tasks. Therefore, a unified model is needed to contextualize embeddings derived from ''any'' tile-level foundation model. TICON addresses this need with a single, shared encoder, pretrained using a masked modeling objective to simultaneously unify and contextualize representations from diverse tile-level pathology foundation models. Our experiments demonstrate that TICON-contextualized embeddings significantly improve performance across many different tasks, establishing new state-of-the-art results on tile-level benchmarks (i.e., HEST-Bench, THUNDER, CATCH) and slide-level benchmarks (i.e., Patho-Bench). Finally, we pretrain an aggregator on TICON to form a slide-level foundation model, using only 11K WSIs, outperforming SoTA slide-level foundation models pretrained with up to 350K WSIs.

</details>


### [53] [Fast SAM2 with Text-Driven Token Pruning](https://arxiv.org/abs/2512.21333)
*Avilasha Mandal,Chaoning Zhang,Fachrina Dewi Puspitasari,Xudong Wang,Jiaquan Zhang,Caiyan Qin,Guoqing Wang,Yang Yang,Heng Tao Shen*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Segment Anything Model 2 (SAM2), a vision foundation model has significantly advanced in prompt-driven video object segmentation, yet their practical deployment remains limited by the high computational and memory cost of processing dense visual tokens across time. The SAM2 pipelines typically propagate all visual tokens produced by the image encoder through downstream temporal reasoning modules, regardless of their relevance to the target object, resulting in reduced scalability due to quadratic memory attention overhead. In this work, we introduce a text-guided token pruning framework that improves inference efficiency by selectively reducing token density prior to temporal propagation, without modifying the underlying segmentation architecture. Operating after visual encoding and before memory based propagation, our method ranks tokens using a lightweight routing mechanism that integrates local visual context, semantic relevance derived from object-centric textual descriptions (either user-provided or automatically generated), and uncertainty cues that help preserve ambiguous or boundary critical regions. By retaining only the most informative tokens for downstream processing, the proposed approach reduces redundant computation while maintaining segmentation fidelity. Extensive experiments across multiple challenging video segmentation benchmarks demonstrate that post-encoder token pruning provides a practical and effective pathway to efficient, prompt-aware video segmentation, achieving up to 42.50 percent faster inference and 37.41 percent lower GPU memory usage compared to the unpruned baseline SAM2, while preserving competitive J and F performance. These results highlight the potential of early token selection to improve the scalability of transformer-based video segmentation systems for real-time and resource-constrained applications.

</details>


### [54] [Streaming Video Instruction Tuning](https://arxiv.org/abs/2512.21334)
*Jiaer Xia,Peixian Chen,Mengdan Zhang,Xing Sun,Kaiyang Zhou*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We present Streamo, a real-time streaming video LLM that serves as a general-purpose interactive assistant. Unlike existing online video models that focus narrowly on question answering or captioning, Streamo performs a broad spectrum of streaming video tasks, including real-time narration, action understanding, event captioning, temporal event grounding, and time-sensitive question answering. To develop such versatility, we construct Streamo-Instruct-465K, a large-scale instruction-following dataset tailored for streaming video understanding. The dataset covers diverse temporal contexts and multi-task supervision, enabling unified training across heterogeneous streaming tasks. After training end-to-end on the instruction-following dataset through a streamlined pipeline, Streamo exhibits strong temporal reasoning, responsive interaction, and broad generalization across a variety of streaming benchmarks. Extensive experiments show that Streamo bridges the gap between offline video perception models and real-time multimodal assistants, making a step toward unified, intelligent video understanding in continuous video streams.

</details>


### [55] [Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models](https://arxiv.org/abs/2512.21337)
*Li-Zhong Szu-Tu,Ting-Lin Wu,Chia-Jui Chang,He Syu,Yu-Lun Liu*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We expose a significant popularity bias in state-of-the-art vision-language models (VLMs), which achieve up to 34% higher accuracy on famous buildings compared to ordinary ones, indicating a reliance on memorization over generalizable understanding. To systematically investigate this, we introduce the largest open benchmark for this task: the YearGuessr dataset, a collection of 55,546 building images with multi-modal attributes from 157 countries, annotated with continuous ordinal labels of their construction year (1001-2024), GPS data, and page-view counts as a proxy for popularity. Using this dataset, we frame the construction year prediction task as ordinal regression and introduce popularity-aware interval accuracy metrics to quantify this bias. Our resulting benchmark of 30+ models, including our YearCLIP model, confirms that VLMs excel on popular, memorized items but struggle significantly with unrecognized subjects, exposing a critical flaw in their reasoning capabilities. Project page: https://sytwu.github.io/BeyondMemo/

</details>


### [56] [HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming](https://arxiv.org/abs/2512.21338)
*Haonan Qiu,Shikun Liu,Zijian Zhou,Zhaochong An,Weiming Ren,Zhiheng Liu,Jonas Schult,Sen He,Shoufa Chen,Yuren Cong,Tao Xiang,Ziwei Liu,Juan-Manuel Perez-Rua*

Main category: cs.CV

TL;DR: HiStream 是一个高效的自回归框架，通过空间、时间和时间步压缩三种优化，显著加速高分辨率视频生成，在保持视觉质量的同时实现76.2倍至107.5倍的加速。


<details>
  <summary>Details</summary>
Motivation: 现有高分辨率视频生成方法受扩散模型二次复杂度的计算瓶颈限制，导致推理速度过慢，难以实际应用。需要开发更高效的框架来使高分辨率视频生成变得实用和可扩展。

Method: HiStream采用三轴压缩策略：1）空间压缩：先在低分辨率去噪，然后利用缓存特征在高分辨率精炼；2）时间压缩：采用分块策略和固定大小的锚点缓存，确保稳定的推理速度；3）时间步压缩：对后续缓存条件块应用更少的去噪步骤。HiStream模型包含前两种优化，HiStream+包含全部三种优化。

Result: 在1080p基准测试中，HiStream模型在视觉质量上达到最先进的水平，去噪速度比Wan2.1基线快76.2倍，质量损失可忽略不计。HiStream+实现了107.5倍的加速，在速度和质量之间提供了理想的权衡。

Conclusion: HiStream框架通过系统性减少冗余，显著加速高分辨率视频生成，使这项技术变得实用且可扩展，为实际应用铺平了道路。

Abstract: High-resolution video generation, while crucial for digital media and film, is computationally bottlenecked by the quadratic complexity of diffusion models, making practical inference infeasible. To address this, we introduce HiStream, an efficient autoregressive framework that systematically reduces redundancy across three axes: i) Spatial Compression: denoising at low resolution before refining at high resolution with cached features; ii) Temporal Compression: a chunk-by-chunk strategy with a fixed-size anchor cache, ensuring stable inference speed; and iii) Timestep Compression: applying fewer denoising steps to subsequent, cache-conditioned chunks. On 1080p benchmarks, our primary HiStream model (i+ii) achieves state-of-the-art visual quality while demonstrating up to 76.2x faster denoising compared to the Wan2.1 baseline and negligible quality loss. Our faster variant, HiStream+, applies all three optimizations (i+ii+iii), achieving a 107.5x acceleration over the baseline, offering a compelling trade-off between speed and quality, thereby making high-resolution video generation both practical and scalable.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [57] [Uncovering Competency Gaps in Large Language Models and Their Benchmarks](https://arxiv.org/abs/2512.20638)
*Matyas Bohacek,Nino Scherrer,Nicholas Dufour,Thomas Leung,Christoph Bregler,Stephanie C. Y. Chan*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The evaluation of large language models (LLMs) relies heavily on standardized benchmarks. These benchmarks provide useful aggregated metrics for a given capability, but those aggregated metrics can obscure (i) particular sub-areas where the LLMs are weak ("model gaps") and (ii) imbalanced coverage in the benchmarks themselves ("benchmark gaps"). We propose a new method that uses sparse autoencoders (SAEs) to automatically uncover both types of gaps. By extracting SAE concept activations and computing saliency-weighted performance scores across benchmark data, the method grounds evaluation in the model's internal representations and enables comparison across benchmarks. As examples demonstrating our approach, we applied the method to two popular open-source models and ten benchmarks. We found that these models consistently underperformed on concepts that stand in contrast to sycophantic behaviors (e.g., politely refusing a request or asserting boundaries) and concepts connected to safety discussions. These model gaps align with observations previously surfaced in the literature; our automated, unsupervised method was able to recover them without manual supervision. We also observed benchmark gaps: many of the evaluated benchmarks over-represented concepts related to obedience, authority, or instruction-following, while missing core concepts that should fall within their intended scope. In sum, our method offers a representation-grounded approach to evaluation, enabling concept-level decomposition of benchmark scores. Rather than replacing conventional aggregated metrics, CG complements them by providing a concept-level decomposition that can reveal why a model scored as it did and how benchmarks could evolve to better reflect their intended scope. Code is available at https://competency-gaps.github.io.

</details>


### [58] [SA-DiffuSeq: Addressing Computational and Scalability Challenges in Long-Document Generation with Sparse Attention](https://arxiv.org/abs/2512.20724)
*Alexandros Christoforos,Chadbourne Davis*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Diffusion based approaches to long form text generation suffer from prohibitive computational cost and memory overhead as sequence length increases. We introduce SA-DiffuSeq, a diffusion framework that integrates sparse attention to fundamentally improve scalability for long document modeling. By selectively allocating attention within the diffusion process, SA-DiffuSeq significantly reduces computational complexity while maintaining semantic coherence and generation quality. A key component of our method is a soft absorbing state tailored to sparse attention dynamics, which stabilizes diffusion trajectories and accelerates sequence reconstruction. This design improves sampling efficiency and enhances precision in long range dependency modeling. Extensive experiments demonstrate that SA-DiffuSeq consistently surpasses state of the art diffusion baselines in both training efficiency and sampling speed, with especially strong gains on extended sequences. These properties make SA-DiffuSeq well suited for demanding long form applications such as scientific writing, large scale code generation, and multi turn long context dialogue. Overall, our results indicate that incorporating structured sparsity into diffusion models is a promising direction for efficient and expressive long text generation.

</details>


### [59] [TokSuite: Measuring the Impact of Tokenizer Choice on Language Model Behavior](https://arxiv.org/abs/2512.20757)
*Gül Sena Altıntaş,Malikeh Ehghaghi,Brian Lester,Fengyuan Liu,Wanru Zhao,Marco Ciccone,Colin Raffel*

Main category: cs.CL

TL;DR: 通过构建同一架构、数据集和训练预算但采用不同分词器的14个模型及新基准，研究分词器对语言模型性能和行为的独立影响。


<details>
  <summary>Details</summary>
Motivation: 分词器是语言模型处理文本的基础，但其对模型性能和行为的影响难以独立评估，因此需要工具来分离分词器的影响并深入理解其作用。

Method: 开发TokSuite工具包，包含14个仅分词器不同其他方面完全一致的模型，并创建专门基准测试，测量分词器在真实世界扰动下的模型表现。

Result: 该方法能够稳健地分离分词器的影响，揭示了多种流行分词器各自的优势和不足。

Conclusion: TokSuite为分词器研究提供了系统化的评估框架，帮助深入理解分词器对语言模型性能的影响机制。

Abstract: Tokenizers provide the fundamental basis through which text is represented and processed by language models (LMs). Despite the importance of tokenization, its role in LM performance and behavior is poorly understood due to the challenge of measuring the impact of tokenization in isolation. To address this need, we present TokSuite, a collection of models and a benchmark that supports research into tokenization's influence on LMs. Specifically, we train fourteen models that use different tokenizers but are otherwise identical using the same architecture, dataset, training budget, and initialization. Additionally, we curate and release a new benchmark that specifically measures model performance subject to real-world perturbations that are likely to influence tokenization. Together, TokSuite allows robust decoupling of the influence of a model's tokenizer, supporting a series of novel findings that elucidate the respective benefits and shortcomings of a wide range of popular tokenizers.

</details>


### [60] [Adversarial Training for Failure-Sensitive User Simulation in Mental Health Dialogue Optimization](https://arxiv.org/abs/2512.20773)
*Ziyi Zhu,Olivier Tieleman,Caitlin A. Stamatis,Luka Smyth,Thomas D. Hull,Daniel R. Cahn,Matteo Malgaroli*

Main category: cs.CL

TL;DR: 提出一种对抗训练框架，通过生成器（用户模拟器）和判别器之间的竞争动态迭代提升用户模拟器的真实性，应用于心理健康支持聊天机器人领域。


<details>
  <summary>Details</summary>
Motivation: 现实用户模拟对训练和评估面向任务的对话系统至关重要，但创建能准确模拟人类行为的模拟器仍具挑战性。有效模拟器的关键特性是能够暴露被评估系统的故障模式。

Method: 采用对抗训练框架，通过生成器（用户模拟器）和判别器之间的竞争动态迭代改进用户模拟器的真实性。将该方法应用于心理健康支持聊天机器人评估。

Result: 微调模拟器在暴露系统问题方面显著优于零-shot基础模型；对抗训练进一步增强了多样性、分布对齐和预测有效性；模拟器在不同聊天机器人配置下实现了模拟与真实故障发生率之间的强相关性，同时保持低分布发散；判别器准确率在三次对抗迭代后急剧下降，表明模拟器真实性得到提升。

Conclusion: 对抗训练是创建心理健康支持领域中真实用户模拟器的有前景方法，能够在部署前实现快速、可靠、经济高效的系统评估。

Abstract: Realistic user simulation is crucial for training and evaluating task-oriented dialogue (TOD) systems, yet creating simulators that accurately replicate human behavior remains challenging. A key property of effective simulators is their ability to expose failure modes of the systems they evaluate. We present an adversarial training framework that iteratively improves user simulator realism through a competitive dynamic between a generator (user simulator) and a discriminator. Applied to mental health support chatbots, our approach demonstrates that fine-tuned simulators dramatically outperform zero-shot base models at surfacing system issues, and adversarial training further enhances diversity, distributional alignment, and predictive validity. The resulting simulator achieves a strong correlation between simulated and real failure occurrence rates across diverse chatbot configurations while maintaining low distributional divergence of failure modes. Discriminator accuracy decreases drastically after three adversarial iterations, suggesting improved realism. These results provide evidence that adversarial training is a promising approach for creating realistic user simulators in mental health support TOD domains, enabling rapid, reliable, and cost-effective system evaluation before deployment.

</details>


### [61] [Large Language Models Approach Expert Pedagogical Quality in Math Tutoring but Differ in Instructional and Linguistic Profiles](https://arxiv.org/abs/2512.20780)
*Ramatu Oiza Abdulsalam,Segun Aroyehun*

Main category: cs.CL

TL;DR: 本研究通过实验比较了专家人类导师、新手人类导师和大语言模型在数学辅导任务中的表现，发现大语言模型在感知教学质量方面已接近专家水平，但在教学策略和语言特征方面存在系统性差异。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在数学辅导中的教学行为是否与专家人类导师的实际实践相符，评估其教学质量和策略差异，为智能辅导系统的发展提供指导。

Method: 采用受控的回合级比较方法，让专家人类导师、新手人类导师和多个大语言模型分别对同一组数学辅导对话回合作出回应，分析其教学策略和语言特征，包括重述和转述、准确性追问、词汇多样性、可读性、礼貌性和能动性等方面。

Result: 大语言模型的感知教学质量平均接近专家水平，但存在系统性差异：少用专家导师特有的重述和转述策略，回应更长、词汇更丰富、更礼貌。统计分析显示，重述和转述、词汇多样性、准确性追问与感知教学质量正相关，而高能动性和礼貌语言则负相关。

Conclusion: 近期大语言模型在感知教学质量方面已达到专家人类导师相当水平，但依赖不同的教学和语言策略。研究强调了分析教学策略和语言特征在评估人类导师与智能辅导系统回应时的重要性。

Abstract: Recent work has explored the use of large language models for generating tutoring responses in mathematics, yet it remains unclear how closely their instructional behavior aligns with expert human practice. We examine this question using a controlled, turn-level comparison in which expert human tutors, novice human tutors, and multiple large language models respond to the same set of math remediation conversation turns. We examine both instructional strategies and linguistic characteristics of tutoring responses, including restating and revoicing, pressing for accuracy, lexical diversity, readability, politeness, and agency. We find that large language models approach expert levels of perceived pedagogical quality on average but exhibit systematic differences in their instructional and linguistic profiles. In particular, large language models tend to underuse restating and revoicing strategies characteristic of expert human tutors, while producing longer, more lexically diverse, and more polite responses. Statistical analyses show that restating and revoicing, lexical diversity, and pressing for accuracy are positively associated with perceived pedagogical quality, whereas higher levels of agentic and polite language are negatively associated. Overall, recent large language models exhibit levels of perceived pedagogical quality comparable to expert human tutors, while relying on different instructional and linguistic strategies. These findings underscore the value of analyzing instructional strategies and linguistic characteristics when evaluating tutoring responses across human tutors and intelligent tutoring systems.

</details>


### [62] [Investigating Model Editing for Unlearning in Large Language Models](https://arxiv.org/abs/2512.20794)
*Shariqah Hossain,Lalana Kagal*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Machine unlearning aims to remove unwanted information from a model, but many methods are inefficient for LLMs with large numbers of parameters or fail to fully remove the intended information without degrading performance on knowledge that should be retained. Model editing algorithms solve a similar problem of changing information in models, but they focus on redirecting inputs to a new target rather than removing that information altogether. In this work, we explore the editing algorithms ROME, IKE, and WISE and design new editing targets for an unlearning setting. Through this investigation, we show that model editing approaches can exceed baseline unlearning methods in terms of quality of forgetting depending on the setting. Like traditional unlearning techniques, they struggle to encapsulate the scope of what is to be unlearned without damage to the overall model performance.

</details>


### [63] [Measuring Mechanistic Independence: Can Bias Be Removed Without Erasing Demographics?](https://arxiv.org/abs/2512.20796)
*Zhengyang Shan,Aaron Mueller*

Main category: cs.CL

TL;DR: 本研究探讨了语言模型中人口统计偏倚机制与一般人口统计识别的独立性，通过多任务评估发现，基于归因的稀疏自编码器特征消除可减少职业中的种族和性别刻板印象，但不影响姓名识别精度；而基于相关性的方法对教育偏倚更有效。


<details>
  <summary>Details</summary>
Motivation: 研究旨在厘清语言模型中人口统计偏倚机制与一般人口统计识别能力之间的关系，探索是否能在消除偏倚的同时保持模型对人口统计特征的识别能力，为精确的消偏干预提供理论基础。

Method: 采用多任务评估框架，将人口统计特征与姓名、职业和教育水平关联；对比基于归因和基于相关性的方法来定位偏倚特征；使用目标化的稀疏自编码器特征消除进行干预实验。

Result: 归因性特征消除能有效减轻职业中的种族和性别刻板印象，且保持姓名识别准确性；相关性特征消除对教育偏倚更有效；但在教育任务中移除归因特征会引发“先验崩溃”，反而增加整体偏倚。

Conclusion: 人口统计偏倚源自任务特定机制而非绝对的人口统计标记，基于机制的推理时干预可实现外科手术式的消偏，且不损害模型核心能力，但需要针对特定维度设计干预措施。

Abstract: We investigate how independent demographic bias mechanisms are from general demographic recognition in language models. Using a multi-task evaluation setup where demographics are associated with names, professions, and education levels, we measure whether models can be debiased while preserving demographic detection capabilities. We compare attribution-based and correlation-based methods for locating bias features. We find that targeted sparse autoencoder feature ablations in Gemma-2-9B reduce bias without degrading recognition performance: attribution-based ablations mitigate race and gender profession stereotypes while preserving name recognition accuracy, whereas correlation-based ablations are more effective for education bias. Qualitative analysis further reveals that removing attribution features in education tasks induces ``prior collapse'', thus increasing overall bias. This highlights the need for dimension-specific interventions. Overall, our results show that demographic bias arises from task-specific mechanisms rather than absolute demographic markers, and that mechanistic inference-time interventions can enable surgical debiasing without compromising core model capabilities.

</details>


### [64] [Semantic Deception: When Reasoning Models Can't Compute an Addition](https://arxiv.org/abs/2512.20812)
*Nathaniël de Leeuw,Marceau Nahon,Mathis Reymond,Raja Chatila,Mehdi Khamassi*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language models (LLMs) are increasingly used in situations where human values are at stake, such as decision-making tasks that involve reasoning when performed by humans. We investigate the so-called reasoning capabilities of LLMs over novel symbolic representations by introducing an experimental framework that tests their ability to process and manipulate unfamiliar symbols. We introduce semantic deceptions: situations in which symbols carry misleading semantic associations due to their form, such as being embedded in specific contexts, designed to probe whether LLMs can maintain symbolic abstraction or whether they default to exploiting learned semantic associations. We redefine standard digits and mathematical operators using novel symbols, and task LLMs with solving simple calculations expressed in this altered notation. The objective is: (1) to assess LLMs' capacity for abstraction and manipulation of arbitrary symbol systems; (2) to evaluate their ability to resist misleading semantic cues that conflict with the task's symbolic logic. Through experiments with four LLMs we show that semantic cues can significantly deteriorate reasoning models' performance on very simple tasks. They reveal limitations in current LLMs' ability for symbolic manipulations and highlight a tendency to over-rely on surface-level semantics, suggesting that chain-of-thoughts may amplify reliance on statistical correlations. Even in situations where LLMs seem to correctly follow instructions, semantic cues still impact basic capabilities. These limitations raise ethical and societal concerns, undermining the widespread and pernicious tendency to attribute reasoning abilities to LLMs and suggesting how LLMs might fail, in particular in decision-making contexts where robust symbolic reasoning is essential and should not be compromised by residual semantic associations inherited from the model's training.

</details>


### [65] [EssayCBM: Rubric-Aligned Concept Bottleneck Models for Transparent Essay Grading](https://arxiv.org/abs/2512.20817)
*Kumar Satvik Chaudhary,Chengshuai Zhao,Fan Zhang,Yung Hin Tse,Garima Agrawal,Yuli Deng,Huan Liu*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Understanding how automated grading systems evaluate essays remains a significant challenge for educators and students, especially when large language models function as black boxes. We introduce EssayCBM, a rubric-aligned framework that prioritizes interpretability in essay assessment. Instead of predicting grades directly from text, EssayCBM evaluates eight writing concepts, such as Thesis Clarity and Evidence Use, through dedicated prediction heads on an encoder. These concept scores form a transparent bottleneck, and a lightweight network computes the final grade using only concepts. Instructors can adjust concept predictions and instantly view the updated grade, enabling accountable human-in-the-loop evaluation. EssayCBM matches black-box performance while offering actionable, concept-level feedback through an intuitive web interface.

</details>


### [66] [MediEval: A Unified Medical Benchmark for Patient-Contextual and Knowledge-Grounded Reasoning in LLMs](https://arxiv.org/abs/2512.20822)
*Zhan Qu,Michael Färber*

Main category: cs.CL

TL;DR: 这篇论文提出了MediEval基准，将MIMIC-IV电子健康记录与统一知识库连接，用于评估大语言模型在医疗领域的可靠性和安全性。研究发现LLMs存在幻觉支持和真相反转等关键失效模式，并提出反事实风险感知微调方法改善这些问题。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在医学领域应用日益增多，但其采用受到可靠性和安全性担忧的限制。现有的评估要么孤立地测试医学事实知识，要么评估患者层面推理而不验证正确性，存在关键差距。需要建立一个联合考虑知识基础和上下文一致性的评估框架。

Method: 1. 提出MediEval基准：连接MIMIC-IV电子健康记录与从UMLS和其他生物医学词汇表构建的统一知识库，在真实患者情境中生成多样的事实陈述和反事实医学陈述；2. 建立4象限评估框架：联合考虑知识基础和上下文一致性；3. 提出反事实风险感知微调：基于DPO的方法，采用针对不安全混淆的不对称惩罚机制。

Result: 1. 识别出现有专有、开源和领域特定大语言模型的critical failure modes，包括幻觉支持和真相反转；2. CoRFu方法相比基础模型提升16.4宏观F1分数，消除真相反转错误，同时实现更高的准确性和显著改善的安全性。

Conclusion: MediEval基准揭示了当前医疗领域大语言模型的关键失效模式。提出的反事实风险感知微调方法CoRFu能够显著改善模型在医疗场景中的准确性和安全性，解决了现有评估方法的不足，为医疗AI系统的可靠部署提供了重要工具和方法。

Abstract: Large Language Models (LLMs) are increasingly applied to medicine, yet their adoption is limited by concerns over reliability and safety. Existing evaluations either test factual medical knowledge in isolation or assess patient-level reasoning without verifying correctness, leaving a critical gap. We introduce MediEval, a benchmark that links MIMIC-IV electronic health records (EHRs) to a unified knowledge base built from UMLS and other biomedical vocabularies. MediEval generates diverse factual and counterfactual medical statements within real patient contexts, enabling systematic evaluation across a 4-quadrant framework that jointly considers knowledge grounding and contextual consistency. Using this framework, we identify critical failure modes, including hallucinated support and truth inversion, that current proprietary, open-source, and domain-specific LLMs frequently exhibit. To address these risks, we propose Counterfactual Risk-Aware Fine-tuning (CoRFu), a DPO-based method with an asymmetric penalty targeting unsafe confusions. CoRFu improves by +16.4 macro-F1 points over the base model and eliminates truth inversion errors, demonstrating both higher accuracy and substantially greater safety.

</details>


### [67] [Nemotron 3 Nano: Open, Efficient Mixture-of-Experts Hybrid Mamba-Transformer Model for Agentic Reasoning](https://arxiv.org/abs/2512.20848)
*NVIDIA,:,Aaron Blakeman,Aaron Grattafiori,Aarti Basant,Abhibha Gupta,Abhinav Khattar,Adi Renduchintala,Aditya Vavre,Akanksha Shukla,Akhiad Bercovich,Aleksander Ficek,Aleksandr Shaposhnikov,Alex Kondratenko,Alexander Bukharin,Alexandre Milesi,Ali Taghibakhshi,Alisa Liu,Amelia Barton,Ameya Sunil Mahabaleshwarkar,Amir Klein,Amit Zuker,Amnon Geifman,Amy Shen,Anahita Bhiwandiwalla,Andrew Tao,Ann Guan,Anubhav Mandarwal,Arham Mehta,Ashwath Aithal,Ashwin Poojary,Asif Ahamed,Asma Kuriparambil Thekkumpate,Ayush Dattagupta,Banghua Zhu,Bardiya Sadeghi,Barnaby Simkin,Ben Lanir,Benedikt Schifferer,Besmira Nushi,Bilal Kartal,Bita Darvish Rouhani,Boris Ginsburg,Brandon Norick,Brandon Soubasis,Branislav Kisacanin,Brian Yu,Bryan Catanzaro,Carlo del Mundo,Chantal Hwang,Charles Wang,Cheng-Ping Hsieh,Chenghao Zhang,Chenhan Yu,Chetan Mungekar,Chintan Patel,Chris Alexiuk,Christopher Parisien,Collin Neale,Damon Mosk-Aoyama,Dan Su,Dane Corneil,Daniel Afrimi,Daniel Rohrer,Daniel Serebrenik,Daria Gitman,Daria Levy,Darko Stosic,David Mosallanezhad,Deepak Narayanan,Dhruv Nathawani,Dima Rekesh,Dina Yared,Divyanshu Kakwani,Dong Ahn,Duncan Riach,Dusan Stosic,Edgar Minasyan,Edward Lin,Eileen Long,Eileen Peters Long,Elena Lantz,Ellie Evans,Elliott Ning,Eric Chung,Eric Harper,Eric Tramel,Erick Galinkin,Erik Pounds,Evan Briones,Evelina Bakhturina,Faisal Ladhak,Fay Wang,Fei Jia,Felipe Soares,Feng Chen,Ferenc Galko,Frankie Siino,Gal Hubara Agam,Ganesh Ajjanagadde,Gantavya Bhatt,Gargi Prasad,George Armstrong,Gerald Shen,Gorkem Batmaz,Grigor Nalbandyan,Haifeng Qian,Harsh Sharma,Hayley Ross,Helen Ngo,Herman Sahota,Hexin Wang,Himanshu Soni,Hiren Upadhyay,Huizi Mao,Huy C Nguyen,Huy Q Nguyen,Iain Cunningham,Ido Shahaf,Igor Gitman,Ilya Loshchilov,Ivan Moshkov,Izzy Putterman,Jan Kautz,Jane Polak Scowcroft,Jared Casper,Jatin Mitra,Jeffrey Glick,Jenny Chen,Jesse Oliver,Jian Zhang,Jiaqi Zeng,Jie Lou,Jimmy Zhang,Jining Huang,Joey Conway,Joey Guman,John Kamalu,Johnny Greco,Jonathan Cohen,Joseph Jennings,Joyjit Daw,Julien Veron Vialard,Junkeun Yi,Jupinder Parmar,Kai Xu,Kan Zhu,Kari Briski,Katherine Cheung,Katherine Luna,Keshav Santhanam,Kevin Shih,Kezhi Kong,Khushi Bhardwaj,Krishna C. Puvvada,Krzysztof Pawelec,Kumar Anik,Lawrence McAfee,Laya Sleiman,Leon Derczynski,Li Ding,Lucas Liebenwein,Luis Vega,Maanu Grover,Maarten Van Segbroeck,Maer Rodrigues de Melo,Makesh Narsimhan Sreedhar,Manoj Kilaru,Maor Ashkenazi,Marc Romeijn,Mark Cai,Markus Kliegl,Maryam Moosaei,Matvei Novikov,Mehrzad Samadi,Melissa Corpuz,Mengru Wang,Meredith Price,Michael Boone,Michael Evans,Miguel Martinez,Mike Chrzanowski,Mohammad Shoeybi,Mostofa Patwary,Nabin Mulepati,Natalie Hereth,Nave Assaf,Negar Habibi,Neta Zmora,Netanel Haber,Nicola Sessions,Nidhi Bhatia,Nikhil Jukar,Nikki Pope,Nikolai Ludwig,Nima Tajbakhsh,Nirmal Juluru,Oleksii Hrinchuk,Oleksii Kuchaiev,Olivier Delalleau,Oluwatobi Olabiyi,Omer Ullman Argov,Ouye Xie,Parth Chadha,Pasha Shamis,Pavlo Molchanov,Pawel Morkisz,Peter Dykas,Peter Jin,Pinky Xu,Piotr Januszewski,Pranav Prashant Thombre,Prasoon Varshney,Pritam Gundecha,Qing Miao,Rabeeh Karimi Mahabadi,Ran El-Yaniv,Ran Zilberstein,Rasoul Shafipour,Rich Harang,Rick Izzo,Rima Shahbazyan,Rishabh Garg,Ritika Borkar,Ritu Gala,Riyad Islam,Roger Waleffe,Rohit Watve,Roi Koren,Ruoxi Zhang,Russell J. Hewett,Ryan Prenger,Ryan Timbrook,Sadegh Mahdavi,Sahil Modi,Samuel Kriman,Sanjay Kariyappa,Sanjeev Satheesh,Saori Kaji,Satish Pasumarthi,Sean Narentharen,Sean Narenthiran,Seonmyeong Bak,Sergey Kashirsky,Seth Poulos,Shahar Mor,Shanmugam Ramasamy,Shantanu Acharya,Shaona Ghosh,Sharath Turuvekere Sreenivas,Shelby Thomas,Shiqing Fan,Shreya Gopal,Shrimai Prabhumoye,Shubham Pachori,Shubham Toshniwal,Shuoyang Ding,Siddharth Singh,Simeng Sun,Smita Ithape,Somshubra Majumdar,Soumye Singhal,Stefania Alborghetti,Stephen Ge,Sugam Dipak Devare,Sumeet Kumar Barua,Suseella Panguluri,Suyog Gupta,Sweta Priyadarshi,Syeda Nahida Akter,Tan Bui,Teodor-Dumitru Ene,Terry Kong,Thanh Do,Tijmen Blankevoort,Tom Balough,Tomer Asida,Tomer Bar Natan,Tugrul Konuk,Twinkle Vashishth,Udi Karpas,Ushnish De,Vahid Noorozi,Vahid Noroozi,Venkat Srinivasan,Venmugil Elango,Vijay Korthikanti,Vitaly Kurin,Vitaly Lavrukhin,Wanli Jiang,Wasi Uddin Ahmad,Wei Du,Wei Ping,Wenfei Zhou,Will Jennings,William Zhang,Wojciech Prazuch,Xiaowei Ren,Yashaswi Karnati,Yejin Choi,Yev Meyer,Yi-Fu Wu,Yian Zhang,Ying Lin,Yonatan Geifman,Yonggan Fu,Yoshi Subara,Yoshi Suhara,Yubo Gao,Zach Moshe,Zhen Dong,Zihan Liu,Zijia Chen,Zijie Yan*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We present Nemotron 3 Nano 30B-A3B, a Mixture-of-Experts hybrid Mamba-Transformer language model. Nemotron 3 Nano was pretrained on 25 trillion text tokens, including more than 3 trillion new unique tokens over Nemotron 2, followed by supervised fine tuning and large-scale RL on diverse environments. Nemotron 3 Nano achieves better accuracy than our previous generation Nemotron 2 Nano while activating less than half of the parameters per forward pass. It achieves up to 3.3x higher inference throughput than similarly-sized open models like GPT-OSS-20B and Qwen3-30B-A3B-Thinking-2507, while also being more accurate on popular benchmarks. Nemotron 3 Nano demonstrates enhanced agentic, reasoning, and chat abilities and supports context lengths up to 1M tokens. We release both our pretrained Nemotron 3 Nano 30B-A3B Base and post-trained Nemotron 3 Nano 30B-A3B checkpoints on Hugging Face.

</details>


### [68] [How important is Recall for Measuring Retrieval Quality?](https://arxiv.org/abs/2512.20854)
*Shelly Schwartz,Oleg Vasilyev,Randy Sawaya*

Main category: cs.CL

TL;DR: 该研究评估了在未知相关文档总数场景下，如何通过相关度指标与LLM评估之间的相关性来衡量检索质量，并提出了一个不依赖相关文档总数信息的新指标。


<details>
  <summary>Details</summary>
Motivation: 在实际检索系统中，知识库庞大且持续更新，相关文档总数往往未知，无法计算召回率等传统指标。

Method: 评估多种现有应对策略，通过测量检索质量指标与LLM基于检索结果生成的响应质量评估之间的相关性。在多个数据集上进行实验（2-15个相关文档），并提出了一个简单的新检索质量度量方法。

Result: 研究表明，提出的新度量方法在不依赖相关文档总数信息的情况下表现良好。

Conclusion: 为现实检索场景下评估检索质量提供了有效方法，特别是当相关文档总数未知时。

Abstract: In realistic retrieval settings with large and evolving knowledge bases, the total number of documents relevant to a query is typically unknown, and recall cannot be computed. In this paper, we evaluate several established strategies for handling this limitation by measuring the correlation between retrieval quality metrics and LLM-based judgments of response quality, where responses are generated from the retrieved documents. We conduct experiments across multiple datasets with a relatively low number of relevant documents (2-15). We also introduce a simple retrieval quality measure that performs well without requiring knowledge of the total number of relevant documents.

</details>


### [69] [NVIDIA Nemotron 3: Efficient and Open Intelligence](https://arxiv.org/abs/2512.20856)
*NVIDIA,:,Aaron Blakeman,Aaron Grattafiori,Aarti Basant,Abhibha Gupta,Abhinav Khattar,Adi Renduchintala,Aditya Vavre,Akanksha Shukla,Akhiad Bercovich,Aleksander Ficek,Aleksandr Shaposhnikov,Alex Kondratenko,Alexander Bukharin,Alexandre Milesi,Ali Taghibakhshi,Alisa Liu,Amelia Barton,Ameya Sunil Mahabaleshwarkar,Amir Klein,Amit Zuker,Amnon Geifman,Amy Shen,Anahita Bhiwandiwalla,Andrew Tao,Anjulie Agrusa,Ankur Verma,Ann Guan,Anubhav Mandarwal,Arham Mehta,Ashwath Aithal,Ashwin Poojary,Asif Ahamed,Asit Mishra,Asma Kuriparambil Thekkumpate,Ayush Dattagupta,Banghua Zhu,Bardiya Sadeghi,Barnaby Simkin,Ben Lanir,Benedikt Schifferer,Besmira Nushi,Bilal Kartal,Bita Darvish Rouhani,Boris Ginsburg,Brandon Norick,Brandon Soubasis,Branislav Kisacanin,Brian Yu,Bryan Catanzaro,Carlo del Mundo,Chantal Hwang,Charles Wang,Cheng-Ping Hsieh,Chenghao Zhang,Chenhan Yu,Chetan Mungekar,Chintan Patel,Chris Alexiuk,Christopher Parisien,Collin Neale,Cyril Meurillon,Damon Mosk-Aoyama,Dan Su,Dane Corneil,Daniel Afrimi,Daniel Lo,Daniel Rohrer,Daniel Serebrenik,Daria Gitman,Daria Levy,Darko Stosic,David Mosallanezhad,Deepak Narayanan,Dhruv Nathawani,Dima Rekesh,Dina Yared,Divyanshu Kakwani,Dong Ahn,Duncan Riach,Dusan Stosic,Edgar Minasyan,Edward Lin,Eileen Long,Eileen Peters Long,Elad Segal,Elena Lantz,Ellie Evans,Elliott Ning,Eric Chung,Eric Harper,Eric Tramel,Erick Galinkin,Erik Pounds,Evan Briones,Evelina Bakhturina,Evgeny Tsykunov,Faisal Ladhak,Fay Wang,Fei Jia,Felipe Soares,Feng Chen,Ferenc Galko,Frank Sun,Frankie Siino,Gal Hubara Agam,Ganesh Ajjanagadde,Gantavya Bhatt,Gargi Prasad,George Armstrong,Gerald Shen,Gorkem Batmaz,Grigor Nalbandyan,Haifeng Qian,Harsh Sharma,Hayley Ross,Helen Ngo,Herbert Hum,Herman Sahota,Hexin Wang,Himanshu Soni,Hiren Upadhyay,Huizi Mao,Huy C Nguyen,Huy Q Nguyen,Iain Cunningham,Ido Galil,Ido Shahaf,Igor Gitman,Ilya Loshchilov,Itamar Schen,Itay Levy,Ivan Moshkov,Izik Golan,Izzy Putterman,Jan Kautz,Jane Polak Scowcroft,Jared Casper,Jatin Mitra,Jeffrey Glick,Jenny Chen,Jesse Oliver,Jian Zhang,Jiaqi Zeng,Jie Lou,Jimmy Zhang,Jinhang Choi,Jining Huang,Joey Conway,Joey Guman,John Kamalu,Johnny Greco,Jonathan Cohen,Joseph Jennings,Joyjit Daw,Julien Veron Vialard,Junkeun Yi,Jupinder Parmar,Kai Xu,Kan Zhu,Kari Briski,Katherine Cheung,Katherine Luna,Keith Wyss,Keshav Santhanam,Kevin Shih,Kezhi Kong,Khushi Bhardwaj,Kirthi Shankar,Krishna C. Puvvada,Krzysztof Pawelec,Kumar Anik,Lawrence McAfee,Laya Sleiman,Leon Derczynski,Li Ding,Lizzie Wei,Lucas Liebenwein,Luis Vega,Maanu Grover,Maarten Van Segbroeck,Maer Rodrigues de Melo,Mahdi Nazemi,Makesh Narsimhan Sreedhar,Manoj Kilaru,Maor Ashkenazi,Marc Romeijn,Marcin Chochowski,Mark Cai,Markus Kliegl,Maryam Moosaei,Matt Kulka,Matvei Novikov,Mehrzad Samadi,Melissa Corpuz,Mengru Wang,Meredith Price,Michael Andersch,Michael Boone,Michael Evans,Miguel Martinez,Mikail Khona,Mike Chrzanowski,Minseok Lee,Mohammad Dabbah,Mohammad Shoeybi,Mostofa Patwary,Nabin Mulepati,Najeeb Nabwani,Natalie Hereth,Nave Assaf,Negar Habibi,Neta Zmora,Netanel Haber,Nicola Sessions,Nidhi Bhatia,Nikhil Jukar,Nikki Pope,Nikolai Ludwig,Nima Tajbakhsh,Nir Ailon,Nirmal Juluru,Nishant Sharma,Oleksii Hrinchuk,Oleksii Kuchaiev,Olivier Delalleau,Oluwatobi Olabiyi,Omer Ullman Argov,Omri Puny,Oren Tropp,Ouye Xie,Parth Chadha,Pasha Shamis,Paul Gibbons,Pavlo Molchanov,Pawel Morkisz,Peter Dykas,Peter Jin,Pinky Xu,Piotr Januszewski,Pranav Prashant Thombre,Prasoon Varshney,Pritam Gundecha,Przemek Tredak,Qing Miao,Qiyu Wan,Rabeeh Karimi Mahabadi,Rachit Garg,Ran El-Yaniv,Ran Zilberstein,Rasoul Shafipour,Rich Harang,Rick Izzo,Rima Shahbazyan,Rishabh Garg,Ritika Borkar,Ritu Gala,Riyad Islam,Robert Hesse,Roger Waleffe,Rohit Watve,Roi Koren,Ruoxi Zhang,Russell Hewett,Russell J. Hewett,Ryan Prenger,Ryan Timbrook,Sadegh Mahdavi,Sahil Modi,Samuel Kriman,Sangkug Lim,Sanjay Kariyappa,Sanjeev Satheesh,Saori Kaji,Satish Pasumarthi,Saurav Muralidharan,Sean Narentharen,Sean Narenthiran,Seonmyeong Bak,Sergey Kashirsky,Seth Poulos,Shahar Mor,Shanmugam Ramasamy,Shantanu Acharya,Shaona Ghosh,Sharath Turuvekere Sreenivas,Shelby Thomas,Shiqing Fan,Shreya Gopal,Shrimai Prabhumoye,Shubham Pachori,Shubham Toshniwal,Shuoyang Ding,Siddharth Singh,Simeng Sun,Smita Ithape,Somshubra Majumdar,Soumye Singhal,Stas Sergienko,Stefania Alborghetti,Stephen Ge,Sugam Dipak Devare,Sumeet Kumar Barua,Suseella Panguluri,Suyog Gupta,Sweta Priyadarshi,Syeda Nahida Akter,Tan Bui,Teodor-Dumitru Ene,Terry Kong,Thanh Do,Tijmen Blankevoort,Tim Moon,Tom Balough,Tomer Asida,Tomer Bar Natan,Tomer Ronen,Tugrul Konuk,Twinkle Vashishth,Udi Karpas,Ushnish De,Vahid Noorozi,Vahid Noroozi,Venkat Srinivasan,Venmugil Elango,Victor Cui,Vijay Korthikanti,Vinay Rao,Vitaly Kurin,Vitaly Lavrukhin,Vladimir Anisimov,Wanli Jiang,Wasi Uddin Ahmad,Wei Du,Wei Ping,Wenfei Zhou,Will Jennings,William Zhang,Wojciech Prazuch,Xiaowei Ren,Yashaswi Karnati,Yejin Choi,Yev Meyer,Yi-Fu Wu,Yian Zhang,Yigong Qin,Ying Lin,Yonatan Geifman,Yonggan Fu,Yoshi Subara,Yoshi Suhara,Yubo Gao,Zach Moshe,Zhen Dong,Zhongbo Zhu,Zihan Liu,Zijia Chen,Zijie Yan*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We introduce the Nemotron 3 family of models - Nano, Super, and Ultra. These models deliver strong agentic, reasoning, and conversational capabilities. The Nemotron 3 family uses a Mixture-of-Experts hybrid Mamba-Transformer architecture to provide best-in-class throughput and context lengths of up to 1M tokens. Super and Ultra models are trained with NVFP4 and incorporate LatentMoE, a novel approach that improves model quality. The two larger models also include MTP layers for faster text generation. All Nemotron 3 models are post-trained using multi-environment reinforcement learning enabling reasoning, multi-step tool use, and support granular reasoning budget control. Nano, the smallest model, outperforms comparable models in accuracy while remaining extremely cost-efficient for inference. Super is optimized for collaborative agents and high-volume workloads such as IT ticket automation. Ultra, the largest model, provides state-of-the-art accuracy and reasoning performance. Nano is released together with its technical report and this white paper, while Super and Ultra will follow in the coming months. We will openly release the model weights, pre- and post-training software, recipes, and all data for which we hold redistribution rights.

</details>


### [70] [Architectural Trade-offs in Small Language Models Under Compute Constraints](https://arxiv.org/abs/2512.20877)
*Shivraj Singh Bhatti*

Main category: cs.CL

TL;DR: 对小规模语言模型在严格计算约束下的系统实证研究，分析架构选择与训练预算如何影响性能。从线性下一词预测器开始，逐步引入非线性、自注意力和多层Transformer架构，在Tiny Shakespeare（字符级）和PTB/WikiText-2（词级）上评估。


<details>
  <summary>Details</summary>
Motivation: 现有的语言模型研究多集中于大规模模型，但小规模模型在资源受限环境中有实际应用价值。缺乏对小规模模型在严格计算约束下的系统实证研究，特别是不同架构选择如何与训练预算相互作用影响效率-准确率权衡。

Method: 从简单的线性下一词预测器开始，逐步增加模型复杂度：引入非线性激活函数、自注意力机制、多层Transformer。使用字符级别（Tiny Shakespeare）和词级别（PTB、WikiText-2）进行评估。通过测试负对数似然（NLL）、参数数量和近似训练FLOPs来衡量模型性能与效率。

Result: 基于注意力的模型即使在小规模下也显著优于MLP模型，在每FLOP效率上明显更好。增加深度或上下文长度如果没有足够的优化训练预算，反而会降低性能。研究发现RoPE等在大语言模型中成功的技术不一定适用于小规模模型架构。

Conclusion: 小规模语言模型的设计需要不同于大规模模型的考量，简单扩展大型模型的成功技术可能不适用。注意力机制即使在小规模下仍然保持计算效率优势，但增加模型复杂度必须与训练预算相匹配，否则可能导致性能下降。

Abstract: We present a systematic empirical study of small language models under strict compute constraints, analyzing how architectural choices and training budget interact to determine performance. Starting from a linear next-token predictor, we progressively introduce nonlinearities, self-attention, and multi-layer transformer architectures, evaluating each on character-level modeling of Tiny Shakespeare and word-level modeling of Penn Treebank (PTB) and WikiText-2. We compare models using test negative log-likelihood (NLL), parameter count, and approximate training FLOPs to characterize accuracy-efficiency trade-offs. Our results show that attention-based models dominate MLPs in per-FLOP efficiency even at small scale, while increasing depth or context without sufficient optimization can degrade performance. We further examine rotary positional embeddings (RoPE), finding that architectural techniques successful in large language models do not necessarily transfer to small-model regimes.

</details>


### [71] [Where Did This Sentence Come From? Tracing Provenance in LLM Reasoning Distillation](https://arxiv.org/abs/2512.20908)
*Kaiyuan Liu,Shaotian Yan,Rui Miao,Bing Wang,Chen Shen,Jun Zhang,Jieping Ye*

Main category: cs.CL

TL;DR: 本文提出了交叉模型推理蒸馏溯源框架，通过系统追踪中间动作来源证明了学生在测试时能生成来源老师的动作并影响性能，并提出了基于师生差异的数据选择方法提升蒸馏效果。


<details>
  <summary>Details</summary>
Motivation: 先前推理蒸馏方法缺乏对蒸馏模型能力来源的深入分析，不清楚学生模型能否在新测试上下文中保持与老师一致的行为，存在对蒸馏模型泛化性的担忧。

Method: 1) 提出推理蒸馏溯源框架：针对每个模型生成的动作，同时获取老师、原始学生和蒸馏模型在同一上下文中的预测概率，通过比较分类动作来源；2) 提出基于师生训练数据分歧度的教师指导数据选择方法。

Result: 实验证明在测试上下文中蒸馏模型确实能生成老师源头的动作，这些动作与蒸馏模型观察到的性能相关且能解释其表现；所提数据选择方法在多个代表性教师模型和多样学生模型中验证有效。

Conclusion: 溯源框架可用于理解推理蒸馏机制，基于师生分歧度的数据选择方法能提升蒸馏效果，展示了该框架在推理蒸馏研究中的潜力和应用价值。

Abstract: Reasoning distillation has attracted increasing attention. It typically leverages a large teacher model to generate reasoning paths, which are then used to fine-tune a student model so that it mimics the teacher's behavior in training contexts. However, previous approaches have lacked a detailed analysis of the origins of the distilled model's capabilities. It remains unclear whether the student can maintain consistent behaviors with the teacher in novel test-time contexts, or whether it regresses to its original output patterns, raising concerns about the generalization of distillation models. To analyse this question, we introduce a cross-model Reasoning Distillation Provenance Tracing framework. For each action (e.g., a sentence) produced by the distilled model, we obtain the predictive probabilities assigned by the teacher, the original student, and the distilled model under the same context. By comparing these probabilities, we classify each action into different categories. By systematically disentangling the provenance of each action, we experimentally demonstrate that, in test-time contexts, the distilled model can indeed generate teacher-originated actions, which correlate with and plausibly explain observed performance on distilled model. Building on this analysis, we further propose a teacher-guided data selection method. Unlike prior approach that rely on heuristics, our method directly compares teacher-student divergences on the training data, providing a principled selection criterion. We validate the effectiveness of our approach across multiple representative teacher models and diverse student models. The results highlight the utility of our provenance-tracing framework and underscore its promise for reasoning distillation. We hope to share Reasoning Distillation Provenance Tracing and our insights into reasoning distillation with the community.

</details>


### [72] [Foundation Model-based Evaluation of Neuropsychiatric Disorders: A Lifespan-Inclusive, Multi-Modal, and Multi-Lingual Study](https://arxiv.org/abs/2512.20948)
*Zhongren Dong,Haotian Guo,Weixiang Xu,Huan Zhao,Zixing Zhang*

Main category: cs.CL

TL;DR: FEND是一个基于基础模型的多模态神经精神障碍评估框架，整合语音和文本模态用于检测阿尔茨海默病、抑郁和自闭症谱系障碍，在13个多语言数据集上进行系统评估。


<details>
  <summary>Details</summary>
Motivation: 神经精神障碍（如AD、抑郁、ASD）具有语言和声学异常特征，可作为早期检测的生物标志物。目前多模态方法面临多语言泛化挑战，且缺乏统一评估框架。

Method: 提出FEND框架，整合语音和文本模态，在13个多语言数据集（英语、中文、希腊语、法语、荷兰语）上系统评估多模态融合性能，分析模态不平衡和跨语料库泛化能力。

Result: 多模态融合在AD和抑郁检测中表现优异，但在ASD检测中因数据集异质性而表现不佳；发现模态不平衡问题，多模态融合未能超越最佳单模态模型；跨语料实验显示任务和语言一致时性能稳健，但在多语言和任务异质场景中性能下降。

Conclusion: FEND为自动化、跨生命周期、多语言的神经精神障碍评估提供了统一框架、广泛基准和影响因素分析，鼓励研究者采用该框架进行公平比较和可复现研究。

Abstract: Neuropsychiatric disorders, such as Alzheimer's disease (AD), depression, and autism spectrum disorder (ASD), are characterized by linguistic and acoustic abnormalities, offering potential biomarkers for early detection. Despite the promise of multi-modal approaches, challenges like multi-lingual generalization and the absence of a unified evaluation framework persist. To address these gaps, we propose FEND (Foundation model-based Evaluation of Neuropsychiatric Disorders), a comprehensive multi-modal framework integrating speech and text modalities for detecting AD, depression, and ASD across the lifespan. Leveraging 13 multi-lingual datasets spanning English, Chinese, Greek, French, and Dutch, we systematically evaluate multi-modal fusion performance. Our results show that multi-modal fusion excels in AD and depression detection but underperforms in ASD due to dataset heterogeneity. We also identify modality imbalance as a prevalent issue, where multi-modal fusion fails to surpass the best mono-modal models. Cross-corpus experiments reveal robust performance in task- and language-consistent scenarios but noticeable degradation in multi-lingual and task-heterogeneous settings. By providing extensive benchmarks and a detailed analysis of performance-influencing factors, FEND advances the field of automated, lifespan-inclusive, and multi-lingual neuropsychiatric disorder assessment. We encourage researchers to adopt the FEND framework for fair comparisons and reproducible research.

</details>


### [73] [MultiMind at SemEval-2025 Task 7: Crosslingual Fact-Checked Claim Retrieval via Multi-Source Alignment](https://arxiv.org/abs/2512.20950)
*Mohammad Mahdi Abootorabi,Alireza Ghahramani Kure,Mohammadali Mohammadkhani,Sina Elahimanesh,Mohammad Ali Ali Panah*

Main category: cs.CL

TL;DR: 提出了TriAligner系统用于多语言和跨语言事实核查声明检索，采用双编码器架构结合对比学习，利用多模态数据增强和硬负采样提升性能。


<details>
  <summary>Details</summary>
Motivation: 在虚假信息快速传播的时代，有效的事实核查变得越来越重要，需要开发能够跨语言检索声明的高效系统。

Method: 使用基于对比学习的双编码器架构TriAligner，结合多模态数据预处理增强和大语言模型数据增强，采用硬负采样优化表示学习。

Result: 在单语言和跨语言基准测试中，相比基线模型在检索准确率和事实核查性能上有显著提升。

Conclusion: TriAligner系统通过创新的多模态对齐学习和数据增强策略，在多语言和跨语言事实核查声明检索任务中表现优异，为解决虚假信息传播提供了有效工具。

Abstract: This paper presents our system for SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval. In an era where misinformation spreads rapidly, effective fact-checking is increasingly critical. We introduce TriAligner, a novel approach that leverages a dual-encoder architecture with contrastive learning and incorporates both native and English translations across different modalities. Our method effectively retrieves claims across multiple languages by learning the relative importance of different sources in alignment. To enhance robustness, we employ efficient data preprocessing and augmentation using large language models while incorporating hard negative sampling to improve representation learning. We evaluate our approach on monolingual and crosslingual benchmarks, demonstrating significant improvements in retrieval accuracy and fact-checking performance over baselines.

</details>


### [74] [Reflection Pretraining Enables Token-Level Self-Correction in Biological Sequence Models](https://arxiv.org/abs/2512.20954)
*Xiang Zhang,Jiaqi Wei,Yuejin Yang,Zijie Qiu,Yuhan Chen,Zhiqiang Gao,Muhammad Abdul-Mageed,Laks V. S. Lakshmanan,Wanli Ouyang,Chenyu You,Siqi Sun*

Main category: cs.CL

TL;DR: 该论文提出了蛋白质语言模型中链式思维提示的概念，但指出由于蛋白质语言表达力的限制而无法直接应用，为此首次在生物序列模型中引入了反射预训练方法。


<details>
  <summary>Details</summary>
Motivation: 链式思维提示显著提升了NLP任务中的推理能力，但无法直接应用于蛋白质等非自然语言领域，因为蛋白质语言的有限表达力（如氨基酸标记）限制了中间推理步骤的生成。

Method: 首先提出了语言表达力的概念，然后引入反射预训练方法，使蛋白质模型能够生成超出简单答案标记的辅助"思考标记"，从而进行中间推理。

Result: 理论上证明了增强的标记集显著提高了生物语言的表达力；实验上表明新的预训练方法使蛋白质模型能够自我修正，相比标准预训练取得了显著性能提升。

Conclusion: 通过反射预训练方法，成功将链式思维推理引入蛋白质语言模型领域，通过增强生物语言的表达力改善了模型的整体推理能力。

Abstract: Chain-of-Thought (CoT) prompting has significantly advanced task-solving capabilities in natural language processing with large language models. Unlike standard prompting, CoT encourages the model to generate intermediate reasoning steps, non-answer tokens, that help guide the model toward more accurate final outputs. These intermediate steps enable more complex reasoning processes such as error correction, memory management, future planning, and self-reflection. However, applying CoT to non-natural language domains, such as protein and RNA language models, is not yet possible, primarily due to the limited expressiveness of their token spaces (e.g., amino acid tokens). In this work, we propose and define the concept of language expressiveness: the ability of a given language, using its tokens and grammar, to encode information. We show that the limited expressiveness of protein language severely restricts the applicability of CoT-style reasoning. To overcome this, we introduce reflection pretraining, for the first time in a biological sequence model, which enables the model to engage in intermediate reasoning through the generation of auxiliary "thinking tokens" beyond simple answer tokens. Theoretically, we demonstrate that our augmented token set significantly enhances biological language expressiveness, thereby improving the overall reasoning capacity of the model. Experimentally, our pretraining approach teaches protein models to self-correct and leads to substantial performance gains compared to standard pretraining.

</details>


### [75] [Automatic Replication of LLM Mistakes in Medical Conversations](https://arxiv.org/abs/2512.20983)
*Oleksii Proniakin,Diego Fajardo,Ruslan Nazarenko,Razvan Marinescu*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language models (LLMs) are increasingly evaluated in clinical settings using multi-dimensional rubrics which quantify reasoning quality, safety, and patient-centeredness. Yet, replicating specific mistakes in other LLM models is not straightforward and often requires manual effort. We introduce MedMistake, an automatic pipeline that extracts mistakes LLMs make in patient-doctor conversations and converts them into a benchmark of single-shot QA pairs. Our pipeline (1) creates complex, conversational data between an LLM patient and LLM doctor, (2) runs an evaluation with a committee of 2 LLM judges across a variety of dimensions and (3) creates simplified single-shot QA scenarios from those mistakes. We release MedMistake-All, a dataset of 3,390 single-shot QA pairs where GPT-5 and Gemini 2.5 Pro are currently failing to answer correctly, as judged by two LLM judges. We used medical experts to validate a subset of 211/3390 questions (MedMistake-Bench), which we used to run a final evaluation of 12 frontier LLMs: Claude Opus 4.5, Claude Sonnet 4.5, DeepSeek-Chat, Gemini 2.5 Pro, Gemini 3 Pro, GPT-4o, GPT-5, GPT-5.1, GPT-5.2, Grok 4, Grok 4.1, Mistral Large. We found that GPT models, Claude and Grok obtained the best performance on MedMistake-Bench. We release both the doctor-validated benchmark (MedMistake-Bench), as well as the full dataset (MedMistake-All) at https://huggingface.co/datasets/TheLumos/MedicalMistakeBenchmark.

</details>


### [76] [Distilling the Essence: Efficient Reasoning Distillation via Sequence Truncation](https://arxiv.org/abs/2512.21002)
*Wei-Rui Chen,Vignesh Kothapalli,Ata Fatahibaarzi,Hejian Sang,Shao Tang,Qingquan Song,Zhipeng Wang,Muhammad Abdul-Mageed*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Distilling the reasoning capabilities from a large language model (LLM) to a smaller student model often involves training on substantial amounts of reasoning data. However, distillation over lengthy sequences with prompt (P), chain-of-thought (CoT), and answer (A) segments makes the process computationally expensive. In this work, we investigate how the allocation of supervision across different segments (P, CoT, A) affects student performance. Our analysis shows that selective knowledge distillation over only the CoT tokens can be effective when the prompt and answer information is encompassed by it. Building on this insight, we establish a truncation protocol to quantify computation-quality tradeoffs as a function of sequence length. We observe that training on only the first $50\%$ of tokens of every training sequence can retain, on average, $\approx94\%$ of full-sequence performance on math benchmarks while reducing training time, memory usage, and FLOPs by about $50\%$ each. These findings suggest that reasoning distillation benefits from prioritizing early reasoning tokens and provides a simple lever for computation-quality tradeoffs. Codes are available at https://github.com/weiruichen01/distilling-the-essence.

</details>


### [77] [Rethinking Supervised Fine-Tuning: Emphasizing Key Answer Tokens for Improved LLM Accuracy](https://arxiv.org/abs/2512.21017)
*Xiaofeng Shi,Qian Kou,Yuduo Li,Hua Zhou*

Main category: cs.CL

TL;DR: SFTKey是一种两阶段训练方案，通过在第二阶段只对最终答案部分进行微调，解决传统SFT中模型过度关注较长推理链而忽视短但关键答案的问题，平均准确率提升超过5%。


<details>
  <summary>Details</summary>
Motivation: 传统监督微调中，大语言模型可能过度关注冗长的思维链序列，而对决定任务成功的简短关键答案部分关注不足，这影响了评估质量和任务成功率。

Method: 提出SFTKey两阶段训练方案：第一阶段进行传统SFT确保正确输出格式；第二阶段仅对关键答案部分进行微调以提高准确性。

Result: 在多个基准测试和模型系列上的实验表明，SFTKey相比传统SFT平均准确率提升超过5%，同时保留了生成正确格式的能力。

Conclusion: 这项研究通过明确平衡思维链学习和答案相关标记的额外优化，推进了大语言模型的微调方法，实现了更好的准确率和格式保持。

Abstract: With the rapid advancement of Large Language Models (LLMs), the Chain-of-Thought (CoT) component has become significant for complex reasoning tasks. However, in conventional Supervised Fine-Tuning (SFT), the model could allocate disproportionately more attention to CoT sequences with excessive length. This reduces focus on the much shorter but essential Key portion-the final answer, whose correctness directly determines task success and evaluation quality. To address this limitation, we propose SFTKey, a two-stage training scheme. In the first stage, conventional SFT is applied to ensure proper output format, while in the second stage, only the Key portion is fine-tuned to improve accuracy. Extensive experiments across multiple benchmarks and model families demonstrate that SFTKey achieves an average accuracy improvement exceeding 5\% over conventional SFT, while preserving the ability to generate correct formats. Overall, this study advances LLM fine-tuning by explicitly balancing CoT learning with additional optimization on answer-relevant tokens.

</details>


### [78] [Semi-Supervised Learning for Large Language Models Safety and Content Moderation](https://arxiv.org/abs/2512.21107)
*Eduard Stefan Dinuta,Iustin Sirbu,Traian Rebedea*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Safety for Large Language Models (LLMs) has been an ongoing research focus since their emergence and is even more relevant nowadays with the increasing capacity of those models. Currently, there are several guardrails in place for all public LLMs and multiple proposed datasets for training safety classifiers. However, training these safety classifiers relies on large quantities of labeled data, which can be problematic to acquire, prone to labeling errors, or often include synthetic data. To address these issues, we suggest a different approach: utilizing semi-supervised learning techniques, which leverage both labeled and unlabeled data, to improve the performance on the safety task. We analyze the improvements that these techniques can offer for both prompts given to Large Language Models and the responses to those requests. Moreover, since augmentation is the central part of semi-supervised algorithms, we demonstrate the importance of using task-specific augmentations, which significantly increase the performance when compared to general-purpose augmentation techniques.

</details>


### [79] [ClarifyMT-Bench: Benchmarking and Improving Multi-Turn Clarification for Conversational Large Language Models](https://arxiv.org/abs/2512.21120)
*Sichun Luo,Yi Huang,Mukai Li,Shichang Meng,Fengyuan Liu,Zefa Hu,Junlan Feng,Qi Liu*

Main category: cs.CL

TL;DR: 提出了ClarifyMT-Bench多轮澄清基准和ClarifyAgent方法，解决LLM在开放域多轮对话中过早回答和性能随对话深度下降的问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM澄清基准多基于单轮互动或假设用户完全配合，无法真实评估多轮对话中的澄清行为，而实际使用中用户常提供不完整或模糊信息。

Method: 1) 基于五维度模糊分类学和六种行为多样用户角色构建ClarifyMT-Bench基准，通过混合LLM-人工流程生成6,120个多轮对话；2) 提出ClarifyAgent智能体方法，将澄清分解为感知、预测、追踪、规划四个模块。

Result: 评估10个代表性LLM发现存在一致的澄清不足偏好：模型倾向于过早回答，且性能随对话深度增加而下降；ClarifyAgent方法显著提高了在各种模糊条件下的鲁棒性。

Conclusion: ClarifyMT-Bench为研究LLM在真实人机交互中何时该提问、何时该回答以及如何应对模糊性提供了可复现的基础框架。

Abstract: Large language models (LLMs) are increasingly deployed as conversational assistants in open-domain, multi-turn settings, where users often provide incomplete or ambiguous information. However, existing LLM-focused clarification benchmarks primarily assume single-turn interactions or cooperative users, limiting their ability to evaluate clarification behavior in realistic settings. We introduce \textbf{ClarifyMT-Bench}, a benchmark for multi-turn clarification grounded in a five-dimensional ambiguity taxonomy and a set of six behaviorally diverse simulated user personas. Through a hybrid LLM-human pipeline, we construct 6,120 multi-turn dialogues capturing diverse ambiguity sources and interaction patterns. Evaluating ten representative LLMs uncovers a consistent under-clarification bias: LLMs tend to answer prematurely, and performance degrades as dialogue depth increases. To mitigate this, we propose \textbf{ClarifyAgent}, an agentic approach that decomposes clarification into perception, forecasting, tracking, and planning, substantially improving robustness across ambiguity conditions. ClarifyMT-Bench establishes a reproducible foundation for studying when LLMs should ask, when they should answer, and how to navigate ambiguity in real-world human-LLM interactions.

</details>


### [80] [SpidR-Adapt: A Universal Speech Representation Model for Few-Shot Adaptation](https://arxiv.org/abs/2512.21204)
*Mahi Luthra,Jiayi Shen,Maxime Poli,Angelo Ortiz,Yosuke Higuchi,Youssef Benchekroun,Martin Gleize,Charles-Eric Saint-James,Dongyan Lin,Phillip Rust,Angel Villar,Surya Parimi,Vanessa Stark,Rashel Moritz,Juan Pino,Yann LeCun,Emmanuel Dupoux*

Main category: cs.CL

TL;DR: 该论文针对人工智能语音模型数据需求大的问题，提出了SpidR-Adapt方法，通过元学习和多任务自适应预训练，让模型仅用极少未标注数据就能快速适应新语言。


<details>
  <summary>Details</summary>
Motivation: 人类婴儿仅需几百小时语音接触就能掌握新语言的基本单元，而现有的自监督语音模型却需要大量数据。这揭示了两者之间存在显著的效率差距，需要开发更高效的新语言适应方法。

Method: 提出SpidR-Adapt框架，将低资源语音表示学习构建为元学习问题；采用多任务自适应预训练(MAdaPT)协议，将适应过程制定为双层优化框架；提出一阶双层优化(FOBLO)减轻计算负担；通过交替自监督和监督目标的交错监督实现稳定训练初始化。

Result: SpidR-Adapt在不到1小时目标语言音频上训练后，在音素可区分性（ABX）和口语语言建模（sWUGGY, sBLIMP, tSC）方面快速超越领域内语言模型，比标准训练数据效率提高100倍以上。

Conclusion: 该研究展示了一条实用的、架构无关的路径，朝着生物启发的、高数据效率的语音表示学习方向发展，相关代码和模型已在GitHub开源。

Abstract: Human infants, with only a few hundred hours of speech exposure, acquire basic units of new languages, highlighting a striking efficiency gap compared to the data-hungry self-supervised speech models. To address this gap, this paper introduces SpidR-Adapt for rapid adaptation to new languages using minimal unlabeled data. We cast such low-resource speech representation learning as a meta-learning problem and construct a multi-task adaptive pre-training (MAdaPT) protocol which formulates the adaptation process as a bi-level optimization framework. To enable scalable meta-training under this framework, we propose a novel heuristic solution, first-order bi-level optimization (FOBLO), avoiding heavy computation costs. Finally, we stabilize meta-training by using a robust initialization through interleaved supervision which alternates self-supervised and supervised objectives. Empirically, SpidR-Adapt achieves rapid gains in phonemic discriminability (ABX) and spoken language modeling (sWUGGY, sBLIMP, tSC), improving over in-domain language models after training on less than 1h of target-language audio, over $100\times$ more data-efficient than standard training. These findings highlight a practical, architecture-agnostic path toward biologically inspired, data-efficient representations. We open-source the training code and model checkpoints at https://github.com/facebookresearch/spidr-adapt.

</details>


### [81] [Parallel Token Prediction for Language Models](https://arxiv.org/abs/2512.21323)
*Felix Draxler,Justus Will,Farrin Marouf Sofian,Theofanis Karaletsos,Sameer Singh,Stephan Mandt*

Main category: cs.CL

TL;DR: PTP是一个通用并行序列生成框架，通过单次Transformer调用联合预测多个依赖令牌，解决自回归解码的延迟瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 现有自回归解码方法存在延迟瓶颈，而现有的多令牌预测方法通常依赖独立性假设，限制了建模能力。需要一种既能降低延迟又不损失建模能力的并行生成框架。

Method: PTP将采样过程融入模型，在单个transformer调用中联合预测多个依赖令牌。采用蒸馏训练或无教师的反向自回归训练。

Result: 在Vicuna-7B上实现了当前最优的推测解码性能，在Spec-Bench上每个步骤接受超过4个令牌。理论证明PTP可以表示任意自回归序列分布。

Conclusion: 该框架表明，在不损失建模能力的前提下并行生成长序列是可行的，为语言模型的高效解码提供了通用解决方案。

Abstract: We propose Parallel Token Prediction (PTP), a universal framework for parallel sequence generation in language models. PTP jointly predicts multiple dependent tokens in a single transformer call by incorporating the sampling procedure into the model. This reduces the latency bottleneck of autoregressive decoding, and avoids the restrictive independence assumptions common in existing multi-token prediction methods. We prove that PTP can represent arbitrary autoregressive sequence distributions. PTP is trained either by distilling an existing model or through inverse autoregressive training without a teacher. Experimentally, we achieve state-of-the-art speculative decoding performance on Vicuna-7B by accepting over four tokens per step on Spec-Bench. The universality of our framework indicates that parallel generation of long sequences is feasible without loss of modeling power.

</details>


### [82] [Your Reasoning Benchmark May Not Test Reasoning: Revealing Perception Bottleneck in Abstract Reasoning Benchmarks](https://arxiv.org/abs/2512.21329)
*Xinhe Wang,Jin Huang,Xingjian Zhang,Tianhao Wang,Jiaqi W. Ma*

Main category: cs.CL

TL;DR: 论文挑战了传统观点，认为ARC类基准测试的困难主要源于视觉感知限制而非推理缺陷，通过两阶段实验验证了感知能力是性能差距的主因。


<details>
  <summary>Details</summary>
Motivation: 当前AI推理基准（如ARC）常被用于评估机器的核心推理能力，但前沿视觉语言模型在这些任务上表现不佳。传统观点将此归因于机器推理能力不足，但作者认为这种解释忽视了视觉感知限制的影响，希望验证性能差距是否主要源于感知而非推理问题。

Method: 引入两阶段实验框架：1）感知阶段将每幅图像独立转换为自然语言描述；2）推理阶段基于描述进行规则归纳和应用。该方法隔离了感知瓶颈，防止跨图像归纳信号泄漏，并在Mini-ARC、ACRE和Bongard-LOGO三个数据集上对比了两阶段与端到端单阶段评估。

Result: 实验表明感知能力是导致性能差距的主导因素。通过检查VLM输出发现约80%的模型失败源于感知错误，而非推理缺陷。这证明ARC类基准测试将感知和推理挑战混为一谈，可能夸大机器推理能力的不足。

Conclusion: ARC式评估基准未能区分感知和推理挑战，导致对机器推理能力的评价存在偏差。需要开发能够分离感知与推理的评估方法，以更准确地衡量机器智能进展。

Abstract: Reasoning benchmarks such as the Abstraction and Reasoning Corpus (ARC) and ARC-AGI are widely used to assess progress in artificial intelligence and are often interpreted as probes of core, so-called ``fluid'' reasoning abilities. Despite their apparent simplicity for humans, these tasks remain challenging for frontier vision-language models (VLMs), a gap commonly attributed to deficiencies in machine reasoning. We challenge this interpretation and hypothesize that the gap arises primarily from limitations in visual perception rather than from shortcomings in inductive reasoning.
  To verify this hypothesis, we introduce a two-stage experimental pipeline that explicitly separates perception and reasoning. In the perception stage, each image is independently converted into a natural-language description, while in the reasoning stage a model induces and applies rules using these descriptions. This design prevents leakage of cross-image inductive signals and isolates reasoning from perception bottlenecks. Across three ARC-style datasets, Mini-ARC, ACRE, and Bongard-LOGO, we show that the perception capability is the dominant factor underlying the observed performance gap by comparing the two-stage pipeline with against standard end-to-end one-stage evaluation. Manual inspection of reasoning traces in the VLM outputs further reveals that approximately 80 percent of model failures stem from perception errors. Together, these results demonstrate that ARC-style benchmarks conflate perceptual and reasoning challenges and that observed performance gaps may overstate deficiencies in machine reasoning. Our findings underscore the need for evaluation protocols that disentangle perception from reasoning when assessing progress in machine intelligence.

</details>


### [83] [C2LLM Technical Report: A New Frontier in Code Retrieval via Adaptive Cross-Attention Pooling](https://arxiv.org/abs/2512.21332)
*Jin Qin,Zihan Liao,Ziyin Zhang,Hang Yu,Peng Di,Rui Wang*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We present C2LLM - Contrastive Code Large Language Models, a family of code embedding models in both 0.5B and 7B sizes. Building upon Qwen-2.5-Coder backbones, C2LLM adopts a Pooling by Multihead Attention (PMA) module for generating sequence embedding from token embeddings, effectively 1) utilizing the LLM's causal representations acquired during pretraining, while also 2) being able to aggregate information from all tokens in the sequence, breaking the information bottleneck in EOS-based sequence embeddings, and 3) supporting flexible adaptation of embedding dimension, serving as an alternative to MRL. Trained on three million publicly available data, C2LLM models set new records on MTEB-Code among models of similar sizes, with C2LLM-7B ranking 1st on the overall leaderboard.

</details>


### [84] [Optimizing Decoding Paths in Masked Diffusion Models by Quantifying Uncertainty](https://arxiv.org/abs/2512.21336)
*Ziyu Chen,Xinbei Jiang,Peng Sun,Tao Lin*

Main category: cs.CL

TL;DR: 本文针对掩码扩散模型生成顺序敏感问题，提出了基于解噪熵的路径优化方法，显著提升了生成质量。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散模型在生成过程中，最终输出质量对解码顺序高度敏感，这源于生成路径中累积的预测不确定性。

Method: 1. 提出解噪熵作为量化累积不确定性的可计算指标；
2. 设计两种解码路径优化算法：事后选择方法和实时引导策略。

Result: 实验表明，熵引导方法在推理、规划和代码基准测试中显著提升了生成质量，将MDMs中的不确定性转化为发现高质量解决方案的优势。

Conclusion: 解噪熵为理解和控制生成过程提供了原则性工具，将MDMs中的不确定性从负担转变为关键优势。

Abstract: Masked Diffusion Models (MDMs) offer flexible, non-autoregressive generation, but this freedom introduces a challenge: final output quality is highly sensitive to the decoding order. We are the first to formalize this issue, attributing the variability in output quality to the cumulative predictive uncertainty along a generative path. To quantify this uncertainty, we introduce Denoising Entropy, a computable metric that serves as an internal signal for evaluating generative process. Leveraging this metric, we propose two algorithms designed to optimize the decoding path: a post-hoc selection method and a real-time guidance strategy. Experiments demonstrate that our entropy-guided methods significantly improve generation quality, consistently boosting accuracy on challenging reasoning, planning, and code benchmarks. Our work establishes Denoising Entropy as a principled tool for understanding and controlling generation, effectively turning the uncertainty in MDMs from a liability into a key advantage for discovering high-quality solutions.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [85] [Towards Optimal Performance and Action Consistency Guarantees in Dec-POMDPs with Inconsistent Beliefs and Limited Communication](https://arxiv.org/abs/2512.20778)
*Moshe Rafaeli Shimron,Vadim Indelman*

Main category: cs.MA

TL;DR: 提出了一种考虑信念不一致的去中心化多智能体决策框架，具有行动一致性和性能的概率保证，并仅在需要时触发通信。


<details>
  <summary>Details</summary>
Motivation: 现实中多智能体往往因通信受限而持有不一致的信念，现有方法假设信念一致性不符实际，导致协调不佳和潜在安全问题。

Method: 引入新的去中心化框架，显式处理信念不一致问题，为行动一致性和性能提供概率保证，并选择性触发通信。

Result: 模拟实验表明，该方法在性能上优于现有最先进算法。

Conclusion: 该框架有效解决了信念不一致下的多智能体协调问题，通过选择性通信平衡了性能与通信成本。

Abstract: Multi-agent decision-making under uncertainty is fundamental for effective and safe autonomous operation. In many real-world scenarios, each agent maintains its own belief over the environment and must plan actions accordingly. However, most existing approaches assume that all agents have identical beliefs at planning time, implying these beliefs are conditioned on the same data. Such an assumption is often impractical due to limited communication. In reality, agents frequently operate with inconsistent beliefs, which can lead to poor coordination and suboptimal, potentially unsafe, performance. In this paper, we address this critical challenge by introducing a novel decentralized framework for optimal joint action selection that explicitly accounts for belief inconsistencies. Our approach provides probabilistic guarantees for both action consistency and performance with respect to open-loop multi-agent POMDP (which assumes all data is always communicated), and selectively triggers communication only when needed. Furthermore, we address another key aspect of whether, given a chosen joint action, the agents should share data to improve expected performance in inference. Simulation results show our approach outperforms state-of-the-art algorithms.

</details>


### [86] [DAO-Agent: Zero Knowledge-Verified Incentives for Decentralized Multi-Agent Coordination](https://arxiv.org/abs/2512.20973)
*Yihan Xia,Taotao Wang,Wenxin Xu,Shengli Zhang*

Main category: cs.MA

TL;DR: DAOA-Agent是一个集成DAO治理、零知识证明和混合链上链下架构的框架，用于解决自主LLM多智能体系统中透明贡献衡量和公平激励分配的挑战，同时保护隐私并降低链上成本。


<details>
  <summary>Details</summary>
Motivation: 自主LLM多智能体系统在无信任环境中面临中心化协调的挑战，包括贡献衡量不透明和激励分配不公平。区块链虽能提供去中心化协调，但存在高链上计算成本和暴露智能体敏感信息的问题。因此需要一种既能审计任务执行和公平分配激励，又能保护战略隐私并最小化链上成本的解决方案。

Method: 1) 基于链上DAO的治理机制实现透明协调和不可变日志记录；2) 采用零知识证明（ZKP）机制实现基于Shapley值的链下贡献衡量；3) 设计混合链上/链下架构，以最小计算开销在链上验证ZKP验证的贡献衡量结果。

Result: 在加密货币交易任务的端到端实验中，DAOA-Agent相比朴素的链上方案实现了高达99.9%的验证gas成本降低，验证复杂度为常数时间且随联盟规模增大保持稳定，为去中心化环境中的智能体协调建立了可扩展基础。

Conclusion: DAOA-Agent框架成功解决了自主LLM智能体在无信任环境中的协调挑战，通过创新的技术组合实现了隐私保护、成本优化和可扩展性，为跨应用和跨组织的智能体协作提供了实用解决方案。

Abstract: Autonomous Large Language Model (LLM)-based multi-agent systems have emerged as a promising paradigm for facilitating cross-application and cross-organization collaborations. These autonomous agents often operate in trustless environments, where centralized coordination faces significant challenges, such as the inability to ensure transparent contribution measurement and equitable incentive distribution. While blockchain is frequently proposed as a decentralized coordination platform, it inherently introduces high on-chain computation costs and risks exposing sensitive execution information of the agents. Consequently, the core challenge lies in enabling auditable task execution and fair incentive distribution for autonomous LLM agents in trustless environments, while simultaneously preserving their strategic privacy and minimizing on-chain costs. To address this challenge, we propose DAO-Agent, a novel framework that integrates three key technical innovations: (1) an on-chain decentralized autonomous organization (DAO) governance mechanism for transparent coordination and immutable logging; (2) a ZKP mechanism approach that enables Shapley-based contribution measurement off-chain, and (3) a hybrid on-chain/off-chain architecture that verifies ZKP-validated contribution measurements on-chain with minimal computational overhead. We implement DAO-Agent and conduct end-to-end experiments using a crypto trading task as a case study. Experimental results demonstrate that DAO-Agent achieves up to 99.9% reduction in verification gas costs compared to naive on-chain alternatives, with constant-time verification complexity that remains stable as coalition size increases, thereby establishing a scalable foundation for agent coordination in decentralized environments.

</details>


### [87] [A Plan Reuse Mechanism for LLM-Driven Agent](https://arxiv.org/abs/2512.21309)
*Guopeng Li,Ruiqi Wu,Haisheng Tan*

Main category: cs.MA

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Integrating large language models (LLMs) into personal assistants, like Xiao Ai and Blue Heart V, effectively enhances their ability to interact with humans, solve complex tasks, and manage IoT devices. Such assistants are also termed LLM-driven agents. Upon receiving user requests, the LLM-driven agent generates plans using an LLM, executes these plans through various tools, and then returns the response to the user. During this process, the latency for generating a plan with an LLM can reach tens of seconds, significantly degrading user experience. Real-world dataset analysis shows that about 30% of the requests received by LLM-driven agents are identical or similar, which allows the reuse of previously generated plans to reduce latency. However, it is difficult to accurately define the similarity between the request texts received by the LLM-driven agent through directly evaluating the original request texts. Moreover, the diverse expressions of natural language and the unstructured format of plan texts make implementing plan reuse challenging. To address these issues, we present and implement a plan reuse mechanism for LLM-driven agents called AgentReuse. AgentReuse leverages the similarities and differences among requests' semantics and uses intent classification to evaluate the similarities between requests and enable the reuse of plans. Experimental results based on a real-world dataset demonstrate that AgentReuse achieves a 93% effective plan reuse rate, an F1 score of 0.9718, and an accuracy of 0.9459 in evaluating request similarities, reducing latency by 93.12% compared with baselines without using the reuse mechanism.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [88] [MAR:Multi-Agent Reflexion Improves Reasoning Abilities in LLMs](https://arxiv.org/abs/2512.20845)
*Onat Ozer,Grace Wu,Yuchen Wang,Daniel Dosti,Honghao Zhang,Vivi De La Rue*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: LLMs have shown the capacity to improve their performance on reasoning tasks through reflecting on their mistakes, and acting with these reflections in mind. However, continual reflections of the same LLM onto itself exhibit degeneration of thought, where the LLM continues to repeat the same errors again and again even with the knowledge that its wrong. To address this problem, we instead introduce multi-agent with multi-persona debators as the method to generate reflections. Through out extensive experimentation, we've found that the leads to better diversity of in the reflections generated by the llm agent. We demonstrate an accuracy of 47% EM HotPot QA (question answering) and 82.7% on HumanEval (programming), both performances surpassing reflection with a single llm.

</details>


### [89] [A Blockchain-Monitored Agentic AI Architecture for Trusted Perception-Reasoning-Action Pipelines](https://arxiv.org/abs/2512.20985)
*Salman Jan,Hassan Ali Razzaqi,Ali Akarma,Mohammad Riyaz Belgaum*

Main category: cs.AI

TL;DR: LangChain Multi-Agent permissioned Blockchain AI Governance


<details>
  <summary>Details</summary>
Motivation: AI应用日益广泛 如医疗 智慧城市 供应链 但面临信任 监督与完整性挑战

Method: LangChain多智能体系统+Hyperledger许可链 实现感知-概念化-行动循环的区块链治理层 集成MCP执行器

Result: 区块链验证有效防止未授权操作 提供全程追溯 保持合理延迟 通过库存管理 交通信号 医疗监控实验验证

Conclusion: 该框架为高影响力自主AI应用提供负责任实施的通用系统 实现自主与监管平衡

Abstract: The application of agentic AI systems in autonomous decision-making is growing in the areas of healthcare, smart cities, digital forensics, and supply chain management. Even though these systems are flexible and offer real-time reasoning, they also raise concerns of trust and oversight, and integrity of the information and activities upon which they are founded. The paper suggests a single architecture model comprising of LangChain-based multi-agent system with a permissioned blockchain to guarantee constant monitoring, policy enforcement, and immutable auditability of agentic action. The framework relates the perception conceptualization-action cycle to a blockchain layer of governance that verifies the inputs, evaluates recommended actions, and documents the outcomes of the execution. A Hyperledger Fabric-based system, action executors MCP-integrated, and LangChain agent are introduced and experiments of smart inventory management, traffic-signal control, and healthcare monitoring are done. The results suggest that blockchain-security verification is efficient in preventing unauthorized practices, offers traceability throughout the whole decision-making process, and maintains operational latency within reasonable ranges. The suggested framework provides a universal system of implementing high-impact agentic AI applications that are autonomous yet responsible.

</details>


### [90] [MegaRAG: Multimodal Knowledge Graph-Based Retrieval Augmented Generation](https://arxiv.org/abs/2512.20626)
*Chi-Hsiang Hsiao,Yi-Cheng Wang,Tzung-Sheng Lin,Yi-Ren Yeh,Chu-Song Chen*

Main category: cs.AI

TL;DR: 该论文提出了一种结合多模态知识图谱的检索增强生成方法，通过整合视觉线索提升对跨模态内容的理解和推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统的检索增强生成方法在处理长文档时受限于上下文窗口，而现有的知识图谱方法仅限于文本输入，无法充分利用视觉等多模态信息的补充洞察。

Method: 引入多模态知识图谱的检索增强生成框架，将视觉线索融入知识图谱构建、检索和答案生成的整个过程。

Result: 在全局和细粒度问答任务上的实验结果表明，该方法在文本和多模态语料上均优于现有的基于检索增强生成的方法。

Conclusion: 通过融合视觉信息到知识图谱中，该方法能够实现跨模态推理，显著提升对复杂内容的理解能力。

Abstract: Retrieval-augmented generation (RAG) enables large language models (LLMs) to dynamically access external information, which is powerful for answering questions over previously unseen documents. Nonetheless, they struggle with high-level conceptual understanding and holistic comprehension due to limited context windows, which constrain their ability to perform deep reasoning over long-form, domain-specific content such as full-length books. To solve this problem, knowledge graphs (KGs) have been leveraged to provide entity-centric structure and hierarchical summaries, offering more structured support for reasoning. However, existing KG-based RAG solutions remain restricted to text-only inputs and fail to leverage the complementary insights provided by other modalities such as vision. On the other hand, reasoning from visual documents requires textual, visual, and spatial cues into structured, hierarchical concepts. To address this issue, we introduce a multimodal knowledge graph-based RAG that enables cross-modal reasoning for better content understanding. Our method incorporates visual cues into the construction of knowledge graphs, the retrieval phase, and the answer generation process. Experimental results across both global and fine-grained question answering tasks show that our approach consistently outperforms existing RAG-based approaches on both textual and multimodal corpora.

</details>


### [91] [Proceedings of the 20th International Conference on Knowledge, Information and Creativity Support Systems (KICSS 2025)](https://arxiv.org/abs/2512.20628)
*Edited by Tessai Hayama,Takayuki Ito,Takahiro Uchiya,Motoki Miura,Takahiro Kawaji,Takaya Yuizono,Atsuo Yoshitaka,Tokuro Matsuo,Shun Okuhara,Jawad Haqbeen,Sofia Sahab,Wen Gu,Shiyao Ding*

Main category: cs.AI

TL;DR: KICSS 2025会议论文集，展示人工智能等领域的最新研究。


<details>
  <summary>Details</summary>
Motivation: 为人工智能、知识工程等领域的学者提供学术交流平台，促进跨学科研究合作。

Method: 通过双盲评审接受论文，部分优秀论文推荐至IEICE期刊发表需进一步评审。

Result: 会议成功举办并收录经同行评审的高质量论文。

Conclusion: KICSS 2025有效促进了相关领域学术交流与知识共享，为后续研究奠定基础。

Abstract: This volume presents the proceedings of the 20th International Conference on Knowledge, Information and Creativity Support Systems (KICSS 2025), held in Nagaoka, Japan, on December 3-5, 2025. The conference, organized in cooperation with the IEICE Proceedings Series, provides a multidisciplinary forum for researchers in artificial intelligence, knowledge engineering, human-computer interaction, and creativity support systems. The proceedings include peer-reviewed papers accepted through a double-blind review process. Selected papers have been recommended for publication in IEICE Transactions on Information and Systems after an additional peer-review process.

</details>


### [92] [MicroProbe: Efficient Reliability Assessment for Foundation Models with Minimal Data](https://arxiv.org/abs/2512.20630)
*Aayam Bansal,Ishaan Gangwani*

Main category: cs.AI

TL;DR: 微探针方法使用100个战略选择的探测样本来大幅降低大模型可靠性评估的计算成本，相比传统方法减少90%评估成本的同时保持95%的覆盖率和99.9%统计功效。


<details>
  <summary>Details</summary>
Motivation: 传统基础模型可靠性评估需要数千个评估样本，计算昂贵且耗时，阻碍实际部署中的快速评估需求。

Method: 通过五个关键可靠性维度的战略提示多样性，结合高级不确定性量化和自适应加权，选择100个探测样本来全面评估模型可靠性。

Result: 在多个语言模型上验证，相比随机抽样基准，实现23.5%更高的综合可靠性分数，统计显著性极高；专家评估得分4.14/5.0；达到99.9%统计功效和90%成本降低。

Conclusion: 微探针方法填补了负责任AI部署中高效模型评估的关键空白，为实际应用提供可行的可靠性评估解决方案。

Abstract: Foundation model reliability assessment typically requires thousands of evaluation examples, making it computationally expensive and time-consuming for real-world deployment. We introduce microprobe, a novel approach that achieves comprehensive reliability assessment using only 100 strategically selected probe examples. Our method combines strategic prompt diversity across five key reliability dimensions with advanced uncertainty quantification and adaptive weighting to efficiently detect potential failure modes. Through extensive empirical evaluation on multiple language models (GPT-2 variants, GPT-2 Medium, GPT-2 Large) and cross-domain validation (healthcare, finance, legal), we demonstrate that microprobe achieves 23.5% higher composite reliability scores compared to random sampling baselines, with exceptional statistical significance (p < 0.001, Cohen's d = 1.21). Expert validation by three AI safety researchers confirms the effectiveness of our strategic selection, rating our approach 4.14/5.0 versus 3.14/5.0 for random selection. microprobe completes reliability assessment with 99.9% statistical power while representing a 90% reduction in assessment cost and maintaining 95% of traditional method coverage. Our approach addresses a critical gap in efficient model evaluation for responsible AI deployment.

</details>


### [93] [Erkang-Diagnosis-1.1 Technical Report](https://arxiv.org/abs/2512.20632)
*Jianbing Ma,Ao Feng,Zhenjie Gao,Xinyu Song,Li Su,Bin Chen,Wei Wang,Jiamin Wu*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This report provides a detailed introduction to Erkang-Diagnosis-1.1 model, our AI healthcare consulting assistant developed using Alibaba Qwen-3 model. The Erkang model integrates approximately 500GB of high-quality structured medical knowledge, employing a hybrid approach combining enhanced pre-training and retrieval-enhanced generation to create a secure, reliable, and professional AI health advisor. Through 3-5 efficient interaction rounds, Erkang Diagnosis can accurately understand user symptoms, conduct preliminary analysis, and provide valuable diagnostic suggestions and health guidance. Designed to become users intelligent health companions, it empowers primary healthcare and health management. To validate, Erkang-Diagnosis-1.1 leads GPT-4 in terms of comprehensive medical exams.

</details>


### [94] [Reasoning Relay: Evaluating Stability and Interchangeability of Large Language Models in Mathematical Reasoning](https://arxiv.org/abs/2512.20647)
*Leo Lu,Jonathan Zhang,Sean Chua,Spencer Kim,Kevin Zhu,Sean O'Brien,Vasu Sharma*

Main category: cs.AI

TL;DR: 研究探索大型语言模型之间推理链的互换性，通过模型之间继续已有部分推理链来检验推理的可靠性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 虽然CoT提示显著提升了LLMs的推理能力，但现有研究主要集中在提升单模型的内部推理策略，而关于不同模型间推理的互换性仍未知。本研究旨在探究一个模型的已生成部分推理链能否被另一模型可靠地延续，以期作为评估推理可靠性与信任度的新视角。

Method: 使用Gemma-3-4B-IT和LLaMA-3.1-70B-Instruct作为基线模型，通过词元级对数概率阈值在推理早期、中期和晚期截断推理链，然后用Gemma-3-1B-IT和LLaMA-3.1-8B-Instruct进行延续实验，评估同族和跨族模型的行为。评估流程结合截断阈值与过程奖励模型（PRM），形成一个可复现的框架来通过模型互换评估推理稳定性。

Result: 评估结果显示，混合推理链常常能保持原有准确性和逻辑结构，在某些情况下甚至能有所提高。这表明模型间推理存在互换性，推理的可靠性和连贯性在不同模型替代下得以保持。

Conclusion: 推理链的互换性可作为推理模型的新兴行为特征，这为协作式AI系统中实现可靠模块化推理提供了新的范式和启示。

Abstract: Chain-of-Thought (CoT) prompting has significantly advanced the reasoning capabilities of large language models (LLMs). While prior work focuses on improving model performance through internal reasoning strategies, little is known about the interchangeability of reasoning across different models. In this work, we explore whether a partially completed reasoning chain from one model can be reliably continued by another model, either within the same model family or across families. We achieve this by assessing the sufficiency of intermediate reasoning traces as transferable scaffolds for logical coherence and final answer accuracy. We interpret this interchangeability as a means of examining inference-time trustworthiness, probing whether reasoning remains both coherent and reliable under model substitution. Using token-level log-probability thresholds to truncate reasoning at early, mid, and late stages from our baseline models, Gemma-3-4B-IT and LLaMA-3.1-70B-Instruct, we conduct continuation experiments with Gemma-3-1B-IT and LLaMA-3.1-8B-Instruct to test intra-family and cross-family behaviors. Our evaluation pipeline leverages truncation thresholds with a Process Reward Model (PRM), providing a reproducible framework for assessing reasoning stability via model interchange. Evaluations with a PRM reveal that hybrid reasoning chains often preserve, and in some cases even improve, final accuracy and logical structure. Our findings point towards interchangeability as an emerging behavioral property of reasoning models, offering insights into new paradigms for reliable modular reasoning in collaborative AI systems.

</details>


### [95] [From Pilots to Practices: A Scoping Review of GenAI-Enabled Personalization in Computer Science Education](https://arxiv.org/abs/2512.20714)
*Iman Reihanian,Yunfei Hou,Qingquan Sun*

Main category: cs.AI

TL;DR: 系统性综述（2023-2025）显示，在高等教育计算机科学中，生成式AI可以通过个性化教学提升学习效果，但设计选择至关重要。基于学生作品的语境感知辅导、分层提示结构、与传统工具结合及人工质量监督是成功关键。需注意学术诚信、隐私等风险，建议采用探索优先的采纳框架。


<details>
  <summary>Details</summary>
Motivation: 生成式AI能在规模上实现个性化计算机科学教育，但人们对其是否真正支持学习仍存疑问。本研究旨在系统梳理高等教育计算机科学背景下AI个性化的机制和效果证据，为教育实践提供依据。

Method: 采用范围综述方法，从259项研究中目的性抽样32项（2023-2025年），分析计算机科学教育中生成式AI的个性化应用。研究识别了五个应用领域，并分析设计选择如何影响学习成果。

Result: 研究发现五种主要应用领域：智能辅导、个性化材料、形成性反馈、AI增强评估和代码评审。采用“解释优先引导”、“暂不提供解答”、“分级提示阶梯”以及“基于学生作品”等设计的系统，学习过程效果明显优于无约束的聊天界面。成功的实施方案具有四个共同模式：基于学生作品的语境感知辅导、需要反思的多级提示结构、与传统CS工具（如自动评分器和评分标准）的结合，以及人工参与的質量保证。

Conclusion: 证据表明，当生成式AI嵌入可审计的工作流程并保留学生的“有效努力”时，可以成为精准支架式教学的有效机制。研究提出了强调试点、监测工具、学习保护默认设置和基于证据扩展的“探索优先”采纳框架，并针对学术诚信、隐私、偏见和过度依赖等风险提出了操作层面的缓解措施，以在规模化提供个性化支持的同时保持教育质量。

Abstract: Generative AI enables personalized computer science education at scale, yet questions remain about whether such personalization supports or undermines learning. This scoping review synthesizes 32 studies (2023-2025) purposively sampled from 259 records to map personalization mechanisms and effectiveness signals in higher-education computer science contexts. We identify five application domains: intelligent tutoring, personalized materials, formative feedback, AI-augmented assessment, and code review, and analyze how design choices shape learning outcomes. Designs incorporating explanation-first guidance, solution withholding, graduated hint ladders, and artifact grounding (student code, tests, and rubrics) consistently show more positive learning processes than unconstrained chat interfaces. Successful implementations share four patterns: context-aware tutoring anchored in student artifacts, multi-level hint structures requiring reflection, composition with traditional CS infrastructure (autograders and rubrics), and human-in-the-loop quality assurance. We propose an exploration-first adoption framework emphasizing piloting, instrumentation, learning-preserving defaults, and evidence-based scaling. Recurrent risks include academic integrity, privacy, bias and equity, and over-reliance, and we pair these with operational mitigation. The evidence supports generative AI as a mechanism for precision scaffolding when embedded in audit-ready workflows that preserve productive struggle while scaling personalized support.

</details>


### [96] [AIAuditTrack: A Framework for AI Security system](https://arxiv.org/abs/2512.20649)
*Zixun Luo,Yuhang Fan,Yufei Li,Youzhi Zhang,Hengyu Lin,Ziqi Wang*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The rapid expansion of AI-driven applications powered by large language models has led to a surge in AI interaction data, raising urgent challenges in security, accountability, and risk traceability. This paper presents AiAuditTrack (AAT), a blockchain-based framework for AI usage traffic recording and governance. AAT leverages decentralized identity (DID) and verifiable credentials (VC) to establish trusted and identifiable AI entities, and records inter-entity interaction trajectories on-chain to enable cross-system supervision and auditing. AI entities are modeled as nodes in a dynamic interaction graph, where edges represent time-specific behavioral trajectories. Based on this model, a risk diffusion algorithm is proposed to trace the origin of risky behaviors and propagate early warnings across involved entities. System performance is evaluated using blockchain Transactions Per Second (TPS) metrics, demonstrating the feasibility and stability of AAT under large-scale interaction recording. AAT provides a scalable and verifiable solution for AI auditing, risk management, and responsibility attribution in complex multi-agent environments.

</details>


### [97] [Mixture of Attention Schemes (MoAS): Learning to Route Between MHA, GQA, and MQA](https://arxiv.org/abs/2512.20650)
*Esmail Gumaan*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The choice of attention mechanism in Transformer models involves a critical trade-off between modeling quality and inference efficiency. Multi-Head Attention (MHA) offers the best quality but suffers from large Key-Value (KV) cache memory requirements during inference. Multi-Query Attention (MQA) and Grouped-Query Attention (GQA) reduce memory usage but often at the cost of model performance. In this work, we propose Mixture of Attention Schemes (MoAS), a novel architecture that dynamically selects the optimal attention scheme (MHA, GQA, or MQA) for each token via a learned router. We demonstrate that dynamic routing performs better than static averaging of schemes and achieves performance competitive with the MHA baseline while offering potential for conditional compute efficiency. Experimental results on WikiText-2 show that dynamic routing (val loss 2.3074) outperforms a static mixture (2.3093), validating the effectiveness of the proposed method. Our code is available at https://github.com/Esmail-ibraheem/Mixture-of-Attention-Schemes-MoAS.

</details>


### [98] [Memory Bear AI A Breakthrough from Memory to Cognition Toward Artificial General Intelligence](https://arxiv.org/abs/2512.20651)
*Deliang Wen,Ke Sun*

Main category: cs.AI

TL;DR: Memory Bear系统基于认知科学原理构建类人记忆架构，解决了大语言模型在记忆机制上的固有局限，显著提升了知识保真度和推理能力，在各种应用场景中表现优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在上下文窗口受限、长期知识遗忘、信息冗余积累和幻觉生成等内在记忆限制，严重制约了持续对话和个性化服务的发展，需要从根本上重构记忆机制。

Method: 基于认知科学原理构建类人记忆架构，集成多模态信息感知、动态记忆维护和自适应认知服务，实现LLM记忆机制的全链条重构。

Result: 在医疗、企业运营、教育等领域展现显著工程创新和性能突破，相较于Mem0、MemGPT、Graphiti等现有方案，在准确性、token效率和响应延迟等关键指标上都表现更优。

Conclusion: 这是推动AI从'记忆'向'认知'迈进的关键一步，通过记忆-认知一体化显著改善了知识保真度、检索效率，并减少幻觉生成，增强了上下文适应性和推理能力。

Abstract: Large language models (LLMs) face inherent limitations in memory, including restricted context windows, long-term knowledge forgetting, redundant information accumulation, and hallucination generation. These issues severely constrain sustained dialogue and personalized services. This paper proposes the Memory Bear system, which constructs a human-like memory architecture grounded in cognitive science principles. By integrating multimodal information perception, dynamic memory maintenance, and adaptive cognitive services, Memory Bear achieves a full-chain reconstruction of LLM memory mechanisms. Across domains such as healthcare, enterprise operations, and education, Memory Bear demonstrates substantial engineering innovation and performance breakthroughs. It significantly improves knowledge fidelity and retrieval efficiency in long-term conversations, reduces hallucination rates, and enhances contextual adaptability and reasoning capability through memory-cognition integration. Experimental results show that, compared with existing solutions (e.g., Mem0, MemGPT, Graphiti), Memory Bear outperforms them across key metrics, including accuracy, token efficiency, and response latency. This marks a crucial step forward in advancing AI from "memory" to "cognition".

</details>


### [99] [From Fake Focus to Real Precision: Confusion-Driven Adversarial Attention Learning in Transformers](https://arxiv.org/abs/2512.20661)
*Yawei Liu*

Main category: cs.AI

TL;DR: 针对Transformer模型在情感分析中注意力分配偏向常见词而忽略关键稀有词导致性能不佳的问题，提出AFA对抗训练机制，通过动态掩码策略和策略梯度优化注意力分布，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 虽然Transformer模型在情感分析中表现出色，但其注意力分布往往过度集中于常见词汇，忽略了那些低频但任务关键性的词汇，这导致在某些场景下模型的准确率下降。

Method: 提出了对抗性注意力反馈(AFA)训练机制，它引入一个动态掩码策略：用一个生成器尝试掩蔽各种词汇来欺骗判别器，同时判别器努力检测这些掩码带来的显著差异。此外，利用Transformer对token级扰动的敏感性，采用策略梯度方法优化注意力分布，以实现快速高效收敛。

Result: 在三个公开数据集上的实验表明，该方法达到了最先进的性能。此外，将该训练机制应用于提升大型语言模型的注意力，获得了12.6%的进一步性能提升。

Conclusion: AFA机制能有效引导Transformer模型合理分配注意力权重至关键任务词汇，显著提升情感分析性能，并且该机制可推广至大型语言模型，带来实质性改进。

Abstract: Transformer-based models have been widely adopted for sentiment analysis tasks due to their exceptional ability to capture contextual information. However, these methods often exhibit suboptimal accuracy in certain scenarios. By analyzing their attention distributions, we observe that existing models tend to allocate attention primarily to common words, overlooking less popular yet highly task-relevant terms, which significantly impairs overall performance. To address this issue, we propose an Adversarial Feedback for Attention(AFA) training mechanism that enables the model to automatically redistribute attention weights to appropriate focal points without requiring manual annotations. This mechanism incorporates a dynamic masking strategy that attempts to mask various words to deceive a discriminator, while the discriminator strives to detect significant differences induced by these masks. Additionally, leveraging the sensitivity of Transformer models to token-level perturbations, we employ a policy gradient approach to optimize attention distributions, which facilitates efficient and rapid convergence. Experiments on three public datasets demonstrate that our method achieves state-of-the-art results. Furthermore, applying this training mechanism to enhance attention in large language models yields a further performance improvement of 12.6%

</details>


### [100] [Quantifying Laziness, Decoding Suboptimality, and Context Degradation in Large Language Models](https://arxiv.org/abs/2512.20662)
*Yiqing Ma,Jung-Hua Liu*

Main category: cs.AI

TL;DR: 研究表明LLMs普遍存在懒惰行为（如不完全遵循复杂指令），但在简单推理任务中解码次优性证据有限，且在长对话中展现出超预期的语境保持能力，建议通过自优化和动态提示等策略改进指令遵从性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在行为缺陷问题，特别是懒惰（截断响应、部分执行多部分请求）、解码次优性（短视解码导致高质量序列选择失败）和语境退化（长对话中遗忘核心指令）。本研究旨在通过实验量化这些现象的发生程度，为改进模型可靠性提供依据。

Method: 设计三个控制实验（A、B、C），使用多款先进LLMs（OpenAI GPT-4变体、DeepSeek），评估：（A）多部分指令遵循的懒惰行为；（B）简单推理任务的解码次优性；（C）200回合混沌对话测试的语境维持能力，分析模型在各场景下的表现。

Result: 结果显示：模型在复杂多部分指令中普遍存在懒惰行为，常省略必要部分或无视长度要求；简单推理任务中解码次优性证据有限，贪婪回答与高置信度解保持一致；200回合混沌对话中模型展现出超预期的语境维持能力，能较好保持关键信息和指令。

Conclusion: 尽管现代LLMs在详细指令遵循上仍有挑战，但内部机制已能有效缓解一些假设的故障模式（如语境遗忘）。建议采用自优化、动态提示等策略减少懒惰行为，提升多指令遵从能力，并讨论了该发现对可靠性和长上下文处理研究的启示。

Abstract: Large Language Models (LLMs) often exhibit behavioral artifacts such as laziness (premature truncation of responses or partial compliance with multi-part requests), decoding suboptimality (failure to select higher-quality sequences due to myopic decoding), and context degradation (forgetting or ignoring core instructions over long conversations). We conducted three controlled experiments (A, B, and C) to quantify these phenomena across several advanced LLMs (OpenAI GPT-4 variant, DeepSeek). Our results indicate widespread laziness in satisfying complex multi-part instructions: models frequently omitted required sections or failed to meet length requirements despite explicit prompting. However, we found limited evidence of decoding suboptimality in a simple reasoning task (the models' greedy answers appeared to align with their highest-confidence solution), and we observed surprising robustness against context degradation in a 200-turn chaotic conversation test - the models maintained key facts and instructions far better than expected. These findings suggest that while compliance with detailed instructions remains an open challenge, modern LLMs may internally mitigate some hypothesized failure modes (such as context forgetting) in straightforward retrieval scenarios. We discuss implications for reliability, relate our findings to prior work on instruction-following and long-context processing, and recommend strategies (such as self-refinement and dynamic prompting) to reduce laziness and bolster multi-instruction compliance.

</details>


### [101] [Eidoku: A Neuro-Symbolic Verification Gate for LLM Reasoning via Structural Constraint Satisfaction](https://arxiv.org/abs/2512.20664)
*Shinobu Miya*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Models (LLMs) frequently produce hallucinated statements that are assigned high likelihood by the model itself, exposing a fundamental limitation of probability-based verification. This suggests that hallucination is often not a low-confidence phenomenon, but a failure of structural consistency. In this work, we reformulate the verification of LLM reasoning as a Constraint Satisfaction Problem (CSP) operating independently of the generation likelihood. Rather than optimizing for statistical plausibility, we model verification as a feasibility check based on structural violation cost -- the computational cost required to embed a candidate reasoning step into the contextual graph structure. We define a total cost function composed of three proxies: (i) graph connectivity (structural), (ii) feature space consistency (geometric), and (iii) logical entailment (symbolic). Crucially, verification is performed via a lightweight System-2 gate, Eidoku, which rejects candidates exceeding a context-calibrated cost threshold. The threshold is not learned but is derived from the intrinsic statistics of the context, avoiding ad hoc heuristics. We demonstrate that this approach successfully rejects ``smooth falsehoods'' -- statements that are highly probable yet structurally disconnected -- that probability-based verifiers are principally incapable of detecting. Our experiments on a controlled diagnostic dataset show that explicitly enforcing structural constraints allows for the deterministic rejection of this specific class of hallucinations, serving as a neuro-symbolic sanity check for generative reasoning.

</details>


### [102] [Bridging the AI Trustworthiness Gap between Functions and Norms](https://arxiv.org/abs/2512.20671)
*Daan Di Scala,Sophie Lathouwers,Michael van Bekkum*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Trustworthy Artificial Intelligence (TAI) is gaining traction due to regulations and functional benefits. While Functional TAI (FTAI) focuses on how to implement trustworthy systems, Normative TAI (NTAI) focuses on regulations that need to be enforced. However, gaps between FTAI and NTAI remain, making it difficult to assess trustworthiness of AI systems. We argue that a bridge is needed, specifically by introducing a conceptual language which can match FTAI and NTAI. Such a semantic language can assist developers as a framework to assess AI systems in terms of trustworthiness. It can also help stakeholders translate norms and regulations into concrete implementation steps for their systems. In this position paper, we describe the current state-of-the-art and identify the gap between FTAI and NTAI. We will discuss starting points for developing a semantic language and the envisioned effects of it. Finally, we provide key considerations and discuss future actions towards assessment of TAI.

</details>


### [103] [AgentMath: Empowering Mathematical Reasoning for Large Language Models via Tool-Augmented Agent](https://arxiv.org/abs/2512.20745)
*Haipeng Luo,Huawen Feng,Qingfeng Sun,Can Xu,Kai Zheng,Yufei Wang,Tao Yang,Han Hu,Yansong Tang,Di Wang*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Reasoning Models (LRMs) like o3 and DeepSeek-R1 have achieved remarkable progress in natural language reasoning with long chain-of-thought. However, they remain computationally inefficient and struggle with accuracy when solving problems requiring complex mathematical operations. In this work, we present AgentMath, an agent framework that seamlessly integrates language models' reasoning capabilities with code interpreters' computational precision to efficiently tackle complex mathematical problems. Our approach introduces three key innovations: (1) An automated method that converts natural language chain-of-thought into structured tool-augmented trajectories, generating high-quality supervised fine-tuning (SFT) data to alleviate data scarcity; (2) A novel agentic reinforcement learning (RL) paradigm that dynamically interleaves natural language generation with real-time code execution. This enables models to autonomously learn optimal tool-use strategies through multi-round interactive feedback, while fostering emergent capabilities in code refinement and error correction; (3) An efficient training system incorporating innovative techniques, including request-level asynchronous rollout scheduling, agentic partial rollout, and prefix-aware weighted load balancing, achieving 4-5x speedup and making efficient RL training feasible on ultra-long sequences with scenarios with massive tool calls.Extensive evaluations show that AgentMath achieves state-of-the-art performance on challenging mathematical competition benchmarks including AIME24, AIME25, and HMMT25. Specifically, AgentMath-30B-A3B attains 90.6%, 86.4%, and 73.8% accuracy respectively, achieving advanced capabilities.These results validate the effectiveness of our approach and pave the way for building more sophisticated and scalable mathematical reasoning agents.

</details>


### [104] [A Benchmark for Evaluating Outcome-Driven Constraint Violations in Autonomous AI Agents](https://arxiv.org/abs/2512.20798)
*Miles Q. Li,Benjamin C. M. Fung,Martin Weiss,Pulei Xiong,Khalil Al-Hussaeni,Claude Fachkha*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: As autonomous AI agents are increasingly deployed in high-stakes environments, ensuring their safety and alignment with human values has become a paramount concern. Current safety benchmarks often focusing only on single-step decision-making, simulated environments for tasks with malicious intent, or evaluating adherence to explicit negative constraints. There is a lack of benchmarks that are designed to capture emergent forms of outcome-driven constraint violations, which arise when agents pursue goal optimization under strong performance incentives while deprioritizing ethical, legal, or safety constraints over multiple steps in realistic production settings. To address this gap, we introduce a new benchmark comprising 40 distinct scenarios. Each scenario presents a task that requires multi-step actions, and the agent's performance is tied to a specific Key Performance Indicator (KPI). Each scenario features Mandated (instruction-commanded) and Incentivized (KPI-pressure-driven) variations to distinguish between obedience and emergent misalignment. Across 12 state-of-the-art large language models, we observe outcome-driven constraint violations ranging from 1.3% to 71.4%, with 9 of the 12 evaluated models exhibiting misalignment rates between 30% and 50%. Strikingly, we find that superior reasoning capability does not inherently ensure safety; for instance, Gemini-3-Pro-Preview, one of the most capable models evaluated, exhibits the highest violation rate at over 60%, frequently escalating to severe misconduct to satisfy KPIs. Furthermore, we observe significant "deliberative misalignment", where the models that power the agents recognize their actions as unethical during separate evaluation. These results emphasize the critical need for more realistic agentic-safety training before deployment to mitigate their risks in the real world.

</details>


### [105] [Safety Alignment of LMs via Non-cooperative Games](https://arxiv.org/abs/2512.20806)
*Anselm Paulus,Ilia Kulikov,Brandon Amos,Rémi Munos,Ivan Evtimov,Kamalika Chaudhuri,Arman Zharmagambetov*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Ensuring the safety of language models (LMs) while maintaining their usefulness remains a critical challenge in AI alignment. Current approaches rely on sequential adversarial training: generating adversarial prompts and fine-tuning LMs to defend against them. We introduce a different paradigm: framing safety alignment as a non-zero-sum game between an Attacker LM and a Defender LM trained jointly via online reinforcement learning. Each LM continuously adapts to the other's evolving strategies, driving iterative improvement. Our method uses a preference-based reward signal derived from pairwise comparisons instead of point-wise scores, providing more robust supervision and potentially reducing reward hacking. Our RL recipe, AdvGame, shifts the Pareto frontier of safety and utility, yielding a Defender LM that is simultaneously more helpful and more resilient to adversarial attacks. In addition, the resulting Attacker LM converges into a strong, general-purpose red-teaming agent that can be directly deployed to probe arbitrary target models.

</details>


### [106] [Context-Sensitive Abstractions for Reinforcement Learning with Parameterized Actions](https://arxiv.org/abs/2512.20831)
*Rashmeet Kaur Nayyar,Naman Shah,Siddharth Srivastava*

Main category: cs.AI

TL;DR: 本文提出了一种抽象驱动的强化学习方法，用于解决参数化动作空间中的长视野稀疏奖励问题，通过在线学习状态和动作抽象来提高样本效率。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的顺序决策往往涉及参数化动作空间，需要同时决定离散动作和连续动作参数。现有方法存在严重局限性：规划方法需要手工制作动作模型，标准强化学习算法仅适用于离散或连续动作，少数处理参数化动作的RL方法依赖领域特定工程且无法利用这类空间的潜在结构。

Method: 我们引入了一种算法，让智能体能够在学习过程中自主在线学习状态和动作抽象。这些算法在学习过程中逐步细化这些抽象，在状态-动作空间的关键区域增加精细细节，这些区域的更高分辨率能提升性能。

Result: 在多个连续状态、参数化动作的领域中，我们的抽象驱动方法使TD(λ)算法比目前最先进的基线方法实现了显著更高的样本效率。

Conclusion: 该方法成功扩展了强化学习算法在参数化动作空间中的适用范围，通过自主学习抽象机制有效解决了长视野稀疏奖励环境中的学习问题。

Abstract: Real-world sequential decision-making often involves parameterized action spaces that require both, decisions regarding discrete actions and decisions about continuous action parameters governing how an action is executed. Existing approaches exhibit severe limitations in this setting -- planning methods demand hand-crafted action models, and standard reinforcement learning (RL) algorithms are designed for either discrete or continuous actions but not both, and the few RL methods that handle parameterized actions typically rely on domain-specific engineering and fail to exploit the latent structure of these spaces. This paper extends the scope of RL algorithms to long-horizon, sparse-reward settings with parameterized actions by enabling agents to autonomously learn both state and action abstractions online. We introduce algorithms that progressively refine these abstractions during learning, increasing fine-grained detail in the critical regions of the state-action space where greater resolution improves performance. Across several continuous-state, parameterized-action domains, our abstraction-driven approach enables TD($λ$) to achieve markedly higher sample efficiency than state-of-the-art baselines.

</details>


### [107] [The Silent Scholar Problem: A Probabilistic Framework for Breaking Epistemic Asymmetry in LLM Agents](https://arxiv.org/abs/2512.20884)
*Zan-Kai Chong,Hiroyuki Ohsaki,Bryan Ng*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Autonomous agents powered by LLMs and Retrieval-Augmented Generation (RAG) are proficient consumers of digital content but remain unidirectional, a limitation we term epistemic asymmetry. This isolation leads to redundant reasoning and stagnates collective intelligence. Current self-reflection frameworks remain largely heuristic and private, lacking a probabilistic foundation to quantify certainty or justify external interaction.To bridge this gap, we propose a formal probabilistic framework that provides agents with a non-altruistic motive for bidirectional knowledge exchange. We model an agent's belief in a proposition using a Beta-Bernoulli distribution with a forgetting factor ($γ$). This allows us to isolate epistemic uncertainty as the variance of belief, establishing a dual drive for interaction: A homeostatic motive: The need to maintain certainty against the temporal decay introduced by $γ$. An optimal learning strategy: Targeting points of maximum ambiguity ($\mathbb{E}[θ]=0.5$) to maximize information gain. Under this framework, public contribution is reframed as optimal active learning: sharing solutions to elicit feedback is the most efficient method for an agent to reduce its own uncertainty. To ensure scalability, we introduce epistemic caching, which leverages the forgetting factor to dynamically prioritize resources for the active head of non-stationary knowledge distributions. Finally, we demonstrate how these accumulated belief states serve as verifiable reward signals for Reinforcement Learning from Human Feedback (RLHF) and high-quality data filters for Supervised Fine-Tuning (SFT). Simulation results validate that this uncertainty-driven strategy significantly outperforms random baselines in heterogeneous (Zipfian) environments, maintaining high adaptability to concept drift.

</details>


### [108] [TrafficSimAgent: A Hierarchical Agent Framework for Autonomous Traffic Simulation with MCP Control](https://arxiv.org/abs/2512.20996)
*Yuwei Du,Jun Zhang,Jie Feng,Zhicheng Liu,Jian Yuan,Yong Li*

Main category: cs.AI

TL;DR: TrafficSimAgent是一个基于LLM的智能体框架，用于通用交通仿真任务，通过专家级智能体协作实现自然语言指令到仿真实验的自动化设计与优化。


<details>
  <summary>Details</summary>
Motivation: 现有交通仿真平台（如SUMO、MATSim）虽功能完善，但用户需要专业知识才能从零开始实验，在实际工作中应用门槛高。

Method: 框架采用跨层级专家智能体协作：高层专家智能体解析自然语言指令、规划实验流程、按需调用MCP兼容工具；低层专家智能体基于实时交通状况选择基础元素的最优操作方案。

Result: 多场景实验表明，TrafficSimAgent能在各种条件下有效执行仿真，即使面对模糊的用户指令也能产生合理结果；其专家级自主决策优化机制在性能和效果上优于其他系统和SOTA LLM方法。

Conclusion: TrafficSimAgent通过LLM驱动的专家智能体协作，显著降低了交通仿真的技术门槛，提升了实验设计的智能化和优化效果，为交通优化和政策制定提供了高效工具。

Abstract: Traffic simulation is important for transportation optimization and policy making. While existing simulators such as SUMO and MATSim offer fully-featured platforms and utilities, users without too much knowledge about these platforms often face significant challenges when conducting experiments from scratch and applying them to their daily work. To solve this challenge, we propose TrafficSimAgent, an LLM-based agent framework that serves as an expert in experiment design and decision optimization for general-purpose traffic simulation tasks. The framework facilitates execution through cross-level collaboration among expert agents: high-level expert agents comprehend natural language instructions with high flexibility, plan the overall experiment workflow, and invoke corresponding MCP-compatible tools on demand; meanwhile, low-level expert agents select optimal action plans for fundamental elements based on real-time traffic conditions. Extensive experiments across multiple scenarios show that TrafficSimAgent effectively executes simulations under various conditions and consistently produces reasonable outcomes even when user instructions are ambiguous. Besides, the carefully designed expert-level autonomous decision-driven optimization in TrafficSimAgent yields superior performance when compared with other systems and SOTA LLM based methods.

</details>


### [109] [LLM Personas as a Substitute for Field Experiments in Method Benchmarking](https://arxiv.org/abs/2512.21080)
*Enoch Hyunwook Kang*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Field experiments (A/B tests) are often the most credible benchmark for methods in societal systems, but their cost and latency create a major bottleneck for iterative method development. LLM-based persona simulation offers a cheap synthetic alternative, yet it is unclear whether replacing humans with personas preserves the benchmark interface that adaptive methods optimize against. We prove an if-and-only-if characterization: when (i) methods observe only the aggregate outcome (aggregate-only observation) and (ii) evaluation depends only on the submitted artifact and not on the algorithm's identity or provenance (algorithm-blind evaluation), swapping humans for personas is just panel change from the method's point of view, indistinguishable from changing the evaluation population (e.g., New York to Jakarta). Furthermore, we move from validity to usefulness: we define an information-theoretic discriminability of the induced aggregate channel and show that making persona benchmarking as decision-relevant as a field experiment is fundamentally a sample-size question, yielding explicit bounds on the number of independent persona evaluations required to reliably distinguish meaningfully different methods at a chosen resolution.

</details>


### [110] [Beyond Context: Large Language Models Failure to Grasp Users Intent](https://arxiv.org/abs/2512.21110)
*Ahmed M. Hussain,Salahuddin Salahuddin,Panos Papadimitratos*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Current Large Language Models (LLMs) safety approaches focus on explicitly harmful content while overlooking a critical vulnerability: the inability to understand context and recognize user intent. This creates exploitable vulnerabilities that malicious users can systematically leverage to circumvent safety mechanisms. We empirically evaluate multiple state-of-the-art LLMs, including ChatGPT, Claude, Gemini, and DeepSeek. Our analysis demonstrates the circumvention of reliable safety mechanisms through emotional framing, progressive revelation, and academic justification techniques. Notably, reasoning-enabled configurations amplified rather than mitigated the effectiveness of exploitation, increasing factual precision while failing to interrogate the underlying intent. The exception was Claude Opus 4.1, which prioritized intent detection over information provision in some use cases. This pattern reveals that current architectural designs create systematic vulnerabilities. These limitations require paradigmatic shifts toward contextual understanding and intent recognition as core safety capabilities rather than post-hoc protective mechanisms.

</details>


### [111] [A Real-World Evaluation of LLM Medication Safety Reviews in NHS Primary Care](https://arxiv.org/abs/2512.21127)
*Oliver Normand,Esther Borsi,Mitch Fruin,Lauren E Walker,Jamie Heagerty,Chris C. Holmes,Anthony J Avery,Iain E Buchan,Harry Coppock*

Main category: cs.AI

TL;DR: 本研究首次在真实NHS初级医疗数据上评估了基于大语言模型（LLM）的用药安全审查系统，发现尽管LLM在检测临床问题方面表现良好（敏感性100%，特异性83.1%），但仅为46.9%的患者完全识别所有问题和干预措施。主要失败模式为语境推理而非药物知识缺失。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在医学基准测试中常达到或超过临床医生水平，但很少在真实临床数据上进行评估。本研究旨在全面评估LLM系统在真实英国国家医疗服务体系初级医疗环境中的实际表现，识别关键失败行为模式，为临床AI安全部署提供依据。

Method: 采用回顾性研究设计，使用涵盖2,125,549名成年人的NHS Cheshire和Merseyside人群规模电子健康记录。通过策略性抽样选择277名患者（已排除数据质量问题），覆盖广泛临床复杂性和用药安全风险。由专家临床医生审查这些患者，对系统识别的问题和提出的干预措施进行分级。

Result: 主要LLM系统检测临床问题的敏感性为100%，特异性为83.1%，但仅46.9%的患者中完全正确识别所有问题和干预措施。失败分析显示主要失败机制是语境推理问题，包括五个模式：1）过度自信处理不确定性；2）未根据患者具体情况调整标准指南；3）误解实际医疗实践方式；4）事实错误；5）流程盲点。这些模式在不同患者复杂性、人口统计学特征以及多种最先进模型配置中持续存在。

Conclusion: 这项工作突显了基于LLM的临床AI在安全部署前必须解决的重大缺陷，特别是语境推理能力不足的问题。研究建议进行更大规模的前瞻性评估，并在临床情境中对LLM行为进行更深入研究。研究提供了45个详细病例分析，全面涵盖了所有识别出的失败案例。

Abstract: Large language models (LLMs) often match or exceed clinician-level performance on medical benchmarks, yet very few are evaluated on real clinical data or examined beyond headline metrics. We present, to our knowledge, the first evaluation of an LLM-based medication safety review system on real NHS primary care data, with detailed characterisation of key failure behaviours across varying levels of clinical complexity. In a retrospective study using a population-scale EHR spanning 2,125,549 adults in NHS Cheshire and Merseyside, we strategically sampled patients to capture a broad range of clinical complexity and medication safety risk, yielding 277 patients after data-quality exclusions. An expert clinician reviewed these patients and graded system-identified issues and proposed interventions. Our primary LLM system showed strong performance in recognising when a clinical issue is present (sensitivity 100\% [95\% CI 98.2--100], specificity 83.1\% [95\% CI 72.7--90.1]), yet correctly identified all issues and interventions in only 46.9\% [95\% CI 41.1--52.8] of patients. Failure analysis reveals that, in this setting, the dominant failure mechanism is contextual reasoning rather than missing medication knowledge, with five primary patterns: overconfidence in uncertainty, applying standard guidelines without adjusting for patient context, misunderstanding how healthcare is delivered in practice, factual errors, and process blindness. These patterns persisted across patient complexity and demographic strata, and across a range of state-of-the-art models and configurations. We provide 45 detailed vignettes that comprehensively cover all identified failure cases. This work highlights shortcomings that must be addressed before LLM-based clinical AI can be safely deployed. It also begs larger-scale, prospective evaluations and deeper study of LLM behaviours in clinical contexts.

</details>


### [112] [RoboSafe: Safeguarding Embodied Agents via Executable Safety Logic](https://arxiv.org/abs/2512.21220)
*Le Wang,Zonghao Ying,Xiao Yang,Quanchen Zou,Zhenfei Yin,Tianlin Li,Jian Yang,Yaodong Yang,Aishan Liu,Xianglong Liu*

Main category: cs.AI

TL;DR: 提出RoboSafe：一种通过可执行谓词安全逻辑实现的混合推理运行时安全保障系统，用于防范具身智能体中的危险指令。


<details>
  <summary>Details</summary>
Motivation: 基于视觉语言模型的具身智能体在执行复杂任务时容易受到危险指令的影响，现有防御方法依赖静态规则过滤或提示层控制，难以应对动态、时间依赖和上下文丰富的环境中的隐式风险。

Method: RoboSafe通过可执行谓词安全逻辑和混合长短安全记忆库，结合后向反思推理和前向预测推理这两个互补的推理过程。后向反思推理持续检查短期记忆中的轨迹以推断时间安全谓词并主动触发重新规划；前向预测推理通过从长期安全记忆和智能体多模态观察中生成上下文感知的安全谓词来预测风险。

Result: 在多个智能体上的实验表明，RoboSafe相比领先基线方法显著减少危险动作（风险发生减少36.8%），同时保持接近原始的任务性能。在物理机械臂上的真实世界评估进一步证实了其实用性。

Conclusion: RoboSafe提供了一个自适应、可验证的安全逻辑框架，既能解释又可作为代码执行，有效解决了具身智能体在动态环境中的运行时安全问题，在减少风险的同时保持了任务性能。

Abstract: Embodied agents powered by vision-language models (VLMs) are increasingly capable of executing complex real-world tasks, yet they remain vulnerable to hazardous instructions that may trigger unsafe behaviors. Runtime safety guardrails, which intercept hazardous actions during task execution, offer a promising solution due to their flexibility. However, existing defenses often rely on static rule filters or prompt-level control, which struggle to address implicit risks arising in dynamic, temporally dependent, and context-rich environments. To address this, we propose RoboSafe, a hybrid reasoning runtime safeguard for embodied agents through executable predicate-based safety logic. RoboSafe integrates two complementary reasoning processes on a Hybrid Long-Short Safety Memory. We first propose a Backward Reflective Reasoning module that continuously revisits recent trajectories in short-term memory to infer temporal safety predicates and proactively triggers replanning when violations are detected. We then propose a Forward Predictive Reasoning module that anticipates upcoming risks by generating context-aware safety predicates from the long-term safety memory and the agent's multimodal observations. Together, these components form an adaptive, verifiable safety logic that is both interpretable and executable as code. Extensive experiments across multiple agents demonstrate that RoboSafe substantially reduces hazardous actions (-36.8% risk occurrence) compared with leading baselines, while maintaining near-original task performance. Real-world evaluations on physical robotic arms further confirm its practicality. Code will be released upon acceptance.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [113] [Learning Evolving Latent Strategies for Multi-Agent Language Systems without Model Fine-Tuning](https://arxiv.org/abs/2512.20629)
*Wenlong Tang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This study proposes a multi-agent language framework that enables continual strategy evolution without fine-tuning the language model's parameters. The core idea is to liberate the latent vectors of abstract concepts from traditional static semantic representations, allowing them to be continuously updated through environmental interaction and reinforcement feedback. We construct a dual-loop architecture: the behavior loop adjusts action preferences based on environmental rewards, while the language loop updates the external latent vectors by reflecting on the semantic embeddings of generated text.
  Together, these mechanisms allow agents to develop stable and disentangled strategic styles over long-horizon multi-round interactions. Experiments show that agents' latent spaces exhibit clear convergence trajectories under reflection-driven updates, along with structured shifts at critical moments. Moreover, the system demonstrates an emergent ability to implicitly infer and continually adapt to emotional agents, even without shared rewards. These results indicate that, without modifying model parameters, an external latent space can provide language agents with a low-cost, scalable, and interpretable form of abstract strategic representation.

</details>


### [114] [Zero-Training Temporal Drift Detection for Transformer Sentiment Models: A Comprehensive Analysis on Authentic Social Media Streams](https://arxiv.org/abs/2512.20631)
*Aayam Bansal,Ishaan Gangwani*

Main category: cs.LG

TL;DR: 本研究对基于Transformer的情感模型进行了无需重新训练的时间漂移分析，在真实社交媒体数据上验证发现事件驱动期间模型准确率最大下降23.4%，引入四个新漂移指标优于基线方法


<details>
  <summary>Details</summary>
Motivation: 社交媒体中的动态事件内容可能导致预训练情感模型性能不稳定，现有监控方法通常依赖于重新训练或复杂的嵌入计算，需要开发高效的零训练方法来实时检测模型漂移

Method: 本研究提出无需重新训练的漂移分析方法，使用三个Transformer架构模型，在12279个真实社交媒体帖子上进行系统评估，引入四个新颖的漂移指标，并通过严格统计验证

Result: 在事件驱动期间模型准确率下降达到23.4%，置信度最大下降13.0%（Bootstrap 95% CI: [9.1%, 16.5%]），新漂移指标优于基于嵌入的基线方法，计算效率适合生产部署

Conclusion: 提出的零训练方法能够立即部署到实时情感监控系统，为理解Transformer模型在动态内容期间的行为提供了新见解，统计验证确认了在实际应用中的稳健检测能力

Abstract: We present a comprehensive zero-training temporal drift analysis of transformer-based sentiment models validated on authentic social media data from major real-world events. Through systematic evaluation across three transformer architectures and rigorous statistical validation on 12,279 authentic social media posts, we demonstrate significant model instability with accuracy drops reaching 23.4% during event-driven periods. Our analysis reveals maximum confidence drops of 13.0% (Bootstrap 95% CI: [9.1%, 16.5%]) with strong correlation to actual performance degradation. We introduce four novel drift metrics that outperform embedding-based baselines while maintaining computational efficiency suitable for production deployment. Statistical validation across multiple events confirms robust detection capabilities with practical significance exceeding industry monitoring thresholds. This zero-training methodology enables immediate deployment for real-time sentiment monitoring systems and provides new insights into transformer model behavior during dynamic content periods.

</details>


### [115] [Enhancing Lung Cancer Treatment Outcome Prediction through Semantic Feature Engineering Using Large Language Models](https://arxiv.org/abs/2512.20633)
*MunHwan Lee,Shaika Chowdhury,Xiaodi Li,Sivaraman Rajaganapathy,Eric W Klee,Ping Yang,Terence Sio,Liewei Wang,James Cerhan,Nansu NA Zong*

Main category: cs.LG

TL;DR: 该文引入了一种基于大语言模型（LLM）的目标导向知识提取（GKC）框架，用于将多模态的临床数据转化为高保真、任务对齐的特征，从而在稀疏、异构的肺癌真实世界数据中提升疗效预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 肺癌疗效预测面临挑战，主要源于真实世界电子健康数据的稀疏性、异质性和信息过载。传统模型难以捕捉多模态数据流中的语义信息，而大规模微调方法在临床工作流中不实用。

Method: 提出了基于大语言模型（LLM）的目标导向知识提取（GKC）框架，将实验室、基因组和用药数据转化为高保真、任务对齐的特征表示。该框架作为离线预处理步骤，可无缝集成到医院信息系统中。

Result: 在184例肺癌患者队列中，GKC方法的平均AUROC为0.803（95% CI: 0.799-0.807），优于专家特征工程、直接文本嵌入和端到端Transformer基线。消融研究证实了三种模态结合的有效性。

Conclusion: 结果表明，在稀疏临床数据的预测任务中，语义表示的质量是决定预测准确性的关键因素。将LLM重新定义为知识提取引擎而非黑盒预测器，为肿瘤学AI决策支持提供了可扩展、可解释且兼容工作流的实现路径。

Abstract: Accurate prediction of treatment outcomes in lung cancer remains challenging due to the sparsity, heterogeneity, and contextual overload of real-world electronic health data. Traditional models often fail to capture semantic information across multimodal streams, while large-scale fine-tuning approaches are impractical in clinical workflows. We introduce a framework that uses Large Language Models (LLMs) as Goal-oriented Knowledge Curators (GKC) to convert laboratory, genomic, and medication data into high-fidelity, task-aligned features. Unlike generic embeddings, GKC produces representations tailored to the prediction objective and operates as an offline preprocessing step that integrates naturally into hospital informatics pipelines. Using a lung cancer cohort (N=184), we benchmarked GKC against expert-engineered features, direct text embeddings, and an end-to-end transformer. Our approach achieved a mean AUROC of 0.803 (95% CI: 0.799-0.807) and outperformed all baselines. An ablation study further confirmed the complementary value of combining all three modalities. These results show that the quality of semantic representation is a key determinant of predictive accuracy in sparse clinical data settings. By reframing LLMs as knowledge curation engines rather than black-box predictors, this work demonstrates a scalable, interpretable, and workflow-compatible pathway for advancing AI-driven decision support in oncology.

</details>


### [116] [Real Time Detection and Quantitative Analysis of Spurious Forgetting in Continual Learning](https://arxiv.org/abs/2512.20634)
*Weiwei Wang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Catastrophic forgetting remains a fundamental challenge in continual learning for large language models. Recent work revealed that performance degradation may stem from spurious forgetting caused by task alignment disruption rather than true knowledge loss. However, this work only qualitatively describes alignment, relies on post-hoc analysis, and lacks automatic distinction mechanisms.
  We introduce the shallow versus deep alignment framework, providing the first quantitative characterization of alignment depth. We identify that current task alignment approaches suffer from shallow alignment - maintained only over the first few output tokens (approximately 3-5) - making models vulnerable to forgetting. This explains why spurious forgetting occurs, why it is reversible, and why fine-tuning attacks are effective.
  We propose a comprehensive framework addressing all gaps: (1) quantitative metrics (0-1 scale) to measure alignment depth across token positions; (2) real-time detection methods for identifying shallow alignment during training; (3) specialized analysis tools for visualization and recovery prediction; and (4) adaptive mitigation strategies that automatically distinguish forgetting types and promote deep alignment. Extensive experiments on multiple datasets and model architectures (Qwen2.5-3B to Qwen2.5-32B) demonstrate 86.2-90.6% identification accuracy and show that promoting deep alignment improves robustness against forgetting by 3.3-7.1% over baselines.

</details>


### [117] [Data-Free Pruning of Self-Attention Layers in LLMs](https://arxiv.org/abs/2512.20636)
*Dhananjay Saikumar,Blesson Varghese*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Many self-attention sublayers in large language models (LLMs) can be removed with little to no loss. We attribute this to the Attention Suppression Hypothesis: during pre-training, some deep attention layers learn to mute their own contribution, leaving the residual stream and the MLP to carry the representation. We propose Gate-Norm, a one-shot, weight-only criterion that ranks attention sublayers by query--key coupling and removes the least coupled ones, requiring no calibration data, no forward passes, no fine-tuning, and no specialized kernels. On 40-layer, 13B-parameter LLaMA models, Gate-Norm prunes the model in under a second. Pruning $8$--$16$ attention sublayers yields up to $1.30\times$ higher inference throughput while keeping average zero-shot accuracy within $2\%$ of the unpruned baseline across BoolQ, RTE, HellaSwag, WinoGrande, ARC-Easy/Challenge, and OpenBookQA. Across these settings, Gate-Norm matches data-driven pruning methods in accuracy while being $\sim 1000\times$ faster to score layers, enabling practical, data-free compression of LLMs.

</details>


### [118] [Forecasting N-Body Dynamics: A Comparative Study of Neural Ordinary Differential Equations and Universal Differential Equations](https://arxiv.org/abs/2512.20643)
*Suriya R S,Prathamesh Dinesh Joshi,Rajat Dandekar,Raj Dandekar,Sreedath Panat*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The n body problem, fundamental to astrophysics, simulates the motion of n bodies acting under the effect of their own mutual gravitational interactions. Traditional machine learning models that are used for predicting and forecasting trajectories are often data intensive black box models, which ignore the physical laws, thereby lacking interpretability. Whereas Scientific Machine Learning ( Scientific ML ) directly embeds the known physical laws into the machine learning framework. Through robust modelling in the Julia programming language, our method uses the Scientific ML frameworks: Neural ordinary differential equations (NODEs) and Universal differential equations (UDEs) to predict and forecast the system dynamics. In addition, an essential component of our analysis involves determining the forecasting breakdown point, which is the smallest possible amount of training data our models need to predict future, unseen data accurately. We employ synthetically created noisy data to simulate real-world observational limitations. Our findings indicate that the UDE model is much more data efficient, needing only 20% of data for a correct forecast, whereas the Neural ODE requires 90%.

</details>


### [119] [Q-RUN: Quantum-Inspired Data Re-uploading Networks](https://arxiv.org/abs/2512.20654)
*Wenbo Qiao,Shuaixian Wang,Peng Zhang,Yan Ming,Jiaming Zhao*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Data re-uploading quantum circuits (DRQC) are a key approach to implementing quantum neural networks and have been shown to outperform classical neural networks in fitting high-frequency functions. However, their practical application is limited by the scalability of current quantum hardware. In this paper, we introduce the mathematical paradigm of DRQC into classical models by proposing a quantum-inspired data re-uploading network (Q-RUN), which retains the Fourier-expressive advantages of quantum models without any quantum hardware. Experimental results demonstrate that Q-RUN delivers superior performance across both data modeling and predictive modeling tasks. Compared to the fully connected layers and the state-of-the-art neural network layers, Q-RUN reduces model parameters while decreasing error by approximately one to three orders of magnitude on certain tasks. Notably, Q-RUN can serve as a drop-in replacement for standard fully connected layers, improving the performance of a wide range of neural architectures. This work illustrates how principles from quantum machine learning can guide the design of more expressive artificial intelligence.

</details>


### [120] [MaskOpt: A Large-Scale Mask Optimization Dataset to Advance AI in Integrated Circuit Manufacturing](https://arxiv.org/abs/2512.20655)
*Yuting Hu,Lei Zhuang,Hua Xiang,Jinjun Xiong,Gi-Joon Nam*

Main category: cs.LG

TL;DR: MaskOpt：基于真实45nm节点IC设计的大规模掩模优化基准数据集，包含22.6万+个金属层和通孔层图块，支持不同上下文窗口大小，保留标准单元层次信息，用于评估深度学习掩模优化模型的性能


<details>
  <summary>Details</summary>
Motivation: 随着集成电路尺寸缩小至光刻波长以下，光学临近效应校正和逆光刻技术面临计算成本高、扩展性受限问题；现有深度学习方法多基于合成布局、忽视标准单元层次结构和周围环境，限制了实际掩模优化的适用性

Method: 从45nm节点真实IC设计中构建大型基准数据集，在标准单元布局处裁剪图块以保留单元信息，支持不同上下文窗口大小捕捉光学临近效应；评估多款先进深度学习掩模优化模型，进行上下文大小分析和输入消融研究

Result: 构建了包含104,714个金属层图块和121,952个通孔层图块的MaskOpt数据集；评估结果揭示了基线模型在不同指标间的权衡；分析证实周围几何形状和单元感知输入对准确掩模生成的重要性

Conclusion: MaskOpt为细胞和上下文感知的掩模优化提供了实用基准，推动了深度学习在掩模优化领域的应用，突出了在实际IC设计中考虑层次结构和环境上下文的重要性

Abstract: As integrated circuit (IC) dimensions shrink below the lithographic wavelength, optical lithography faces growing challenges from diffraction and process variability. Model-based optical proximity correction (OPC) and inverse lithography technique (ILT) remain indispensable but computationally expensive, requiring repeated simulations that limit scalability. Although deep learning has been applied to mask optimization, existing datasets often rely on synthetic layouts, disregard standard-cell hierarchy, and neglect the surrounding contexts around the mask optimization targets, thereby constraining their applicability to practical mask optimization. To advance deep learning for cell- and context-aware mask optimization, we present MaskOpt, a large-scale benchmark dataset constructed from real IC designs at the 45$\mathrm{nm}$ node. MaskOpt includes 104,714 metal-layer tiles and 121,952 via-layer tiles. Each tile is clipped at a standard-cell placement to preserve cell information, exploiting repeated logic gate occurrences. Different context window sizes are supported in MaskOpt to capture the influence of neighboring shapes from optical proximity effects. We evaluate state-of-the-art deep learning models for IC mask optimization to build up benchmarks, and the evaluation results expose distinct trade-offs across baseline models. Further context size analysis and input ablation studies confirm the importance of both surrounding geometries and cell-aware inputs in achieving accurate mask generation.

</details>


### [121] [Managing the Stochastic: Foundations of Learning in Neuro-Symbolic Systems for Software Engineering](https://arxiv.org/abs/2512.20660)
*Matthew Thompson*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Current approaches to AI coding agents appear to blur the lines between the Large Language Model (LLM) and the agent itself, asking the LLM to make decisions best left to deterministic processes. This leads to systems prone to stochastic failures such as gaming unit tests or hallucinating syntax. Drawing on established software engineering practices that provide deterministic frameworks for managing unpredictable processes, this paper proposes setting the control boundary such that the LLM is treated as a component of the environment environment -- preserving its creative stochasticity -- rather than the decision-making agent.
  A \textbf{Dual-State Architecture} is formalized, separating workflow state (deterministic control flow) from environment state (stochastic generation). \textbf{Atomic Action Pairs} couple generation with verification as indivisible transactions, where \textbf{Guard Functions} act as sensing actions that project probabilistic outputs onto observable workflow state. The framework is validated on three code generation tasks across 13 LLMs (1.3B--15B parameters). For qualified instruction-following models, task success rates improved by up to 66 percentage points at 1.2--2.1$\times$ baseline computational cost. The results suggest that architectural constraints can substitute for parameter scale in achieving reliable code generation.

</details>


### [122] [Dominating vs. Dominated: Generative Collapse in Diffusion Models](https://arxiv.org/abs/2512.20666)
*Hayeon Jeong,Jong-Seok Lee*

Main category: cs.LG

TL;DR: 研究发现多概念文本到图像生成中存在主导概念抑制其他概念的失衡现象，通过数据集和分析揭示了数据多样性不足与注意力机制中的分布式效应是主要原因。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像扩散模型在多概念提示生成时，常出现某一主导概念压制其他概念的现象，阻碍了可靠且可控的生成，需要系统性地研究其成因以改善生成质量。

Method: 提出了DominanceBench数据集，从数据和架构角度分析失衡；通过实验考察训练数据多样性与概念间干扰的关系，并分析交叉注意力动态及多头注意力机制的影响。

Result: 训练数据中实例多样性有限加剧了概念间的干扰；主导概念token会快速饱和注意力并在扩散步骤中逐步压制其他概念；多头注意力机制共同导致了主导-被主导失衡。

Conclusion: 研究揭示了生成崩溃的关键机制，为提升文本到图像生成的可靠性和可控性提供了重要见解。

Abstract: Text-to-image diffusion models have drawn significant attention for their ability to generate diverse and high-fidelity images. However, when generating from multi-concept prompts, one concept token often dominates the generation, suppressing the others-a phenomenon we term the Dominant-vs-Dominated (DvD) imbalance. To systematically analyze this imbalance, we introduce DominanceBench and examine its causes from both data and architectural perspectives. Through various experiments, we show that the limited instance diversity in training data exacerbates the inter-concept interference. Analysis of cross-attention dynamics further reveals that dominant tokens rapidly saturate attention, progressively suppressing others across diffusion timesteps. In addition, head ablation studies show that the DvD behavior arises from distributed attention mechanisms across multiple heads. Our findings provide key insights into generative collapse, advancing toward more reliable and controllable text-to-image generation.

</details>


### [123] [Forward Only Learning for Orthogonal Neural Networks of any Depth](https://arxiv.org/abs/2512.20668)
*Paul Caillon,Alex Colagrande,Erwan Fagnou,Blaise Delattre,Alexandre Allauzen*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Backpropagation is still the de facto algorithm used today to
  train neural networks.
  With the exponential growth of recent architectures, the
  computational cost of this algorithm also becomes a burden. The
  recent PEPITA and forward-only frameworks have proposed promising
  alternatives, but they failed to scale up to a handful of hidden
  layers, yet limiting their use.
  In this paper, we first analyze theoretically the main limitations of
  these approaches. It allows us the design of a forward-only
  algorithm, which is equivalent to backpropagation under the linear
  and orthogonal assumptions. By relaxing the linear assumption, we
  then introduce FOTON (Forward-Only Training of Orthogonal Networks)
  that bridges the gap with the backpropagation
  algorithm. Experimental results show that it outperforms PEPITA,
  enabling us to train neural networks of any depth, without the need
  for a backward pass.
  Moreover its performance on convolutional networks clearly opens up avenues for its application to more
  advanced architectures. The code is open-sourced at https://github.com/p0lcAi/FOTON .

</details>


### [124] [Improving Cardiac Risk Prediction Using Data Generation Techniques](https://arxiv.org/abs/2512.20669)
*Alexandre Cabodevila,Pedro Gamallo-Fernandez,Juan C. Vidal,Manuel Lama*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Cardiac rehabilitation constitutes a structured clinical process involving multiple interdependent phases, individualized medical decisions, and the coordinated participation of diverse healthcare professionals. This sequential and adaptive nature enables the program to be modeled as a business process, thereby facilitating its analysis. Nevertheless, studies in this context face significant limitations inherent to real-world medical databases: data are often scarce due to both economic costs and the time required for collection; many existing records are not suitable for specific analytical purposes; and, finally, there is a high prevalence of missing values, as not all patients undergo the same diagnostic tests. To address these limitations, this work proposes an architecture based on a Conditional Variational Autoencoder (CVAE) for the synthesis of realistic clinical records that are coherent with real-world observations. The primary objective is to increase the size and diversity of the available datasets in order to enhance the performance of cardiac risk prediction models and to reduce the need for potentially hazardous diagnostic procedures, such as exercise stress testing. The results demonstrate that the proposed architecture is capable of generating coherent and realistic synthetic data, whose use improves the accuracy of the various classifiers employed for cardiac risk detection, outperforming state-of-the-art deep learning approaches for synthetic data generation.

</details>


### [125] [Disentangling Fact from Sentiment: A Dynamic Conflict-Consensus Framework for Multimodal Fake News Detection](https://arxiv.org/abs/2512.20670)
*Weilin Zhou,Zonghao Ying,Junjie Mu,Shengwei Tian,Quanchen Zou,Deyue Zhang,Dongdong Yang,Xiangzheng Zhang*

Main category: cs.LG

TL;DR: DCCF是一种寻求不一致性的多模态假新闻检测框架，通过放大而非抑制跨模态矛盾来识别伪造证据，相比传统的一致性融合方法有明显改进


<details>
  <summary>Details</summary>
Motivation: 现有的多模态假新闻检测方法依赖一致性融合，但这种方法错误地将关键的跨模态差异视为噪声，导致过度平滑并稀释了伪造的重要证据。主流的一致性融合方法本质上是在最小化特征差异以对齐模态，但这种方法从根本上就是失败的，因为无意中平滑了作为伪造主要证据的微妙跨模态矛盾。

Method: 提出动态冲突-共识框架（DCCF），这是一种寻求不一致性的范式。首先，将输入解耦到独立的事实空间和情感空间，以区分客观不匹配和情感不一致。其次，采用物理启发的特征动态来迭代极化这些表示，主动提取最大信息量的冲突。最后，冲突-共识机制将局部差异在全局上下文中标准化，进行稳健的审议判断。

Result: 在三个真实世界数据集上的广泛实验表明，DCCF始终优于最先进的基线方法，平均准确率提高了3.52%。

Conclusion: DCCF通过寻求不一致性而非抑制差异的新范式，有效改进了多模态假新闻检测，证明了放大跨模态矛盾作为证据的价值。

Abstract: Prevalent multimodal fake news detection relies on consistency-based fusion, yet this paradigm fundamentally misinterprets critical cross-modal discrepancies as noise, leading to over-smoothing, which dilutes critical evidence of fabrication. Mainstream consistency-based fusion inherently minimizes feature discrepancies to align modalities, yet this approach fundamentally fails because it inadvertently smoothes out the subtle cross-modal contradictions that serve as the primary evidence of fabrication. To address this, we propose the Dynamic Conflict-Consensus Framework (DCCF), an inconsistency-seeking paradigm designed to amplify rather than suppress contradictions. First, DCCF decouples inputs into independent Fact and Sentiment spaces to distinguish objective mismatches from emotional dissonance. Second, we employ physics-inspired feature dynamics to iteratively polarize these representations, actively extracting maximally informative conflicts. Finally, a conflict-consensus mechanism standardizes these local discrepancies against the global context for robust deliberative judgment.Extensive experiments conducted on three real world datasets demonstrate that DCCF consistently outperforms state-of-the-art baselines, achieving an average accuracy improvement of 3.52\%.

</details>


### [126] [HyDRA: Hierarchical and Dynamic Rank Adaptation for Mobile Vision Language Model](https://arxiv.org/abs/2512.20674)
*Yuanhao Xi,Xiaohuan Bing,Ramin Yahyapour*

Main category: cs.LG

TL;DR: 提出HyDRA框架，通过分层和动态rank调度，高效微调移动端VLM，参数量不增但性能提升4.7%


<details>
  <summary>Details</summary>
Motivation: 移动端VLM训练计算成本高，标准LoRA固定rank不足以处理多模态任务，需要更灵活的rank适应机制

Method: 1. 分层优化：粗粒度分配不同层不同rank，细粒度调整层内rank；2. 动态调整：用轻量性能模型端到端自动优化rank分配

Result: 在多个基准测试中超越基线，平均提升4.7%，部分任务甚至超过全参数微调，且可训练参数未增加

Conclusion: HyDRA能有效平衡计算效率与模型性能，为移动VLM高效微调提供了可行方案

Abstract: Vision Language Models (VLMs) have undergone significant advancements, particularly with the emergence of mobile-oriented VLMs, which offer a wide range of application scenarios. However, the substantial computational requirements for training these models present a significant obstacle to their practical application. To address this issue, Low-Rank Adaptation (LoRA) has been proposed. Nevertheless, the standard LoRA with a fixed rank lacks sufficient capability for training mobile VLMs that process both text and image modalities. In this work, we introduce HyDRA, a parameter-efficient fine-tuning framework designed to implement hierarchical and dynamic rank scheduling for mobile VLMs. This framework incorporates two essential optimization strategies: (1) hierarchical optimization, which involves a coarse-grained approach that assigns different ranks to various layers, as well as a fine-grained method that adjusts ranks within individual layers, and (2) dynamic adjustment, which employs an end-to-end automatic optimization using a lightweight performance model to determine and adjust ranks during the fine-tuning process. Comprehensive experiments conducted on popular benchmarks demonstrate that HyDRA consistently outperforms the baseline, achieving a 4.7\% improvement across various model sizes without increasing the number of trainable parameters. In some tasks, it even surpasses full-parameter fine-tuning.

</details>


### [127] [Revisiting the Learning Objectives of Vision-Language Reward Models](https://arxiv.org/abs/2512.20675)
*Simon Roy,Samuel Barbeau,Giovanni Beltrame,Christian Desrosiers,Nicolas Thome*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Learning generalizable reward functions is a core challenge in embodied intelligence. Recent work leverages contrastive vision language models (VLMs) to obtain dense, domain-agnostic rewards without human supervision. These methods adapt VLMs into reward models through increasingly complex learning objectives, yet meaningful comparison remains difficult due to differences in training data, architectures, and evaluation settings. In this work, we isolate the impact of the learning objective by evaluating recent VLM-based reward models under a unified framework with identical backbones, finetuning data, and evaluation environments. Using Meta-World tasks, we assess modeling accuracy by measuring consistency with ground truth reward and correlation with expert progress. Remarkably, we show that a simple triplet loss outperforms state-of-the-art methods, suggesting that much of the improvements in recent approaches could be attributed to differences in data and architectures.

</details>


### [128] [PHOTON: Hierarchical Autoregressive Modeling for Lightspeed and Memory-Efficient Language Generation](https://arxiv.org/abs/2512.20687)
*Yuma Ichikawa,Naoya Takagi,Takumi Nakagawa,Yuzi Kanazawa,Akira Sakai*

Main category: cs.LG

TL;DR: 基于层次化自回归的模型，用垂直多分辨率上下文访问替代传统Transformer的平面扫描机制，显著提升长上下文解码的吞吐率和内存效率


<details>
  <summary>Details</summary>
Motivation: Transformer在逐token扫描时，由于KV缓存随序列增长而增大，导致预填充延迟增加，长上下文解码变得内存受限，推理吞吐率受KV缓存读写而非计算主导

Method: 提出并行层次化操作模型，通过层次化潜在流：自底向上编码器将token压缩为低速率上下文状态，轻量级自顶向下解码器重建细粒度token表示，实现垂直多分辨率上下文访问

Result: 在吞吐率-质量权衡方面优于Transformer基语言模型，在长上下文和多查询任务中具有显著优势，将解码时KV缓存流量减少，实现单位内存下高达10^3倍的吞吐率提升

Conclusion: 层次化自回归模型通过改变传统transformer的访问模式，有效解决了长上下文解码的内存瓶颈问题，为高效推理提供了新的架构方向

Abstract: Transformers operate as horizontal token-by-token scanners; at each generation step, the model attends to an ever-growing sequence of token-level states. This access pattern increases prefill latency and makes long-context decoding increasingly memory-bound, as KV-cache reads and writes dominate inference throughput rather than arithmetic computation. We propose Parallel Hierarchical Operation for Top-down Networks (PHOTON), a hierarchical autoregressive model that replaces flat scanning with vertical, multi-resolution context access. PHOTON maintains a hierarchy of latent streams: a bottom-up encoder progressively compresses tokens into low-rate contextual states, while lightweight top-down decoders reconstruct fine-grained token representations. Experimental results show that PHOTON is superior to competitive Transformer-based language models regarding the throughput-quality trade-off, offering significant advantages in long-context and multi-query tasks. This reduces decode-time KV-cache traffic, yielding up to $10^{3}\times$ higher throughput per unit memory.

</details>


### [129] [FEM-Bench: A Structured Scientific Reasoning Benchmark for Evaluating Code-Generating LLMs](https://arxiv.org/abs/2512.20732)
*Saeed Mohammadzadeh,Erfan Hamdi,Joel Shor,Emma Lejeune*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: As LLMs advance their reasoning capabilities about the physical world, the absence of rigorous benchmarks for evaluating their ability to generate scientifically valid physical models has become a critical gap. Computational mechanics, which develops and applies mathematical models and numerical methods to predict the behavior of physical systems under forces, deformation, and constraints, provides an ideal foundation for structured scientific reasoning evaluation. Problems follow clear mathematical structure, enforce strict physical and numerical constraints, and support objective verification. The discipline requires constructing explicit models of physical systems and reasoning about geometry, spatial relationships, and material behavior, connecting directly to emerging AI goals in physical reasoning and world modeling. We introduce FEM-Bench, a computational mechanics benchmark designed to evaluate the ability of LLMs to generate correct finite element method (FEM) and related code. FEM-Bench 2025 contains a suite of introductory but nontrivial tasks aligned with material from a first graduate course on computational mechanics. These tasks capture essential numerical and physical modeling challenges while representing only a small fraction of the complexity present in the discipline. Despite their simplicity, state-of-the-art LLMs do not reliably solve all of them. In a five attempt run, the best performing model at function writing, Gemini 3 Pro, completed 30/33 tasks at least once and 26/33 tasks all five times. The best performing model at unit test writing, GPT-5, had an Average Joint Success Rate of 73.8%. Other popular models showed broad performance variation. FEM-Bench establishes a structured foundation for evaluating AI-generated scientific code, and future iterations will incorporate increasingly sophisticated tasks to track progress as models evolve.

</details>


### [130] [Stabilizing Multimodal Autoencoders: A Theoretical and Empirical Analysis of Fusion Strategies](https://arxiv.org/abs/2512.20749)
*Diyar Altinses,Andreas Schwung*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In recent years, the development of multimodal autoencoders has gained significant attention due to their potential to handle multimodal complex data types and improve model performance. Understanding the stability and robustness of these models is crucial for optimizing their training, architecture, and real-world applicability. This paper presents an analysis of Lipschitz properties in multimodal autoencoders, combining both theoretical insights and empirical validation to enhance the training stability of these models. We begin by deriving the theoretical Lipschitz constants for aggregation methods within the multimodal autoencoder framework. We then introduce a regularized attention-based fusion method, developed based on our theoretical analysis, which demonstrates improved stability and performance during training. Through a series of experiments, we empirically validate our theoretical findings by estimating the Lipschitz constants across multiple trials and fusion strategies. Our results demonstrate that our proposed fusion function not only aligns with theoretical predictions but also outperforms existing strategies in terms of consistency, convergence speed, and accuracy. This work provides a solid theoretical foundation for understanding fusion in multimodal autoencoders and contributes a solution for enhancing their performance.

</details>


### [131] [Generalization of RLVR Using Causal Reasoning as a Testbed](https://arxiv.org/abs/2512.20760)
*Brian Lu,Hongyu Zhao,Shuo Sun,Hao Peng,Rui Ding,Hongyuan Mei*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Reinforcement learning with verifiable rewards (RLVR) has emerged as a promising paradigm for post-training large language models (LLMs) on complex reasoning tasks. Yet, the conditions under which RLVR yields robust generalization remain poorly understood. This paper provides an empirical study of RLVR generalization in the setting of probabilistic inference over causal graphical models. This setting offers two natural axes along which to examine generalization: (i) the level of the probabilistic query -- associational, interventional, or counterfactual -- and (ii) the structural complexity of the query, measured by the size of its relevant subgraph. We construct datasets of causal graphs and queries spanning these difficulty axes and fine-tune Qwen-2.5-Instruct models using RLVR or supervised fine-tuning (SFT). We vary both the model scale (3B-32B) and the query level included in training. We find that RLVR yields stronger within-level and across-level generalization than SFT, but only for specific combinations of model size and training query level. Further analysis shows that RLVR's effectiveness depends on the model's initial reasoning competence. With sufficient initial competence, RLVR improves an LLM's marginalization strategy and reduces errors in intermediate probability calculations, producing substantial accuracy gains, particularly on more complex queries. These findings show that RLVR can improve specific causal reasoning subskills, with its benefits emerging only when the model has sufficient initial competence.

</details>


### [132] [TS-Arena Technical Report -- A Pre-registered Live Forecasting Platform](https://arxiv.org/abs/2512.20761)
*Marcel Meyer,Sascha Kaltenpoth,Kevin Zalipski,Henrik Albers,Oliver Müller*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: While Time Series Foundation Models (TSFMs) offer transformative capabilities for forecasting, they simultaneously risk triggering a fundamental evaluation crisis. This crisis is driven by information leakage due to overlapping training and test sets across different models, as well as the illegitimate transfer of global patterns to test data. While the ability to learn shared temporal dynamics represents a primary strength of these models, their evaluation on historical archives often permits the exploitation of observed global shocks, which violates the independence required for valid benchmarking. We introduce TS-Arena, a platform that restores the operational integrity of forecasting by treating the genuinely unknown future as the definitive test environment. By implementing a pre-registration mechanism on live data streams, the platform ensures that evaluation targets remain physically non-existent during inference, thereby enforcing a strict global temporal split. This methodology establishes a moving temporal frontier that prevents historical contamination and provides an authentic assessment of model generalization. Initially applied within the energy sector, TS-Arena provides a sustainable infrastructure for comparing foundation models under real-world constraints. A prototype of the platform is available at https://huggingface.co/spaces/DAG-UPB/TS-Arena.

</details>


### [133] [Subgroup Discovery with the Cox Model](https://arxiv.org/abs/2512.20762)
*Zachary Izzo,Iain Melvin*

Main category: cs.LG

TL;DR: 本文首次研究生存分析中的亚组发现问题，提出新的评估指标EPE和CRS，开发八种算法用于Cox模型的亚组发现，并通过实验验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有亚组发现方法缺乏适合Cox生存模型的评估指标，无法有效寻找高精度预测的亚组。需要解决现有指标在生存分析中的局限性。

Method: 1. 提出两个技术创新：预测熵(EPE)用于评估风险函数预测模型，条件秩统计(CRS)用于量化个体与亚组生存时间分布的偏差。2. 开发八种Cox亚组发现算法，主算法结合EPE和CRS，提供理论正确性保证。

Result: 1. 理论分析表明EPE和CRS能解决现有指标问题。2. 在合成和真实数据上的实验表明，方法能恢复真实亚组，相比在整个数据集上拟合Cox模型获得更好的模型拟合度。3. NASA喷气发动机数据的案例研究发现了已知的非线性/同质性，其设计选择建议在实践中得到验证。

Conclusion: 该研究首次系统解决了Cox生存模型的亚组发现问题，提出的新评估指标和算法能够有效识别高精度预测的可解释亚组，在实际应用中有良好效果。

Abstract: We study the problem of subgroup discovery for survival analysis, where the goal is to find an interpretable subset of the data on which a Cox model is highly accurate. Our work is the first to study this particular subgroup problem, for which we make several contributions.
  Subgroup discovery methods generally require a "quality function" in order to sift through and select the most advantageous subgroups. We first examine why existing natural choices for quality functions are insufficient to solve the subgroup discovery problem for the Cox model. To address the shortcomings of existing metrics, we introduce two technical innovations: the *expected prediction entropy (EPE)*, a novel metric for evaluating survival models which predict a hazard function; and the *conditional rank statistics (CRS)*, a statistical object which quantifies the deviation of an individual point to the distribution of survival times in an existing subgroup. We study the EPE and CRS theoretically and show that they can solve many of the problems with existing metrics.
  We introduce a total of eight algorithms for the Cox subgroup discovery problem. The main algorithm is able to take advantage of both the EPE and the CRS, allowing us to give theoretical correctness results for this algorithm in a well-specified setting. We evaluate all of the proposed methods empirically on both synthetic and real data. The experiments confirm our theory, showing that our contributions allow for the recovery of a ground-truth subgroup in well-specified cases, as well as leading to better model fit compared to naively fitting the Cox model to the whole dataset in practical settings. Lastly, we conduct a case study on jet engine simulation data from NASA. The discovered subgroups uncover known nonlinearities/homogeneity in the data, and which suggest design choices which have been mirrored in practice.

</details>


### [134] [Improving Matrix Exponential for Generative AI Flows: A Taylor-Based Approach Beyond Paterson--Stockmeyer](https://arxiv.org/abs/2512.20777)
*Jorge Sastre,Daniel Faronbi,José Miguel Alonso,Peter Traver,Javier Ibáñez,Nuria Lloret*

Main category: cs.LG

TL;DR: 基于泰勒展开的矩阵指数算法优化，针对生成式AI的高吞吐需求进行设计，通过动态选择泰勒阶数和缩放因子来最小化计算量，在保证精度的同时显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 矩阵指数在科学计算和系统仿真中至关重要，尤其在控制理论、量子力学和现代生成式机器学习中应用广泛。传统Padé近似结合缩放平方法虽是标准方法，但新型泰勒基方法能在精度和计算复杂度上提供更优表现，特别适合生成式AI流程的高吞吐需求。

Method: 提出优化后的泰勒基矩阵指数算法，采用超越经典Paterson-Stockmeyer技术的多项式求值方案。进行严格的误差分析，开发动态选择策略，根据预设误差容限自适应选择泰勒阶数和缩放因子以最小化计算量。

Result: 大量数值实验表明，该方法相比现有最先进实现提供了显著加速，同时保持高数值稳定性。在保证精度的前提下，计算效率大幅提升。

Conclusion: 所提出的方法成为大规模生成式建模的高效工具，特别适合生成式AI应用场景，在计算性能和数值稳定性方面均表现出色。

Abstract: The matrix exponential is a fundamental operator in scientific computing and system simulation, with applications ranging from control theory and quantum mechanics to modern generative machine learning. While Padé approximants combined with scaling and squaring have long served as the standard, recent Taylor-based methods, which utilize polynomial evaluation schemes that surpass the classical Paterson--Stockmeyer technique, offer superior accuracy and reduced computational complexity. This paper presents an optimized Taylor-based algorithm for the matrix exponential, specifically designed for the high-throughput requirements of generative AI flows. We provide a rigorous error analysis and develop a dynamic selection strategy for the Taylor order and scaling factor to minimize computational effort under a prescribed error tolerance. Extensive numerical experiments demonstrate that our approach provides significant acceleration and maintains high numerical stability compared to existing state-of-the-art implementations. These results establish the proposed method as a highly efficient tool for large-scale generative modeling.

</details>


### [135] [FedMPDD: Communication-Efficient Federated Learning with Privacy Preservation Attributes via Projected Directional Derivative](https://arxiv.org/abs/2512.20814)
*Mohammadreza Rostami,Solmaz S. Kia*

Main category: cs.LG

TL;DR: FedMPDD是一种新颖的联邦学习算法，通过多投影定向导数压缩梯度，同时优化带宽使用和增强隐私保护，将通信成本从O(d)降低到O(m)，收敛速度与FedSGD相当，并提供可调隐私性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习面临通信带宽瓶颈和隐私泄露风险的双重挑战，现有方法往往在通信效率与隐私保护之间难以平衡。

Method: FedMPDD通过计算高维梯度在多个随机向量上的定向导数来编码梯度，压缩通信数据；服务器通过相同随机向量投影解码聚合信息，多投影克服单投影维度依赖的收敛限制。

Result: 理论分析显示FedMPDD以O(1/√K)速率收敛，与FedSGD性能匹配；实验验证方法有效，通信成本显著降低，同时低秩投影提供对抗梯度反演攻击的隐私保护。

Conclusion: FedMPDD成功平衡通信效率与隐私保护，通过多投影机制实现高效收敛和可调隐私，为联邦学习提供实用解决方案。

Abstract: This paper introduces \texttt{FedMPDD} (\textbf{Fed}erated Learning via \textbf{M}ulti-\textbf{P}rojected \textbf{D}irectional \textbf{D}erivatives), a novel algorithm that simultaneously optimizes bandwidth utilization and enhances privacy in Federated Learning. The core idea of \texttt{FedMPDD} is to encode each client's high-dimensional gradient by computing its directional derivatives along multiple random vectors. This compresses the gradient into a much smaller message, significantly reducing uplink communication costs from $\mathcal{O}(d)$ to $\mathcal{O}(m)$, where $m \ll d$. The server then decodes the aggregated information by projecting it back onto the same random vectors. Our key insight is that averaging multiple projections overcomes the dimension-dependent convergence limitations of a single projection. We provide a rigorous theoretical analysis, establishing that \texttt{FedMPDD} converges at a rate of $\mathcal{O}(1/\sqrt{K})$, matching the performance of FedSGD. Furthermore, we demonstrate that our method provides some inherent privacy against gradient inversion attacks due to the geometric properties of low-rank projections, offering a tunable privacy-utility trade-off controlled by the number of projections. Extensive experiments on benchmark datasets validate our theory and demonstrates our results.

</details>


### [136] [Defending against adversarial attacks using mixture of experts](https://arxiv.org/abs/2512.20821)
*Mohammad Meymani,Roozbeh Razavi-Far*

Main category: cs.LG

TL;DR: 该论文提出了一种基于专家混合架构的防御系统，通过对抗训练增强模型对对抗性威胁的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管机器学习在不同领域取得进展，但这些模型在面对对抗性威胁时表现出脆弱性。对抗性威胁包括创建难以察觉但能导致错误分类的对抗扰动、毒化训练数据损害模型性能或查询模型窃取敏感信息。因此需要设计更鲁棒的防御系统。

Method: 提出了一个防御系统，在专家混合架构中设计对抗训练模块。使用9个以ResNet-18为骨干的预训练专家模型，通过端到端训练联合更新专家模型参数和门控机制参数，进一步优化专家性能。

Result: 提出的防御系统在对抗性威胁下优于现有的先进防御系统和普通分类器，即使这些分类器使用了比该模型骨干更复杂的架构。

Conclusion: 专家混合架构结合对抗训练能有效提升机器学习模型对对抗性攻击的鲁棒性，为构建更安全的机器学习系统提供了有前景的方法。

Abstract: Machine learning is a powerful tool enabling full automation of a huge number of tasks without explicit programming. Despite recent progress of machine learning in different domains, these models have shown vulnerabilities when they are exposed to adversarial threats. Adversarial threats aim to hinder the machine learning models from satisfying their objectives. They can create adversarial perturbations, which are imperceptible to humans' eyes but have the ability to cause misclassification during inference. Moreover, they can poison the training data to harm the model's performance or they can query the model to steal its sensitive information. In this paper, we propose a defense system, which devises an adversarial training module within mixture-of-experts architecture to enhance its robustness against adversarial threats. In our proposed defense system, we use nine pre-trained experts with ResNet-18 as their backbone. During end-to-end training, the parameters of expert models and gating mechanism are jointly updated allowing further optimization of the experts. Our proposed defense system outperforms state-of-the-art defense systems and plain classifiers, which use a more complex architecture than our model's backbone.

</details>


### [137] [Memory-Efficient Acceleration of Block Low-Rank Foundation Models on Resource Constrained GPUs](https://arxiv.org/abs/2512.20861)
*Pierre Abillama,Changwoo Lee,Juechu Dong,David Blaauw,Dennis Sylvester,Hun-Seok Kim*

Main category: cs.LG

TL;DR: 研究通过自定义Triton内核优化块低秩压缩方法，解决多令牌推理时内存瓶颈问题，在内存受限GPU上实现显著加速和模型压缩。


<details>
  <summary>Details</summary>
Motivation: transformer基础模型规模快速增长导致单个GPU难以加载完整模型，计算成本高昂。块低秩压缩方法虽能保留准确性并减少计算内存，但实际部署中多令牌推理常变为内存受限，增加延迟。

Method: 利用屋面分析揭示多令牌推理瓶颈，为Monarch和BLAST引入自定义Triton内核，采用部分融合和内存布局优化技术提升内存访问效率。

Result: 在NVIDIA Jetson Orin Nano和A40等内存受限GPU上，相比PyTorch密集基线和CUDA后端编译优化，获得最高3.76倍加速和3倍模型大小压缩，支持Llama-7/1B、GPT2-S、DiT-XL/2、ViT-B等多种模型。

Conclusion: 自定义Triton内核能有效解决块低秩压缩方法在实际部署中的内存瓶颈问题，为资源受限环境下的高效模型推理提供了实用解决方案。

Abstract: Recent advances in transformer-based foundation models have made them the default choice for many tasks, but their rapidly growing size makes fitting a full model on a single GPU increasingly difficult and their computational cost prohibitive. Block low-rank (BLR) compression techniques address this challenge by learning compact representations of weight matrices. While traditional low-rank (LR) methods often incur sharp accuracy drops, BLR approaches such as Monarch and BLAST can better capture the underlying structure, thus preserving accuracy while reducing computations and memory footprints. In this work, we use roofline analysis to show that, although BLR methods achieve theoretical savings and practical speedups for single-token inference, multi-token inference often becomes memory-bound in practice, increasing latency despite compiler-level optimizations in PyTorch. To address this, we introduce custom Triton kernels with partial fusion and memory layout optimizations for both Monarch and BLAST. On memory-constrained NVIDIA GPUs such as Jetson Orin Nano and A40, our kernels deliver up to $3.76\times$ speedups and $3\times$ model size compression over PyTorch dense baselines using CUDA backend and compiler-level optimizations, while supporting various models including Llama-7/1B, GPT2-S, DiT-XL/2, and ViT-B. Our code is available at https://github.com/pabillam/mem-efficient-blr .

</details>


### [138] [Robustness Certificates for Neural Networks against Adversarial Attacks](https://arxiv.org/abs/2512.20865)
*Sara Taheri,Mahalakshmi Sabanayagam,Debarghya Ghoshdastidar,Majid Zamani*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The increasing use of machine learning in safety-critical domains amplifies the risk of adversarial threats, especially data poisoning attacks that corrupt training data to degrade performance or induce unsafe behavior. Most existing defenses lack formal guarantees or rely on restrictive assumptions about the model class, attack type, extent of poisoning, or point-wise certification, limiting their practical reliability. This paper introduces a principled formal robustness certification framework that models gradient-based training as a discrete-time dynamical system (dt-DS) and formulates poisoning robustness as a formal safety verification problem. By adapting the concept of barrier certificates (BCs) from control theory, we introduce sufficient conditions to certify a robust radius ensuring that the terminal model remains safe under worst-case ${\ell}_p$-norm based poisoning. To make this practical, we parameterize BCs as neural networks trained on finite sets of poisoned trajectories. We further derive probably approximately correct (PAC) bounds by solving a scenario convex program (SCP), which yields a confidence lower bound on the certified robustness radius generalizing beyond the training set. Importantly, our framework also extends to certification against test-time attacks, making it the first unified framework to provide formal guarantees in both training and test-time attack settings. Experiments on MNIST, SVHN, and CIFAR-10 show that our approach certifies non-trivial perturbation budgets while being model-agnostic and requiring no prior knowledge of the attack or contamination level.

</details>


### [139] [Time-Efficient Evaluation and Enhancement of Adversarial Robustness in Deep Neural Networks](https://arxiv.org/abs/2512.20893)
*Runqi Lin*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: With deep neural networks (DNNs) increasingly embedded in modern society, ensuring their safety has become a critical and urgent issue. In response, substantial efforts have been dedicated to the red-blue adversarial framework, where the red team focuses on identifying vulnerabilities in DNNs and the blue team on mitigating them. However, existing approaches from both teams remain computationally intensive, constraining their applicability to large-scale models. To overcome this limitation, this thesis endeavours to provide time-efficient methods for the evaluation and enhancement of adversarial robustness in DNNs.

</details>


### [140] [DiEC: Diffusion Embedded Clustering](https://arxiv.org/abs/2512.20905)
*Haidong Hu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Deep clustering hinges on learning representations that are inherently clusterable. However, using a single encoder to produce a fixed embedding ignores the representation trajectory formed by a pretrained diffusion model across network hierarchies and noise timesteps, where clusterability varies substantially. We propose DiEC (Diffusion Embedded Clustering), which performs unsupervised clustering by directly reading internal activations from a pretrained diffusion U-Net.
  DiEC formulates representation selection as a two-dimensional search over layer x timestep, and exploits a weak-coupling property to decompose it into two stages. Specifically, we first fix the U-Net bottleneck layer as the Clustering-friendly Middle Layer (CML), and then use Optimal Timestep Search (OTS) to identify the clustering-optimal timestep (t*). During training, we extract bottleneck features at the fixed t* and obtain clustering representations via a lightweight residual mapping. We optimize a DEC-style KL self-training objective, augmented with adaptive graph regularization and entropy regularization to strengthen cluster structures. In parallel, we introduce a denoising-consistency branch at random timesteps to stabilize the representations and preserve generative consistency. Experiments show that DiEC achieves competitive clustering performance on multiple standard benchmarks.

</details>


### [141] [Towards a General Framework for Predicting and Explaining the Hardness of Graph-based Combinatorial Optimization Problems using Machine Learning and Association Rule Mining](https://arxiv.org/abs/2512.20915)
*Bharat Sharman,Elkafi Hassini*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This study introduces GCO-HPIF, a general machine-learning-based framework to predict and explain the computational hardness of combinatorial optimization problems that can be represented on graphs. The framework consists of two stages. In the first stage, a dataset is created comprising problem-agnostic graph features and hardness classifications of problem instances. Machine-learning-based classification algorithms are trained to map graph features to hardness categories. In the second stage, the framework explains the predictions using an association rule mining algorithm. Additionally, machine-learning-based regression models are trained to predict algorithmic computation times. The GCO-HPIF framework was applied to a dataset of 3287 maximum clique problem instances compiled from the COLLAB, IMDB, and TWITTER graph datasets using five state-of-the-art algorithms, namely three exact branch-and-bound-based algorithms (Gurobi, CliSAT, and MOMC) and two graph-neural-network-based algorithms (EGN and HGS). The framework demonstrated excellent performance in predicting instance hardness, achieving a weighted F1 score of 0.9921, a minority-class F1 score of 0.878, and an ROC-AUC score of 0.9083 using only three graph features. The best association rule found by the FP-Growth algorithm for explaining the hardness predictions had a support of 0.8829 for hard instances and an overall accuracy of 87.64 percent, underscoring the framework's usefulness for both prediction and explanation. Furthermore, the best-performing regression model for predicting computation times achieved a percentage RMSE of 5.12 and an R2 value of 0.991.

</details>


### [142] [RevFFN: Memory-Efficient Full-Parameter Fine-Tuning of Mixture-of-Experts LLMs with Reversible Blocks](https://arxiv.org/abs/2512.20920)
*Ningyuan Liu,Jing Yang,Kaitong Cai,Keze Wang*

Main category: cs.LG

TL;DR: RevFFN是一种用于Mixture of Experts (MoE) 大语言模型的高效内存微调方法，通过可逆Transformer块重构激活值，显著降低完整参数微调时的内存峰值使用。


<details>
  <summary>Details</summary>
Motivation: 完整参数微调在适应大语言模型到下游任务时面临严重的内存瓶颈，需要缓存大量中间激活值用于反向传播，这对GPU内存要求很高。现有的分布式训练框架虽然能缓解这个问题，但需要额外的硬件资源并降低训练速度。

Method: RevFFN采用精心设计的可逆Transformer块，在反向传播过程中可以从输出重构层输入激活，从而消除存储大多数中间激活值的需要。该方法保留了MoE架构的表达能力。

Result: RevFFN显著降低了完整参数微调的峰值内存消耗，使得在单个消费级或服务器级GPU上实现高效完整微调成为可能。

Conclusion: RevFFN为MoE大语言模型提供了一种内存高效的完整参数微调范式，解决了传统方法中的内存瓶颈问题，使在有限硬件资源上进行高效微调成为现实。

Abstract: Full parameter fine tuning is a key technique for adapting large language models (LLMs) to downstream tasks, but it incurs substantial memory overhead due to the need to cache extensive intermediate activations for backpropagation. This bottleneck makes full fine tuning of contemporary large scale LLMs challenging in practice. Existing distributed training frameworks such as DeepSpeed alleviate this issue using techniques like ZeRO and FSDP, which rely on multi GPU memory or CPU offloading, but often require additional hardware resources and reduce training speed. We introduce RevFFN, a memory efficient fine tuning paradigm for mixture of experts (MoE) LLMs. RevFFN employs carefully designed reversible Transformer blocks that allow reconstruction of layer input activations from outputs during backpropagation, eliminating the need to store most intermediate activations in memory. While preserving the expressive capacity of MoE architectures, this approach significantly reduces peak memory consumption for full parameter fine tuning. As a result, RevFFN enables efficient full fine tuning on a single consumer grade or server grade GPU.

</details>


### [143] [Guardrailed Elasticity Pricing: A Churn-Aware Forecasting Playbook for Subscription Strategy](https://arxiv.org/abs/2512.20932)
*Deepit Sapru*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper presents a marketing analytics framework that operationalizes subscription pricing as a dynamic, guardrailed decision system, uniting multivariate demand forecasting, segment-level price elasticity, and churn propensity to optimize revenue, margin, and retention. The approach blends seasonal time-series models with tree-based learners, runs Monte Carlo scenario tests to map risk envelopes, and solves a constrained optimization that enforces business guardrails on customer experience, margin floors, and allowable churn. Validated across heterogeneous SaaS portfolios, the method consistently outperforms static tiers and uniform uplifts by reallocating price moves toward segments with higher willingness-to-pay while protecting price-sensitive cohorts. The system is designed for real-time recalibration via modular APIs and includes model explainability for governance and compliance. Managerially, the framework functions as a strategy playbook that clarifies when to shift from flat to dynamic pricing, how to align pricing with CLV and MRR targets, and how to embed ethical guardrails, enabling durable growth without eroding customer trust.

</details>


### [144] [A Multi-fidelity Double-Delta Wing Dataset and Empirical Scaling Laws for GNN-based Aerodynamic Field Surrogate](https://arxiv.org/abs/2512.20941)
*Yiren Shen,Juan J. Alonso*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Data-driven surrogate models are increasingly adopted to accelerate vehicle design. However, open-source multi-fidelity datasets and empirical guidelines linking dataset size to model performance remain limited. This study investigates the relationship between training data size and prediction accuracy for a graph neural network (GNN) based surrogate model for aerodynamic field prediction. We release an open-source, multi-fidelity aerodynamic dataset for double-delta wings, comprising 2448 flow snapshots across 272 geometries evaluated at angles of attack from 11 (degree) to 19 (degree) at Ma=0.3 using both Vortex Lattice Method (VLM) and Reynolds-Averaged Navier-Stokes (RANS) solvers. The geometries are generated using a nested Saltelli sampling scheme to support future dataset expansion and variance-based sensitivity analysis. Using this dataset, we conduct a preliminary empirical scaling study of the MF-VortexNet surrogate by constructing six training datasets with sizes ranging from 40 to 1280 snapshots and training models with 0.1 to 2.4 million parameters under a fixed training budget. We find that the test error decreases with data size with a power-law exponent of -0.6122, indicating efficient data utilization. Based on this scaling law, we estimate that the optimal sampling density is approximately eight samples per dimension in a d-dimensional design space. The results also suggest improved data utilization efficiency for larger surrogate models, implying a potential trade-off between dataset generation cost and model training budget.

</details>


### [145] [Solving Functional PDEs with Gaussian Processes and Applications to Functional Renormalization Group Equations](https://arxiv.org/abs/2512.20956)
*Xianjin Yang,Matthieu Darcy,Matthew Hudes,Francis J. Alexander,Gregory Eyink,Houman Owhadi*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We present an operator learning framework for solving non-perturbative functional renormalization group equations, which are integro-differential equations defined on functionals. Our proposed approach uses Gaussian process operator learning to construct a flexible functional representation formulated directly on function space, making it independent of a particular equation or discretization. Our method is flexible, and can apply to a broad range of functional differential equations while still allowing for the incorporation of physical priors in either the prior mean or the kernel design. We demonstrate the performance of our method on several relevant equations, such as the Wetterich and Wilson--Polchinski equations, showing that it achieves equal or better performance than existing approximations such as the local-potential approximation, while being significantly more flexible. In particular, our method can handle non-constant fields, making it promising for the study of more complex field configurations, such as instantons.

</details>


### [146] [Measuring all the noises of LLM Evals](https://arxiv.org/abs/2512.21326)
*Sida Wang*

Main category: cs.LG

TL;DR: 本文针对LLM评估中的噪声问题，提出了全面测量预测噪声、数据噪声及其总和的方法，设计了一种全对配对分析技术，发现了评估系统可预测的噪声模式，并证明通过平均降低预测噪声能显著提高统计功效。


<details>
  <summary>Details</summary>
Motivation: LLM评估中有效应用统计方法需要理解其独特的噪声特性。现有研究未能清晰定义和测量这些噪声，导致了评估结果的不确定性和统计功效的降低。

Method: 提出全对配对分析方法，应用于所有LLM模型对，基于数百万问题级预测测量三种噪声：预测噪声、数据噪声及其总和，并遵循总方差定律进行量化。

Result: 发现：1）每个评估系统在所有模型对中都表现出特征性且高度可预测的总噪声水平；2）配对预测噪声通常超过配对数据噪声，表明通过平均降低预测噪声能显著提高统计功效。

Conclusion: 这些发现使实践者能够无需定制测试即可评估显著性，并在受控实验中检测更小的效应，为LLM评估提供了更可靠的统计基础。

Abstract: Separating signal from noise is central to experimental science. Applying well-established statistical method effectively to LLM evals requires consideration of their unique noise characteristics. We clearly define and measure three types of noise: prediction noise from generating different answers on a given question, data noise from sampling questions, and their combined total noise following the law of total variance. To emphasize relative comparisons and gain statistical power, we propose the all-pairs paired method, which applies the paired analysis to all pairs of LLMs and measures all the noise components based on millions of question-level predictions across many evals and settings. These measurements revealed clear patterns. First, each eval exhibits a characteristic and highly predictable total noise level across all model pairs. Second, paired prediction noise typically exceeds paired data noise, which means reducing prediction noise by averaging can significantly increase statistical power. These findings enable practitioners to assess significance without custom testing and to detect much smaller effects in controlled experiments.

</details>


### [147] [Can Agentic AI Match the Performance of Human Data Scientists?](https://arxiv.org/abs/2512.20959)
*An Luo,Jin Du,Fangqiao Tian,Xun Xian,Robert Specht,Ganghua Wang,Xuan Bi,Charles Fleming,Jayanth Srinivasa,Ashish Kundu,Mingyi Hong,Jie Ding*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Data science plays a critical role in transforming complex data into actionable insights across numerous domains. Recent developments in large language models (LLMs) have significantly automated data science workflows, but a fundamental question persists: Can these agentic AI systems truly match the performance of human data scientists who routinely leverage domain-specific knowledge? We explore this question by designing a prediction task where a crucial latent variable is hidden in relevant image data instead of tabular features. As a result, agentic AI that generates generic codes for modeling tabular data cannot perform well, while human experts could identify the important hidden variable using domain knowledge. We demonstrate this idea with a synthetic dataset for property insurance. Our experiments show that agentic AI that relies on generic analytics workflow falls short of methods that use domain-specific insights. This highlights a key limitation of the current agentic AI for data science and underscores the need for future research to develop agentic AI systems that can better recognize and incorporate domain knowledge.

</details>


### [148] [Generalization of Diffusion Models Arises with a Balanced Representation Space](https://arxiv.org/abs/2512.20963)
*Zekai Zhang,Xiao Li,Xiang Li,Lianghe Shi,Meng Wu,Molei Tao,Qing Qu*

Main category: cs.LG

TL;DR: 本文分析扩散模型中记忆化与泛化的本质区别，从表示学习角度提出理论证明和实践验证，并提出基于表示的检测与编辑方法。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成高质量多样样本时可能过度记忆训练数据，但其记忆化与泛化的本质区别尚不清楚。本文旨在通过表示学习理论分析这一区别，并为实际应用提供检测和控制方法。

Method: 首先通过两层ReLU去噪自编码器理论分析，证明记忆化对应权重中存储原始样本（尖峰表示），而泛化对应捕获局部统计（平衡表示）。然后在真实无条件/文生图扩散模型上验证。最后提出基于表示的检测方法和无需训练的表示引导编辑技术。

Result: 理论证明记忆化与泛化对应不同的表示结构（尖峰 vs 平衡），实验验证这些结构在深度生成模型中实际存在。提出的检测方法能有效识别记忆化，编辑技术能精确控制生成内容。

Conclusion: 学习良好表示是新颖且有意义的生成建模的核心。记忆化存储原始样本，泛化捕获数据统计；基于表示的分析为理解和控制扩散模型提供了理论框架和实用工具。

Abstract: Diffusion models excel at generating high-quality, diverse samples, yet they risk memorizing training data when overfit to the training objective. We analyze the distinctions between memorization and generalization in diffusion models through the lens of representation learning. By investigating a two-layer ReLU denoising autoencoder (DAE), we prove that (i) memorization corresponds to the model storing raw training samples in the learned weights for encoding and decoding, yielding localized "spiky" representations, whereas (ii) generalization arises when the model captures local data statistics, producing "balanced" representations. Furthermore, we validate these theoretical findings on real-world unconditional and text-to-image diffusion models, demonstrating that the same representation structures emerge in deep generative models with significant practical implications. Building on these insights, we propose a representation-based method for detecting memorization and a training-free editing technique that allows precise control via representation steering. Together, our results highlight that learning good representations is central to novel and meaningful generative modeling.

</details>


### [149] [Generalised Linear Models in Deep Bayesian RL with Learnable Basis Functions](https://arxiv.org/abs/2512.20974)
*Jingyang You,Hanna Kurniawati*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Bayesian Reinforcement Learning (BRL) provides a framework for generalisation of Reinforcement Learning (RL) problems from its use of Bayesian task parameters in the transition and reward models. However, classical BRL methods assume known forms of transition and reward models, reducing their applicability in real-world problems. As a result, recent deep BRL methods have started to incorporate model learning, though the use of neural networks directly on the joint data and task parameters requires optimising the Evidence Lower Bound (ELBO). ELBOs are difficult to optimise and may result in indistinctive task parameters, hence compromised BRL policies. To this end, we introduce a novel deep BRL method, Generalised Linear Models in Deep Bayesian RL with Learnable Basis Functions (GLiBRL), that enables efficient and accurate learning of transition and reward models, with fully tractable marginal likelihood and Bayesian inference on task parameters and model noises. On challenging MetaWorld ML10/45 benchmarks, GLiBRL improves the success rate of one of the state-of-the-art deep BRL methods, VariBAD, by up to 2.7x. Comparing against representative or recent deep BRL / Meta-RL methods, such as MAML, RL2, SDVT, TrMRL and ECET, GLiBRL also demonstrates its low-variance and decent performance consistently.

</details>


### [150] [LLM Swiss Round: Aggregating Multi-Benchmark Performance via Competitive Swiss-System Dynamics](https://arxiv.org/abs/2512.21010)
*Jiashuo Liu,Jiayun Wu,Chunjie Wu,Jingkai Liu,Zaiyuan Wang,Huan Zhou,Wenhao Huang,Hongseok Namkoong*

Main category: cs.LG

TL;DR: 该论文提出竞争性瑞士轮动态评估框架，旨在解决传统LLM评估方法在跨任务统一排序和动态竞争适应性方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型评估主要采用静态评分方法，存在两大缺陷：1)难以确定不同基准测试的权重比例；2)无法捕捉模型在连续高风险任务中的动态竞争能力和脆弱性。需要从分散的任务特定评估转向整体性的竞争性排名系统。

Method: 提出竞争性瑞士轮动态评估框架，通过多轮连续竞赛模拟模型竞争，基于累计胜负记录动态配对模型参与基准测试序列。使用10万次蒙特卡洛模拟计算期望获胜分数，消除随机配对和早期胜负的噪音。通过参数化每轮淘汰数量进行失败敏感性分析，可区分鲁棒通才和激进专才的风险偏好。

Result: CSD框架相比传统聚合评分和静态配对模型，能够提供更细致、基于上下文的排名结果。

Conclusion: CSD代表了向风险知情的新一代LLM评估迈出的重要一步，通过动态竞争模拟实现了更具上下文感知能力的模型评估。

Abstract: The rapid proliferation of Large Language Models (LLMs) and diverse specialized benchmarks necessitates a shift from fragmented, task-specific metrics to a holistic, competitive ranking system that effectively aggregates performance across multiple ability dimensions. Primarily using static scoring, current evaluation methods are fundamentally limited. They struggle to determine the proper mix ratio across diverse benchmarks, and critically, they fail to capture a model's dynamic competitive fitness or its vulnerability when confronted with sequential, high-stakes tasks. To address this, we introduce the novel Competitive Swiss-System Dynamics (CSD) framework. CSD simulates a multi-round, sequential contest where models are dynamically paired across a curated sequence of benchmarks based on their accumulated win-loss record. And Monte Carlo Simulation ($N=100,000$ iterations) is used to approximate the statistically robust Expected Win Score ($E[S_m]$), which eliminates the noise of random pairing and early-round luck. Furthermore, we implement a Failure Sensitivity Analysis by parameterizing the per-round elimination quantity ($T_k$), which allows us to profile models based on their risk appetite--distinguishing between robust generalists and aggressive specialists. We demonstrate that CSD provides a more nuanced and context-aware ranking than traditional aggregate scoring and static pairwise models, representing a vital step towards risk-informed, next-generation LLM evaluation.

</details>


### [151] [Understanding Scaling Laws in Deep Neural Networks via Feature Learning Dynamics](https://arxiv.org/abs/2512.21075)
*Zihan Yao,Ruoyu Wu,Tianxiang Gao*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The empirical success of deep learning is often attributed to scaling laws that predict consistent gains as model, data, and compute grow; however, large models can exhibit training instability and diminishing returns, suggesting that scaling laws describe what success looks like but not when and why scaling succeeds or fails. A central obstacle is the lack of a rigorous understanding of feature learning at large depth. While muP characterizes feature-learning dynamics in the infinite-width limit and enables hyperparameter transfer across width, its depth extension (depth-muP) breaks down for residual blocks with more than one internal layer. We derive Neural Feature Dynamics (NFD) for ResNets with single-layer residual blocks, characterizing feature learning via a coupled forward-backward stochastic system in the joint infinite-width and infinite-depth limit. In this regime, NFD identifies when scaling-law trends persist and explains diminishing returns. It also reveals a vanishing mechanism induced by the 1/sqrt(depth) residual scaling under which the gradient-independence assumption (GIA), known to fail during training at finite depth, becomes provably valid again at infinite depth, yielding an analytically tractable regime for end-to-end feature learning. Motivated by this insight, we study two-layer residual blocks and show that the same mechanism causes feature-learning collapse in the first internal layer at large depth, providing a structural explanation for the empirical failure of depth-muP. Based on this diagnosis, we propose a depth-aware learning-rate correction that counteracts the collapse and empirically restores depth-wise hyperparameter transfer, yielding stronger performance in deeper ResNets.

</details>


### [152] [Shared Representation Learning for High-Dimensional Multi-Task Forecasting under Resource Contention in Cloud-Native Backends](https://arxiv.org/abs/2512.21102)
*Zixiao Huang,Jixiao Yang,Sijia Li,Chi Zhang,Jinyu Chen,Chengda Xu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This study proposes a unified forecasting framework for high-dimensional multi-task time series to meet the prediction demands of cloud native backend systems operating under highly dynamic loads, coupled metrics, and parallel tasks. The method builds a shared encoding structure to represent diverse monitoring indicators in a unified manner and employs a state fusion mechanism to capture trend changes and local disturbances across different time scales. A cross-task structural propagation module is introduced to model potential dependencies among nodes, enabling the model to understand complex structural patterns formed by resource contention, link interactions, and changes in service topology. To enhance adaptability to non-stationary behaviors, the framework incorporates a dynamic adjustment mechanism that automatically regulates internal feature flows according to system state changes, ensuring stable predictions in the presence of sudden load shifts, topology drift, and resource jitter. The experimental evaluation compares multiple models across various metrics and verifies the effectiveness of the framework through analyses of hyperparameter sensitivity, environmental sensitivity, and data sensitivity. The results show that the proposed method achieves superior performance on several error metrics and provides more accurate representations of future states under different operating conditions. Overall, the unified forecasting framework offers reliable predictive capability for high-dimensional, multi-task, and strongly dynamic environments in cloud native systems and provides essential technical support for intelligent backend management.

</details>


### [153] [A Mechanistic Analysis of Transformers for Dynamical Systems](https://arxiv.org/abs/2512.21113)
*Gregory Duthé,Nikolaos Evangelou,Wei Liu,Ioannis G. Kevrekidis,Eleni Chatzi*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Transformers are increasingly adopted for modeling and forecasting time-series, yet their internal mechanisms remain poorly understood from a dynamical systems perspective. In contrast to classical autoregressive and state-space models, which benefit from well-established theoretical foundations, Transformer architectures are typically treated as black boxes. This gap becomes particularly relevant as attention-based models are considered for general-purpose or zero-shot forecasting across diverse dynamical regimes. In this work, we do not propose a new forecasting model, but instead investigate the representational capabilities and limitations of single-layer Transformers when applied to dynamical data. Building on a dynamical systems perspective we interpret causal self-attention as a linear, history-dependent recurrence and analyze how it processes temporal information. Through a series of linear and nonlinear case studies, we identify distinct operational regimes. For linear systems, we show that the convexity constraint imposed by softmax attention fundamentally restricts the class of dynamics that can be represented, leading to oversmoothing in oscillatory settings. For nonlinear systems under partial observability, attention instead acts as an adaptive delay-embedding mechanism, enabling effective state reconstruction when sufficient temporal context and latent dimensionality are available. These results help bridge empirical observations with classical dynamical systems theory, providing insight into when and why Transformers succeed or fail as models of dynamical systems.

</details>


### [154] [STLDM: Spatio-Temporal Latent Diffusion Model for Precipitation Nowcasting](https://arxiv.org/abs/2512.21118)
*Shi Quan Foo,Chi-Ho Wong,Zhihan Gao,Dit-Yan Yeung,Ka-Hing Wong,Wai-Kin Wong*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Precipitation nowcasting is a critical spatio-temporal prediction task for society to prevent severe damage owing to extreme weather events. Despite the advances in this field, the complex and stochastic nature of this task still poses challenges to existing approaches. Specifically, deterministic models tend to produce blurry predictions while generative models often struggle with poor accuracy. In this paper, we present a simple yet effective model architecture termed STLDM, a diffusion-based model that learns the latent representation from end to end alongside both the Variational Autoencoder and the conditioning network. STLDM decomposes this task into two stages: a deterministic forecasting stage handled by the conditioning network, and an enhancement stage performed by the latent diffusion model. Experimental results on multiple radar datasets demonstrate that STLDM achieves superior performance compared to the state of the art, while also improving inference efficiency. The code is available in https://github.com/sqfoo/stldm_official.

</details>


### [155] [MODE: Multi-Objective Adaptive Coreset Selection](https://arxiv.org/abs/2512.21152)
*Tanmoy Mukherjee,Pierre Marquis,Zied Bouraoui*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We present Mode(Multi-Objective adaptive Data Efficiency), a framework that dynamically combines coreset selection strategies based on their evolving contribution to model performance. Unlike static methods, \mode adapts selection criteria to training phases: emphasizing class balance early, diversity during representation learning, and uncertainty at convergence. We show that MODE achieves (1-1/e)-approximation with O(n \log n) complexity and demonstrates competitive accuracy while providing interpretable insights into data utility evolution. Experiments show \mode reduces memory requirements

</details>


### [156] [A Unified Framework for EEG Seizure Detection Using Universum-Integrated Generalized Eigenvalues Proximal Support Vector Machine](https://arxiv.org/abs/2512.21170)
*Yogesh Kumar,Vrushank Ahire,M. A. Ganaie*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The paper presents novel Universum-enhanced classifiers: the Universum Generalized Eigenvalue Proximal Support Vector Machine (U-GEPSVM) and the Improved U-GEPSVM (IU-GEPSVM) for EEG signal classification. Using the computational efficiency of generalized eigenvalue decomposition and the generalization benefits of Universum learning, the proposed models address critical challenges in EEG analysis: non-stationarity, low signal-to-noise ratio, and limited labeled data. U-GEPSVM extends the GEPSVM framework by incorporating Universum constraints through a ratio-based objective function, while IU-GEPSVM enhances stability through a weighted difference-based formulation that provides independent control over class separation and Universum alignment. The models are evaluated on the Bonn University EEG dataset across two binary classification tasks: (O vs S)-healthy (eyes closed) vs seizure, and (Z vs S)-healthy (eyes open) vs seizure. IU-GEPSVM achieves peak accuracies of 85% (O vs S) and 80% (Z vs S), with mean accuracies of 81.29% and 77.57% respectively, outperforming baseline methods.

</details>


### [157] [Analytic and Variational Stability of Deep Learning Systems](https://arxiv.org/abs/2512.21208)
*Ronald Katende*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We propose a unified analytic and variational framework for studying stability in deep learning systems viewed as coupled representation-parameter dynamics. The central object is the Learning Stability Profile, which tracks the infinitesimal response of representations, parameters, and update mechanisms to perturbations along the learning trajectory. We prove a Fundamental Analytic Stability Theorem showing that uniform boundedness of these stability signatures is equivalent, up to norm equivalence, to the existence of a Lyapunov-type energy that dissipates along the learning flow. In smooth regimes, the framework yields explicit stability exponents linking spectral norms, activation regularity, step sizes, and learning rates to contractivity of the learning dynamics. Classical spectral stability results for feedforward networks, a discrete CFL-type condition for residual architectures, and parametric and temporal stability laws for stochastic gradient methods arise as direct consequences. The theory extends to non-smooth learning systems, including ReLU networks, proximal and projected updates, and stochastic subgradient flows, by replacing classical derivatives with Clarke generalized derivatives and smooth energies with variational Lyapunov functionals. The resulting framework provides a unified dynamical description of stability across architectures and optimization methods, clarifying how architectural and algorithmic choices jointly govern robustness and sensitivity to perturbations. It also provides a foundation for further extensions to continuous-time limits and geometric formulations of learning dynamics.

</details>


### [158] [Model Merging via Multi-Teacher Knowledge Distillation](https://arxiv.org/abs/2512.21288)
*Seyed Arshan Dalili,Mehrdad Mahdavi*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Model merging has emerged as a lightweight alternative to joint multi-task learning (MTL), yet the generalization properties of merged models remain largely unexplored. Establishing such theoretical guarantees is non-trivial, as the merging process typically forbids access to the original training data and involves combining fine-tuned models trained on fundamentally heterogeneous data distributions. Without a principled understanding of these dynamics, current methods often rely on heuristics to approximate the optimal combination of parameters. This dependence is most critical in coefficient scaling, the weighting factors that modulate the magnitude of each fine-tuned model's contribution to the shared parameter. However, without a principled objective to guide their selection, these methods lead to brittle performance and are highly sensitive to scaling initialization. We address this gap by (i) establishing a novel flatness-aware PAC-Bayes generalization bound specifically for the model merging setting. This analysis introduces a "cross-task heterogeneity" term that formally captures the mismatch between diverse fine-tuned model priors and the target multi-task distributions. Guided by this theoretical insight, (ii) we frame model merging as multi-teacher knowledge distillation on scarce, unlabeled data. We formally demonstrate that minimizing the student-teacher Kullback-Leibler divergence directly tightens the upper bound on the merged model's excess risk. Guided by the flatness-aware bound derived, (iii) we operationalize this objective via SAMerging, a method that employs Sharpness-Aware Minimization (SAM) to find flat minima. Empirically, SAMerging establishes a new state of the art across vision and NLP benchmarks, achieving remarkable performance. The code is available at https://github.com/arshandalili/SAMerging.

</details>


### [159] [Learning to Solve PDEs on Neural Shape Representations](https://arxiv.org/abs/2512.21311)
*Lilian Welschinger,Yilin Liu,Zican Wang,Niloy Mitra*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Solving partial differential equations (PDEs) on shapes underpins many shape analysis and engineering tasks; yet, prevailing PDE solvers operate on polygonal/triangle meshes while modern 3D assets increasingly live as neural representations. This mismatch leaves no suitable method to solve surface PDEs directly within the neural domain, forcing explicit mesh extraction or per-instance residual training, preventing end-to-end workflows. We present a novel, mesh-free formulation that learns a local update operator conditioned on neural (local) shape attributes, enabling surface PDEs to be solved directly where the (neural) data lives. The operator integrates naturally with prevalent neural surface representations, is trained once on a single representative shape, and generalizes across shape and topology variations, enabling accurate, fast inference without explicit meshing or per-instance optimization while preserving differentiability. Across analytic benchmarks (heat equation and Poisson solve on sphere) and real neural assets across different representations, our method slightly outperforms CPM while remaining reasonably close to FEM, and, to our knowledge, delivers the first end-to-end pipeline that solves surface PDEs on both neural and classical surface representations. Code will be released on acceptance.

</details>


### [160] [Does the Data Processing Inequality Reflect Practice? On the Utility of Low-Level Tasks](https://arxiv.org/abs/2512.21315)
*Roy Turgeman,Tom Tirer*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The data processing inequality is an information-theoretic principle stating that the information content of a signal cannot be increased by processing the observations. In particular, it suggests that there is no benefit in enhancing the signal or encoding it before addressing a classification problem. This assertion can be proven to be true for the case of the optimal Bayes classifier. However, in practice, it is common to perform "low-level" tasks before "high-level" downstream tasks despite the overwhelming capabilities of modern deep neural networks. In this paper, we aim to understand when and why low-level processing can be beneficial for classification. We present a comprehensive theoretical study of a binary classification setup, where we consider a classifier that is tightly connected to the optimal Bayes classifier and converges to it as the number of training samples increases. We prove that for any finite number of training samples, there exists a pre-classification processing that improves the classification accuracy. We also explore the effect of class separation, training set size, and class balance on the relative gain from this procedure. We support our theory with an empirical investigation of the theoretical setup. Finally, we conduct an empirical study where we investigate the effect of denoising and encoding on the performance of practical deep classifiers on benchmark datasets. Specifically, we vary the size and class distribution of the training set, and the noise level, and demonstrate trends that are consistent with our theoretical results.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [161] [Uncovering Patterns of Brain Activity from EEG Data Consistently Associated with Cybersickness Using Neural Network Interpretability Maps](https://arxiv.org/abs/2512.20620)
*Jacqueline Yau,Katherine J. Mimnaugh,Evan G. Center,Timo Ojala,Steven M. LaValle,Wenzhen Yuan,Nancy Amato,Minje Kim,Kara Federmeier*

Main category: cs.HC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Cybersickness poses a serious challenge for users of virtual reality (VR) technology. Consequently, there has been significant effort to track its occurrence during VR use with brain activity through electroencephalography (EEG). However, a significant confound in current methods for detecting sickness from EEG is they do not account for the simultaneous processing of the sickening visual stimulus that is present in the brain data from VR. Using event-related potentials (ERPs) from an auditory stimulus shown to reflect cybersickness impacts, we can more precisely target EEG cybersickness features and use those to achieve better performance in online cybersickness classification. In this article, we introduce a method utilizing trained convolutional neural networks and transformer models and plot interpretability maps from integrated gradients and class activation to give a visual representation of what the model determined was most useful in sickness classification from an EEG dataset consisting of ERPs recorded during the elicitation of cybersickness. Across 12 runs of our method with three different neural networks, the models consistently pointed to a surprising finding: that amplitudes recorded at an electrode placed on the scalp near the left prefrontal cortex were important in the classification of cybersickness. These results help clarify a hidden pattern in other related research and point to exciting opportunities for future investigation: that this scalp location could be used as a tagged feature for better real-time cybersickness classification with EEG. We provide our code at: [anonymized].

</details>


### [162] [Pioneering Multimodal Emotion Recognition in the Era of Large Models: From Closed Sets to Open Vocabularies](https://arxiv.org/abs/2512.20938)
*Jing Han,Zhiqiang Gao,Shihao Gao,Jialing Liu,Hongyu Chen,Zixing Zhang,Björn W. Schuller*

Main category: cs.HC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent advances in multimodal large language models (MLLMs) have demonstrated remarkable multi- and cross-modal integration capabilities. However, their potential for fine-grained emotion understanding remains systematically underexplored. While open-vocabulary multimodal emotion recognition (MER-OV) has emerged as a promising direction to overcome the limitations of closed emotion sets, no comprehensive evaluation of MLLMs in this context currently exists. To address this, our work presents the first large-scale benchmarking study of MER-OV on the OV-MERD dataset, evaluating 19 mainstream MLLMs, including general-purpose, modality-specialized, and reasoning-enhanced architectures. Through systematic analysis of model reasoning capacity, fusion strategies, contextual utilization, and prompt design, we provide key insights into the capabilities and limitations of current MLLMs for MER-OV. Our evaluation reveals that a two-stage, trimodal (audio, video, and text) fusion approach achieves optimal performance in MER-OV, with video emerging as the most critical modality. We further identify a surprisingly narrow gap between open- and closed-source LLMs. These findings establish essential benchmarks and offer practical guidelines for advancing open-vocabulary and fine-grained affective computing, paving the way for more nuanced and interpretable emotion AI systems. Associated code will be made publicly available upon acceptance.

</details>


### [163] [A Design Study Process Model for Medical Visualization](https://arxiv.org/abs/2512.21034)
*Mengjie Fan,Liang Zhou*

Main category: cs.HC

TL;DR: 提出基于医学可视化与可视分析研究的医学可视化设计研究过程模型，强调区分利益相关者、按分析逻辑划分设计阶段、按假设类别分类任务类型，提升设计的针对性、可推广性和可操作性。


<details>
  <summary>Details</summary>
Motivation: 现有设计研究过程模型在应对医学问题的复杂性和多样性方面不足，需要针对医学可视化特点建立更系统、实用的理论框架和指导方法。

Method: 通过文献综述分析现有医学可视化工作，结合跨学科研究经验，构建新的设计过程模型，强调利益相关者区分、分析逻辑驱动的设计阶段划分、假设分类的任务类型，并应用于具体可视化方法和现有工作的重新分析。

Result: 新模型能够提供系统的理论框架和实践指导，适用于不同类型的医学问题，通过三个实际案例验证了其有效性和适用性。

Conclusion: 该模型为跨学科医学可视化研究提供了系统性指导，增强了可视化设计的针对性、可推广性和可操作性，并为未来研究提供了参考建议。

Abstract: We introduce a design study process model for medical visualization based on the analysis of existing medical visualization and visual analysis works, and our own interdisciplinary research experience. With a literature review of related works covering various data types and applications, we identify features of medical visualization and visual analysis research and formulate our model thereafter. Compared to previous design study process models, our new model emphasizes: distinguishing between different stakeholders and target users before initiating specific designs, distinguishing design stages according to analytic logic or cognitive habits, and classifying task types as inferential or descriptive, and further hypothesis-based or hypothesis-free based on whether they involve multiple subgroups. In addition, our model refines previous models according to the characteristics of medical problems and provides referable guidance for each step. These improvements make the visualization design targeted, generalizable, and operational, which can adapt to the complexity and diversity of medical problems. We apply this model to guide the design of a visual analysis method and reanalyze three medical visualization-related works. These examples suggest that the new process model can provide a systematic theoretical framework and practical guidance for interdisciplinary medical visualization research. We give recommendations that future researchers can refer to, report on reflections on the model, and delineate it from existing models.

</details>


### [164] [When LLMs fall short in Deductive Coding: Model Comparison and Human AI Collaboration Workflow Design](https://arxiv.org/abs/2512.21041)
*Zijian Li,Luzhen Tang,Mengyu Xia,Xinyu Li,Naping Chen,Dragan Gašević,Yizhou Fan*

Main category: cs.HC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: With generative artificial intelligence driving the growth of dialogic data in education, automated coding is a promising direction for learning analytics to improve efficiency. This surge highlights the need to understand the nuances of student-AI interactions, especially those rare yet crucial. However, automated coding may struggle to capture these rare codes due to imbalanced data, while human coding remains time-consuming and labour-intensive. The current study examined the potential of large language models (LLMs) to approximate or replace humans in deductive, theory-driven coding, while also exploring how human-AI collaboration might support such coding tasks at scale. We compared the coding performance of small transformer classifiers (e.g., BERT) and LLMs in two datasets, with particular attention to imbalanced head-tail distributions in dialogue codes. Our results showed that LLMs did not outperform BERT-based models and exhibited systematic errors and biases in deductive coding tasks. We designed and evaluated a human-AI collaborative workflow that improved coding efficiency while maintaining coding reliability. Our findings reveal both the limitations of LLMs -- especially their difficulties with semantic similarity and theoretical interpretations and the indispensable role of human judgment -- while demonstrating the practical promise of human-AI collaborative workflows for coding.

</details>


### [165] [Learning Factors in AI-Augmented Education: A Comparative Study of Middle and High School Students](https://arxiv.org/abs/2512.21246)
*Gaia Ebli,Bianca Raimondi,Maurizio Gabbrielli*

Main category: cs.HC

TL;DR: 该研究探讨了K-12教育中AI增强编程学习环境下，关键学习因素（经验、清晰度、舒适度和动力）之间的关联如何在初中和高中生之间呈现差异化的维度结构。


<details>
  <summary>Details</summary>
Motivation: 现有AI教育研究多关注高等教育和常规教学环境，缺乏对K-12教育、尤其是AI增强学习环境中关键学习因素关系的研究，也不清楚这些关系如何随年龄组变化。

Method: 在真实课堂环境中，让学生与AI工具进行编程学习交互，收集四个学习因素数据和学生认知。采用多方法定量分析，结合相关性分析和文本挖掘。

Result: 研究发现两个年龄组的维度结构存在显著差异：初中生在所有维度间呈现强正相关性，表现出整体性评估模式；高中生则在关键维度间呈现弱相关或接近零相关，表现出更分化、独立的评估过程。

Conclusion: 研究表明，学习认知维度在AI增强学习中起中介作用，而发展阶段调节了这些维度的相互依赖关系。这为制定考虑学习者发展阶段和年龄特异性维度结构的AI整合策略奠定了基础。

Abstract: The increasing integration of AI tools in education has led prior research to explore their impact on learning processes. Nevertheless, most existing studies focus on higher education and conventional instructional contexts, leaving open questions about how key learning factors are related in AI-mediated learning environments and how these relationships may vary across different age groups. Addressing these gaps, our work investigates whether four critical learning factors, experience, clarity, comfort, and motivation, maintain coherent interrelationships in AI-augmented educational settings, and how the structure of these relationships differs between middle and high school students. The study was conducted in authentic classroom contexts where students interacted with AI tools as part of programming learning activities to collect data on the four learning factors and students' perceptions. Using a multimethod quantitative analysis, which combined correlation analysis and text mining, we revealed markedly different dimensional structures between the two age groups. Middle school students exhibit strong positive correlations across all dimensions, indicating holistic evaluation patterns whereby positive perceptions in one dimension generalise to others. In contrast, high school students show weak or near-zero correlations between key dimensions, suggesting a more differentiated evaluation process in which dimensions are assessed independently. These findings reveal that perception dimensions actively mediate AI-augmented learning and that the developmental stage moderates their interdependencies. This work establishes a foundation for the development of AI integration strategies that respond to learners' developmental levels and account for age-specific dimensional structures in student-AI interactions.

</details>
