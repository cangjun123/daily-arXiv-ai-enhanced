<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 4]
- [cs.CL](#cs.CL) [Total: 5]
- [cs.LG](#cs.LG) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Towards Blind and Low-Vision Accessibility of Lightweight VLMs and Custom LLM-Evals](https://arxiv.org/abs/2511.10615)
*Shruti Singh Baghel,Yash Pratap Singh Rathore,Sushovan Jena,Anurag Pradhan,Amit Shukla,Arnav Bhavsar,Pawan Goyal*

Main category: cs.CV

TL;DR: 本研究评估了SmolVLM2模型在生成盲人和低视力用户视频描述的有效性，提出了新的评估框架，并比较了不同参数大小和提示设计对可访问性的影响。


<details>
  <summary>Details</summary>
Motivation: To improve the practicality of vision-language models for blind and low-vision users who require detailed and context-aware video descriptions.

Method: Evaluated SmolVLM2 variants with different parameter sizes across two datasets using novel evaluation frameworks to assess accessibility for BLV users.

Result: Found that model size and prompt design significantly affect the quality of descriptions, with specific frameworks developed to assess accessibility.

Conclusion: SmolVLM2 variants show varying effectiveness in generating accessible video descriptions for BLV users, highlighting the importance of model size and prompt design on accessibility.

Abstract: Large Vision-Language Models (VLMs) excel at understanding and generating video descriptions but their high memory, computation, and deployment demands hinder practical use particularly for blind and low-vision (BLV) users who depend on detailed, context-aware descriptions. To study the effect of model size on accessibility-focused description quality, we evaluate SmolVLM2 variants with 500M and 2.2B parameters across two diverse datasets: AVCaps (outdoor), and Charades (indoor). In this work, we introduce two novel evaluation frameworks specifically designed for BLV accessibility assessment: the Multi-Context BLV Framework evaluating spatial orientation, social interaction, action events, and ambience contexts; and the Navigational Assistance Framework focusing on mobility-critical information. Additionally, we conduct a systematic evaluation of four different prompt design strategies and deploy both models on a smartphone, evaluating FP32 and INT8 precision variants to assess real-world performance constraints on resource-limited mobile devices.

</details>


### [2] [One Small Step in Latent, One Giant Leap for Pixels: Fast Latent Upscale Adapter for Your Diffusion Models](https://arxiv.org/abs/2511.10629)
*Aleksandr Razin,Danil Kazantsev,Ilya Makarov*

Main category: cs.CV

TL;DR: LUA是一种高效的超分辨率模块，通过在潜在空间中直接处理生成编码，实现高分辨率图像合成，显著提升速度与效率。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型在超分辨率上的表现有限，直接的高分辨率采样缓慢且成本高，而后处理的图像超分辨率引入了伪影和延迟。

Method: 提出了一种轻量级模块LUA，直接在生成器的潜在编码上执行超分辨率，集成为一个可以无缝加入的组件。

Result: LUA在维持相似感知质量的同时，解码和上采样时间减少近3倍，且能够在不同VAE的潜在空间中良好泛化。

Conclusion: LUA展示了在现代扩散模型中实现可扩展的高保真图像合成的有效路径，能够与原生高分辨率生成的保真度相匹配，同时提供了更快的解码和上采样时间。

Abstract: Diffusion models struggle to scale beyond their training resolutions, as direct high-resolution sampling is slow and costly, while post-hoc image super-resolution (ISR) introduces artifacts and additional latency by operating after decoding. We present the Latent Upscaler Adapter (LUA), a lightweight module that performs super-resolution directly on the generator's latent code before the final VAE decoding step. LUA integrates as a drop-in component, requiring no modifications to the base model or additional diffusion stages, and enables high-resolution synthesis through a single feed-forward pass in latent space. A shared Swin-style backbone with scale-specific pixel-shuffle heads supports 2x and 4x factors and remains compatible with image-space SR baselines, achieving comparable perceptual quality with nearly 3x lower decoding and upscaling time (adding only +0.42 s for 1024 px generation from 512 px, compared to 1.87 s for pixel-space SR using the same SwinIR architecture). Furthermore, LUA shows strong generalization across the latent spaces of different VAEs, making it easy to deploy without retraining from scratch for each new decoder. Extensive experiments demonstrate that LUA closely matches the fidelity of native high-resolution generation while offering a practical and efficient path to scalable, high-fidelity image synthesis in modern diffusion pipelines.

</details>


### [3] [Depth Anything 3: Recovering the Visual Space from Any Views](https://arxiv.org/abs/2511.10647)
*Haotong Lin,Sili Chen,Junhao Liew,Donny Y. Chen,Zhenyu Li,Guang Shi,Jiashi Feng,Bingyi Kang*

Main category: cs.CV

TL;DR: DA3是一个新的模型，能够从任意数量的视觉输入预测一致的几何结构，并在多个视觉几何基准测试中达到新的最先进水平。


<details>
  <summary>Details</summary>
Motivation: 探索最小建模策略，并提高视觉几何预测的准确性和一致性。

Method: 采用教师-学生训练范式，仅使用单一的Transformer作为主干，并通过单一的深度光射线预测目标简化了学习过程。

Result: DA3在相机位姿和几何准确度上分别平均超越了之前的VGGT模型44.3%和25.1%，并在单目深度估计中超越DA2。

Conclusion: DA3在所有任务上设立了新的最先进水平，尤其在相机位姿准确度和几何准确度方面显著优于之前的模型。

Abstract: We present Depth Anything 3 (DA3), a model that predicts spatially consistent geometry from an arbitrary number of visual inputs, with or without known camera poses. In pursuit of minimal modeling, DA3 yields two key insights: a single plain transformer (e.g., vanilla DINO encoder) is sufficient as a backbone without architectural specialization, and a singular depth-ray prediction target obviates the need for complex multi-task learning. Through our teacher-student training paradigm, the model achieves a level of detail and generalization on par with Depth Anything 2 (DA2). We establish a new visual geometry benchmark covering camera pose estimation, any-view geometry and visual rendering. On this benchmark, DA3 sets a new state-of-the-art across all tasks, surpassing prior SOTA VGGT by an average of 44.3% in camera pose accuracy and 25.1% in geometric accuracy. Moreover, it outperforms DA2 in monocular depth estimation. All models are trained exclusively on public academic datasets.

</details>


### [4] [Enhancing the Outcome Reward-based RL Training of MLLMs with Self-Consistency Sampling](https://arxiv.org/abs/2511.10648)
*Jiahao Wang,Weiye Xu,Aijun Yang,Wengang Zhou,Lewei Lu,Houqiang Li,Xiaohua Wang,Jinguo Zhu*

Main category: cs.CV

TL;DR: 本文提出了自一致性采样(SCS)方法，以提高多模态大语言模型在结果奖励强化学习中的可靠性，显著提升了模型的准确性。


<details>
  <summary>Details</summary>
Motivation: The study aims to tackle the problem of rewarding flawed reasoning pathways in reinforcement learning, which can mislead model training and reduce overall performance.

Method: SCS addresses unfaithful trajectories in reinforcement learning by introducing visual perturbations and performing repeated truncation and resampling of initial trajectories to derive a consistency score.

Result: Implementation of SCS improved accuracy by up to 7.7 percentage points on six multimodal benchmarks with negligible additional computation, demonstrating effectiveness in enhancing model reliability.

Conclusion: Self-Consistency Sampling (SCS) significantly enhances the reliability of outcome-reward reinforcement learning in multimodal large language models, achieving improved accuracy across multiple benchmarks with minimal computational cost.

Abstract: Outcome-reward reinforcement learning (RL) is a common and increasingly significant way to refine the step-by-step reasoning of multimodal large language models (MLLMs). In the multiple-choice setting - a dominant format for multimodal reasoning benchmarks - the paradigm faces a significant yet often overlooked obstacle: unfaithful trajectories that guess the correct option after a faulty chain of thought receive the same reward as genuine reasoning, which is a flaw that cannot be ignored. We propose Self-Consistency Sampling (SCS) to correct this issue. For each question, SCS (i) introduces small visual perturbations and (ii) performs repeated truncation and resampling of an initial trajectory; agreement among the resulting trajectories yields a differentiable consistency score that down-weights unreliable traces during policy updates. Based on Qwen2.5-VL-7B-Instruct, plugging SCS into RLOO, GRPO, and REINFORCE++ series improves accuracy by up to 7.7 percentage points on six multimodal benchmarks with negligible extra computation. SCS also yields notable gains on both Qwen2.5-VL-3B-Instruct and InternVL3-8B, offering a simple, general remedy for outcome-reward RL in MLLMs.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [5] [Know Your Limits: Entropy Estimation Modeling for Compression and Generalization](https://arxiv.org/abs/2511.10618)
*Benjamin L. Badger,Matthew Neligeorge*

Main category: cs.CL

TL;DR: 本文探讨了语言预测中的信息熵限制，并提出了一种新的模型架构，显示通过熵估算可提升语言模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 研究语言预测中的信息熵限制，以提高语言模型的准确性和压缩效率。

Method: 提出了编码增强的因果解码器架构，展示了如何在每个 token 基础上获取熵估计。

Result: 通过比较，展示了基于熵的模型训练具有更好的泛化能力。

Conclusion: 引入的编码增强因果解码器模型在训练效率和语言压缩方面表现优于传统的因果变换器模型。

Abstract: Language prediction is constrained by informational entropy intrinsic to language, such that there exists a limit to how accurate any language model can become and equivalently a lower bound to language compression. The most efficient language compression algorithms today are causal (next token prediction) large language models, but the use of these models to form accurate estimates of language entropy is currently computationally infeasible. We introduce encoder-augmented causal decoder model architectures that exhibit superior training efficiency characteristics and achieve higher compression than causal transformers even when trained on modest hardware. We demonstrate how entropy estimates can be obtained on a per-token basis, and show that the generalization of models trained to approach the entropy of their training data necessarily exceeds the generalization of models trained to minimize loss beyond this value. We show empirically that causal models trained to approach but not exceed estimated per-token entropies exhibit greater generalization than models trained without taking entropy into account.

</details>


### [6] [SSR: Socratic Self-Refine for Large Language Model Reasoning](https://arxiv.org/abs/2511.10621)
*Haizhou Shi,Ye Liu,Bo Pang,Zeyu Leo Liu,Hao Wang,Silvio Savarese,Caiming Xiong,Yingbo Zhou,Semih Yavuz*

Main category: cs.CL

TL;DR: 本文提出了一种新框架 SSR，通过细粒度评估和迭代改进 LLM 的推理能力，显著提升了性能并提供了内在推理过程的可理解性。


<details>
  <summary>Details</summary>
Motivation: 现有的测试框架多依赖粗糙的自我验证和自我修正，限制了在复杂任务中的有效性，因此需要一种新的框架来进行更细致的评估和改进。

Method: 通过将模型响应分解为可验证的（子问题，子答案）对，SSR 在控制重解和自我一致性检查中估计逐步置信度，从而进行精细化评估和精确化改进。

Result: 经过实验，SSR 在五个推理基准和三个 LLM 上表现超越了最新的自我修正基线，并实现了更准确和可解释的推理链。

Conclusion: Socratic Self-Refine (SSR) 在各项推理基准测试中表现优异，不仅提供性能提升，也为理解 LLM 内部推理过程提供了黑箱评估方法。

Abstract: Large Language Models (LLMs) have demonstrated remarkable reasoning abilities, yet existing test-time frameworks often rely on coarse self-verification and self-correction, limiting their effectiveness on complex tasks. In this paper, we propose Socratic Self-Refine (SSR), a novel framework for fine-grained evaluation and precise refinement of LLM reasoning. Our proposed SSR decomposes model responses into verifiable (sub-question, sub-answer) pairs, enabling step-level confidence estimation through controlled re-solving and self-consistency checks. By pinpointing unreliable steps and iteratively refining them, SSR produces more accurate and interpretable reasoning chains. Empirical results across five reasoning benchmarks and three LLMs show that SSR consistently outperforms state-of-the-art iterative self-refinement baselines. Beyond performance gains, SSR provides a principled black-box approach for evaluating and understanding the internal reasoning processes of LLMs. Code is available at https://github.com/SalesforceAIResearch/socratic-self-refine-reasoning.

</details>


### [7] [Instella: Fully Open Language Models with Stellar Performance](https://arxiv.org/abs/2511.10628)
*Jiang Liu,Jialian Wu,Xiaodong Yu,Yusheng Su,Prakamya Mishra,Gowtham Ramesh,Sudhanshu Ranjan,Chaitanya Manem,Ximeng Sun,Ze Wang,Pratik Prabhanjan Brahma,Zicheng Liu,Emad Barsoum*

Main category: cs.CL

TL;DR: Instella introduces fully open language models that outperform contemporaries while emphasizing transparency and reproducibility.


<details>
  <summary>Details</summary>
Motivation: To address the lack of transparency and reproducibility in high-performing language models by introducing fully open models trained on accessible resources.

Method: Instella models are developed through large-scale pre-training and general-purpose instruction tuning, using openly available data and codebase, and powered by AMD GPUs.

Result: Instella achieves state-of-the-art performance among fully open models and is competitive with leading models of similar sizes, including specialized variants for long context and mathematical reasoning tasks.

Conclusion: Instella provides a transparent and efficient alternative to existing language models, advancing open and reproducible research in the field.

Abstract: Large language models (LLMs) have demonstrated remarkable performance across a wide range of tasks, yet the majority of high-performing models remain closed-source or partially open, limiting transparency and reproducibility. In this work, we introduce Instella, a family of fully open three billion parameter language models trained entirely on openly available data and codebase. Powered by AMD Instinct MI300X GPUs, Instella is developed through large-scale pre-training, general-purpose instruction tuning, and alignment with human preferences. Despite using substantially fewer pre-training tokens than many contemporaries, Instella achieves state-of-the-art results among fully open models and is competitive with leading open-weight models of comparable size. We further release two specialized variants: Instella-Long, capable of handling context lengths up to 128K tokens, and Instella-Math, a reasoning-focused model enhanced through supervised fine-tuning and reinforcement learning on mathematical tasks. Together, these contributions establish Instella as a transparent, performant, and versatile alternative for the community, advancing the goal of open and reproducible language modeling research.

</details>


### [8] [Black-Box On-Policy Distillation of Large Language Models](https://arxiv.org/abs/2511.10643)
*Tianzhu Ye,Li Dong,Zewen Chi,Xun Wu,Shaohan Huang,Furu Wei*

Main category: cs.CL

TL;DR: 本研究提出了生成对抗蒸馏（GAD）方法，显著提升了黑箱大型语言模型的蒸馏效果。


<details>
  <summary>Details</summary>
Motivation: 旨在通过不直接访问教师模型的内部信息，仅通过文本输出学习，来提升大型语言模型的蒸馏性能.

Method: 引入生成对抗蒸馏（GAD），将学生LLM视为生成器，并训练一个判别器来区分学生和教师LLM的输出，从而形成一个极小极大博弈.

Result: 实验结果显示，GAD显著超越了传统的序列级知识蒸馏，尤其是在LMSYS-Chat自动评估中，GAD训练的Qwen2.5-14B-Instruct表现与教师模型GPT-5-Chat相当.

Conclusion: GAD作为一种新的黑箱LLM蒸馏方法，证明了比传统序列级知识蒸馏更具有效性.

Abstract: Black-box distillation creates student large language models (LLMs) by learning from a proprietary teacher model's text outputs alone, without access to its internal logits or parameters. In this work, we introduce Generative Adversarial Distillation (GAD), which enables on-policy and black-box distillation. GAD frames the student LLM as a generator and trains a discriminator to distinguish its responses from the teacher LLM's, creating a minimax game. The discriminator acts as an on-policy reward model that co-evolves with the student, providing stable, adaptive feedback. Experimental results show that GAD consistently surpasses the commonly used sequence-level knowledge distillation. In particular, Qwen2.5-14B-Instruct (student) trained with GAD becomes comparable to its teacher, GPT-5-Chat, on the LMSYS-Chat automatic evaluation. The results establish GAD as a promising and effective paradigm for black-box LLM distillation.

</details>


### [9] [ParoQuant: Pairwise Rotation Quantization for Efficient Reasoning LLM Inference](https://arxiv.org/abs/2511.10645)
*Yesheng Liang,Haisheng Chen,Song Han,Zhijian Liu*

Main category: cs.CL

TL;DR: 本研究提出了 ParoQuant，一种新型的权重后训练量化方法，旨在提高大型语言模型在推理任务中的表现，同时降低推理开销。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型(LML)在推理任务中的应用增加，现有的量化方法在处理权重和激活值中的异常值时表现不佳，导致准确性显著下降，因此需要一种新的解决方案。

Method: 通过结合独立的 Givens 旋转和通道级缩放，ParoQuant 能够均衡各通道之间的权重，并缩小量化组内的动态范围，从而降低量化误差。

Result: ParoQuant 在推理任务上较 AWQ 实现了平均 2.4% 的准确性提升，并且运行时开销低于 10%。

Conclusion: ParoQuant 提出了一个高效且准确的权重后训练量化方法，显著提高了推理任务的准确性，同时保持了较低的运行时开销。

Abstract: Weight-only post-training quantization (PTQ) compresses the weights of Large Language Models (LLMs) into low-precision representations to reduce memory footprint and accelerate inference. However, the presence of outliers in weights and activations often leads to large quantization errors and severe accuracy degradation, especially in recent reasoning LLMs where errors accumulate across long chains of thought. Existing PTQ methods either fail to sufficiently suppress outliers or introduce significant overhead during inference. In this paper, we propose Pairwise Rotation Quantization (ParoQuant), a weight-only PTQ method that combines hardware-efficient and optimizable independent Givens rotations with channel-wise scaling to even out the magnitude across channels and narrow the dynamic range within each quantization group. We further co-design the inference kernel to fully exploit GPU parallelism and keep the rotations and scaling lightweight at runtime. ParoQuant achieves an average 2.4% accuracy improvement over AWQ on reasoning tasks with less than 10% overhead. This paves the way for more efficient and accurate deployment of reasoning LLMs.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [10] [Algorithm Design and Stronger Guarantees for the Improving Multi-Armed Bandits Problem](https://arxiv.org/abs/2511.10619)
*Avrim Blum,Marten Garicano,Kavya Ravichandran,Dravyansh Sharma*

Main category: cs.LG

TL;DR: 本文提出两种新算法以解决改善多臂老虎机问题，实现了在不确定性条件下的更优数据依赖保证。


<details>
  <summary>Details</summary>
Motivation: 改善多臂老虎机问题是一个在不确定条件下分配努力的形式模型，动机来源于对新技术的研究投资、临床试验和学习曲线的超参数选择等场景。

Method: 定义了两个算法家族，第一个家族包含先前工作中的最优随机算法，第二个家族确保在良好实例上识别最佳臂，在较差实例上退回到最坏情况保证。

Result: 提出的算法在特定条件下实现了更强的保证，并且在不验证假设的情况下获得了更强的数据依赖保证。

Conclusion: 本文提出了两个新的参数化的老虎机算法族，并使用离线数据界定了学习近似最优算法的样本复杂性。

Abstract: The improving multi-armed bandits problem is a formal model for allocating effort under uncertainty, motivated by scenarios such as investing research effort into new technologies, performing clinical trials, and hyperparameter selection from learning curves. Each pull of an arm provides reward that increases monotonically with diminishing returns. A growing line of work has designed algorithms for improving bandits, albeit with somewhat pessimistic worst-case guarantees. Indeed, strong lower bounds of $Ω(k)$ and $Ω(\sqrt{k})$ multiplicative approximation factors are known for both deterministic and randomized algorithms (respectively) relative to the optimal arm, where $k$ is the number of bandit arms. In this work, we propose two new parameterized families of bandit algorithms and bound the sample complexity of learning the near-optimal algorithm from each family using offline data. The first family we define includes the optimal randomized algorithm from prior work. We show that an appropriately chosen algorithm from this family can achieve stronger guarantees, with optimal dependence on $k$, when the arm reward curves satisfy additional properties related to the strength of concavity. Our second family contains algorithms that both guarantee best-arm identification on well-behaved instances and revert to worst case guarantees on poorly-behaved instances. Taking a statistical learning perspective on the bandit rewards optimization problem, we achieve stronger data-dependent guarantees without the need for actually verifying whether the assumptions are satisfied.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [11] [Querying Labeled Time Series Data with Scenario Programs](https://arxiv.org/abs/2511.10627)
*Edward Kim,Devan Shanker,Varun Bharadwaj,Hongbeen Park,Jinkyu Kim,Hazem Torfah,Daniel J Fremont,Sanjit A Seshia*

Main category: cs.AI

TL;DR: 本论文提出了一种有效的查询算法，用于验证自动驾驶车辆在模拟中发现的失败场景在现实世界数据集中是否可重复。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶汽车的安全性保障需求增加，验证模拟中发现的失败场景在现实世界中是否可重现显得尤为重要。

Method: 提出了一种查询算法，通过场景程序和带标签的数据集来识别符合指定场景的数据子集。

Result: 通过有效的查询算法，将模拟失败场景与现实世界数据集进行匹配，并验证这些场景在实际数据中的存在。

Conclusion: 所提出的算法在查询场景时比现有的商业视觉大型语言模型更为准确且快得多，并且能够随着查询时间序列数据的持续时间进行扩展。

Abstract: Simulation-based testing has become a crucial complement to road testing for ensuring the safety of cyber physical systems (CPS). As a result, significant research efforts have been directed toward identifying failure scenarios within simulation environments. However, a critical question remains. Are the AV failure scenarios discovered in simulation reproducible on actual systems in the real world? The sim-to-real gap caused by differences between simulated and real sensor data means that failure scenarios identified in simulation might either be artifacts of synthetic sensor data or actual issues that also occur with real sensor data. To address this, an effective approach to validating simulated failure scenarios is to locate occurrences of these scenarios within real-world datasets and verify whether the failure persists on the datasets. To this end, we introduce a formal definition of how labeled time series sensor data can match an abstract scenario, represented as a scenario program using the Scenic probabilistic programming language. We present a querying algorithm that, given a scenario program and a labeled dataset, identifies the subset of data that matches the specified scenario. Our experiment shows that our algorithm is more accurate and orders of magnitude faster in querying scenarios than the state-of-the-art commercial vision large language models, and can scale with the duration of queried time series data.

</details>
