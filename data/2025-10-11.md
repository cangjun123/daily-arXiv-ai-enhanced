<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 86]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.HC](#cs.HC) [Total: 13]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Deep Learning Based Approach to Enhanced Recognition of Emotions and Behavioral Patterns of Autistic Children](https://arxiv.org/abs/2510.07320)
*Nelaka K. A. R,Peiris M. K. V,Liyanage R. P. B*

Main category: cs.LG

TL;DR: 该研究重点在于识别和映射自闭症儿童的行为模式，以改善其学习和软技能发展。通过纵向研究，提出了用于开发应用和技术辅助工具的框架，以促进更包容的学习环境。


<details>
  <summary>Details</summary>
Motivation: 尽管早期干预和定制化教育策略对改善结果至关重要，但在技能发展之前理解和解决自闭症儿童的细微行为模式和情感识别方面存在关键差距。

Method: 采用纵向方法监测情感和行为，以建立对自闭症学生面临的独特需求和挑战的基础性理解，特别是在信息技术领域。

Result: 通过对行为趋势的详细分析，提出了开发应用程序和技术辅助工具的针对性框架，证明了基于深刻理解每个儿童的行为和情感状况的循序渐进和循证干预方法的重要性。

Conclusion: 通过对行为趋势的详细分析，该研究提出了一个针对性框架，以开发应用程序和技术辅助工具来满足这些识别出的需求。通过将重点转向早期识别行为模式，该研究旨在促进更包容和支持的学习环境，从而显著改善患有自闭症谱系障碍儿童的教育和发展轨迹。

Abstract: Autism Spectrum Disorder significantly influences the communication
abilities, learning processes, behavior, and social interactions of
individuals. Although early intervention and customized educational strategies
are critical to improving outcomes, there is a pivotal gap in understanding and
addressing nuanced behavioral patterns and emotional identification in autistic
children prior to skill development. This extended research delves into the
foundational step of recognizing and mapping these patterns as a prerequisite
to improving learning and soft skills. Using a longitudinal approach to monitor
emotions and behaviors, this study aims to establish a baseline understanding
of the unique needs and challenges faced by autistic students, particularly in
the Information Technology domain, where opportunities are markedly limited.
Through a detailed analysis of behavioral trends over time, we propose a
targeted framework for developing applications and technical aids designed to
meet these identified needs. Our research underscores the importance of a
sequential and evidence-based intervention approach that prioritizes a deep
understanding of each child's behavioral and emotional landscape as the basis
for effective skill development. By shifting the focus toward early
identification of behavioral patterns, we aim to foster a more inclusive and
supportive learning environment that can significantly improve the educational
and developmental trajectory of children with ASD.

</details>


### [2] [A Modality-Aware Cooperative Co-Evolutionary Framework for Multimodal Graph Neural Architecture Search](https://arxiv.org/abs/2510.07325)
*Sixuan Wang,Jiao Yin,Jinli Cao,Mingjian Tang,Yong-Feng Ge*

Main category: cs.LG

TL;DR: 提出了一种新的多模态图神经架构搜索算法MACC-MGNAS，提高准确度，减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 因为现有方法局限于单一模态，并忽略了模态异质性，所以提出MACC-MGNAS来解决这一限制，并提高攻击预测的准确性。

Method: 提出了一种面向模态的合作协同演化算法，用于多模态图神经架构搜索（MACC-MGNAS）。该方法包括基于分而治之的MACC框架，模态感知双轨替代（MADTS）方法，以及基于相似性的人口多样性指示器（SPDI）策略。

Result: 在标准漏洞共利用（VulCE）数据集上，MACC-MGNAS仅在3个GPU小时内实现了F1-score 81.67%，比最先进的竞争对手高8.7%的F1，并减少了27%的计算成本。

Conclusion: MACC-MGNAS有效地捕获了单一模态GNAS所忽略的模态异质性，同时在提高预测准确性和降低计算成本方面有显著优势。

Abstract: Co-exploitation attacks on software vulnerabilities pose severe risks to
enterprises, a threat that can be mitigated by analyzing heterogeneous and
multimodal vulnerability data. Multimodal graph neural networks (MGNNs) are
well-suited to integrate complementary signals across modalities, thereby
improving attack-prediction accuracy. However, designing an effective MGNN
architecture is challenging because it requires coordinating modality-specific
components at each layer, which is infeasible through manual tuning. Genetic
algorithm (GA)-based graph neural architecture search (GNAS) provides a natural
solution, yet existing methods are confined to single modalities and overlook
modality heterogeneity. To address this limitation, we propose a modality-aware
cooperative co-evolutionary algorithm for multimodal graph neural architecture
search, termed MACC-MGNAS. First, we develop a modality-aware cooperative
co-evolution (MACC) framework under a divide-and-conquer paradigm: a
coordinator partitions a global chromosome population into modality-specific
gene groups, local workers evolve them independently, and the coordinator
reassembles chromosomes for joint evaluation. This framework effectively
captures modality heterogeneity ignored by single-modality GNAS. Second, we
introduce a modality-aware dual-track surrogate (MADTS) method to reduce
evaluation cost and accelerate local gene evolution. Third, we design a
similarity-based population diversity indicator (SPDI) strategy to adaptively
balance exploration and exploitation, thereby accelerating convergence and
avoiding local optima. On a standard vulnerabilities co-exploitation (VulCE)
dataset, MACC-MGNAS achieves an F1-score of 81.67% within only 3 GPU-hours,
outperforming the state-of-the-art competitor by 8.7% F1 while reducing
computation cost by 27%.

</details>


### [3] [MultiFair: Multimodal Balanced Fairness-Aware Medical Classification with Dual-Level Gradient Modulation](https://arxiv.org/abs/2510.07328)
*Md Zubair,Hao Zheng,Nussdorf Jonathan,Grayson W. Armstrong,Lucy Q. Shen,Gabriela Wilson,Yu Tian,Xingquan Zhu,Min Shi*

Main category: cs.LG

TL;DR: MultiFair is a new approach for fair and unbiased multimodal medical classification, using dual-level gradient modulation to improve model performance across data modalities and demographic groups.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal learning models often result in biased and unfair learning due to uneven learning across data modalities and emphasis on certain demographic groups. Therefore, a new approach is necessary to ensure balanced and fair multimodal medical decision-making.

Method: The proposed MultiFair approach uses a dual-level gradient modulation process to dynamically adjust training gradients based on optimization direction and magnitude at both data modality and demographic group levels.

Result: Extensive experiments on two multimodal medical datasets demonstrated that MultiFair effectively addresses imbalances and fairness issues, achieving superior performance compared to current methods.

Conclusion: MultiFair outperforms state-of-the-art models in addressing bias and fairness in multimodal medical classification.

Abstract: Medical decision systems increasingly rely on data from multiple sources to
ensure reliable and unbiased diagnosis. However, existing multimodal learning
models fail to achieve this goal because they often ignore two critical
challenges. First, various data modalities may learn unevenly, thereby
converging to a model biased towards certain modalities. Second, the model may
emphasize learning on certain demographic groups causing unfair performances.
The two aspects can influence each other, as different data modalities may
favor respective groups during optimization, leading to both imbalanced and
unfair multimodal learning. This paper proposes a novel approach called
MultiFair for multimodal medical classification, which addresses these
challenges with a dual-level gradient modulation process. MultiFair dynamically
modulates training gradients regarding the optimization direction and magnitude
at both data modality and group levels. We conduct extensive experiments on two
multimodal medical datasets with different demographic groups. The results show
that MultiFair outperforms state-of-the-art multimodal learning and fairness
learning methods.

</details>


### [4] [Out-of-Distribution Generalization in Climate-Aware Yield Prediction with Earth Observation Data](https://arxiv.org/abs/2510.07350)
*Aditya Chakravarty*

Main category: cs.LG

TL;DR: 研究评估了GNN-RNN和MMST-ViT模型在跨区域和年度预测中的性能，GNN-RNN表现更佳且训练速度更快。


<details>
  <summary>Details</summary>
Motivation: 准确的作物产量预测对于粮食安全至关重要，深度学习模型的泛化能力需在现实条件下进行测试。

Method: 使用CropNet数据集进行离群集验证，评估地理转移和时间预测场景下模型性能。

Result: GNN-RNN模型能够在地理转移条件下实现良好的正相关，且训练速度快于MMST-ViT。

Conclusion: GNN-RNN在地理转移条件下表现出色，而MMST-ViT在域内表现良好但在OOD条件下表现急剧下降。跨区域可转移性存在显著差异，空间-时间对齐是稳健泛化的关键。

Abstract: Climate change is increasingly disrupting agricultural systems, making
accurate crop yield forecasting essential for food security. While deep
learning models have shown promise in yield prediction using satellite and
weather data, their ability to generalize across geographic regions and years -
critical for real-world deployment - remains largely untested. We benchmark two
state-of-the-art models, GNN-RNN and MMST-ViT, under realistic
out-of-distribution (OOD) conditions using the large-scale CropNet dataset
spanning 1,200+ U.S. counties from 2017-2022. Through leave-one-cluster-out
cross-validation across seven USDA Farm Resource Regions and year-ahead
prediction scenarios, we identify substantial variability in cross-region
transferability. GNN-RNN demonstrates superior generalization with positive
correlations under geographic shifts, while MMST-ViT performs well in-domain
but degrades sharply under OOD conditions. Regions like Heartland and Northern
Great Plains show stable transfer dynamics (RMSE less than 10 bu/acre for
soybean), whereas Prairie Gateway exhibits persistent underperformance (RMSE
greater than 20 bu/acre) across both models and crops, revealing structural
dissimilarities likely driven by semi-arid climate, irrigation patterns, and
incomplete spectral coverage. Beyond accuracy differences, GNN-RNN achieves
135x faster training than MMST-ViT (14 minutes vs. 31.5 hours), making it more
viable for sustainable deployment. Our findings underscore that
spatial-temporal alignment - not merely model complexity or data scale - is key
to robust generalization, and highlight the need for transparent OOD evaluation
protocols to ensure equitable and reliable climate-aware agricultural
forecasting.

</details>


### [5] [ConCuR: Conciseness Makes State-of-the-Art Kernel Generation](https://arxiv.org/abs/2510.07356)
*Lingcheng Kong,Jiateng Wei,Hanzhang Shen,Huan Wang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: GPU kernel generation by LLMs has recently experienced rapid development,
leveraging test-time scaling and reinforcement learning techniques. However, a
key challenge for kernel generation is the scarcity of high-quality data, as
most high-quality kernels are proprietary and not open-source. This challenge
prevents us from leveraging supervised fine-tuning to align LLMs to the kernel
generation task. To address this challenge, we develop a pipeline that
generates and curates high-quality CUDA kernels with reasoning traces,
motivated by a critical observation that concise yet informative reasoning
traces result in robust generation of high-performance kernels. Using this
pipeline, we construct our dataset ConCuR and introduce our model KernelCoder,
which is the first model trained on a curated dataset consisting of PyTorch,
reasoning, and CUDA kernel pairs, to our knowledge. In the KernelBench setup,
our model achieves significant improvements over the existing top-performing
model, QwQ-32B, and outperforms all open-source models fine-tuned for kernel
generation, as well as frontier models such as DeepSeek-V3.1-Think and
Claude-4-sonnet. Finally, we show that the average reasoning length can serve
as a metric to assess the difficulty of kernel generation tasks. The
observations, metrics, and our data collection and curation pipeline can help
obtain better data in the kernel generation task in the future.

</details>


### [6] [Best-of-Both Worlds for linear contextual bandits with paid observations](https://arxiv.org/abs/2510.07424)
*Nathan Boyer,Dorian Baudry,Patrick Rebeschini*

Main category: cs.LG

TL;DR: 提出一种计算效率高的BOBW算法用于解决线性上下文bandits问题，优化遗憾表现。


<details>
  <summary>Details</summary>
Motivation: 研究在线性上下文bandits问题中在付费观察情况下的行动选择，旨在最小化给定环境中的损失。

Method: 论文采用了矩阵几何重抽样的有效估计器，通过Follow-the-Regularized-Leader框架进行研究，设计了一种计算效率高的BOBW算法。

Result: 该算法在对抗性环境中实现了最小最大化的遗憾为$\Theta(T^{2/3})$，在（损坏的）随机环境中保证了多对数的遗憾。

Conclusion: 该论文提出了一种计算效率高的“两全其美”(BOBW)算法，用于解决线性上下文bandits问题，该算法在对抗性环境中实现了最小最大化的遗憾，并在（损坏的）随机环境中保证了多对数的遗憾。

Abstract: We study the problem of linear contextual bandits with paid observations,
where at each round the learner selects an action in order to minimize its loss
in a given context, and can then decide to pay a fixed cost to observe the loss
of any arm. Building on the Follow-the-Regularized-Leader framework with
efficient estimators via Matrix Geometric Resampling, we introduce a
computationally efficient Best-of-Both-Worlds (BOBW) algorithm for this
problem. We show that it achieves the minimax-optimal regret of
$\Theta(T^{2/3})$ in adversarial settings, while guaranteeing poly-logarithmic
regret in (corrupted) stochastic regimes. Our approach builds on the framework
from \cite{BOBWhardproblems} to design BOBW algorithms for ``hard problem'',
using analysis techniques tailored for the setting that we consider.

</details>


### [7] [Learning to Route LLMs from Bandit Feedback: One Policy, Many Trade-offs](https://arxiv.org/abs/2510.07429)
*Wang Wei,Tiankai Yang,Hongjie Chen,Yue Zhao,Franck Dernoncourt,Ryan A. Rossi,Hoda Eldardiry*

Main category: cs.LG

TL;DR: 提出了BaRP方法，通过模拟在线反馈环境提高大规模语言模型的路由决策性能，在不同任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型（LLMs）的高效使用对于大规模部署至关重要。系统需要在强模型的高支付和弱模型的低性能之间找到平衡，选择合适的LLM实际上是一个在线决策问题。模型各有优势，价格波动，用户对准确性和成本的重视程度不同。

Method: 提出了一种称为BaRP（Bandit-feedback Routing with Preferences）的方法，利用上下文bandit算法对提示特征和用户偏好进行建模。我们的方法在训练时模拟在线反馈环境，通过支持偏好可调推理，允许操作员在测试时调整性能和成本的权衡，而无需重新训练。

Result: 通过全面实验，该方法在性能上至少比强离线路由器高出12.46%，比最大的大规模语言模型高出至少2.45%，并且在面对未见过的任务时表现强劲。

Conclusion: BaRP方法解决了离线训练与实际部署之间的反馈鸿沟，并能够在不依赖全信息离线监督的情况下，为每个新提示自适应地做出路由决策。

Abstract: Efficient use of large language models (LLMs) is critical for deployment at
scale: without adaptive routing, systems either overpay for strong models or
risk poor performance from weaker ones. Selecting the right LLM for each query
is fundamentally an online decision problem: models differ in strengths, prices
fluctuate, and users value accuracy and cost differently. Yet most routers are
trained offline with labels for all candidate models, an assumption that breaks
in deployment, where only the outcome of the chosen model is observed. We
bridge this gap with BaRP, a Bandit-feedback Routing with Preferences approach
that trains under the same partial-feedback restriction as deployment, while
supporting preference-tunable inference: operators can dial the
performance/cost trade-off at test time without retraining. Framed as a
contextual bandit over prompt features and a user preference vector, our method
simulates an online feedback setting during training and adapts its routing
decisions to each new prompt, rather than depending on full-information offline
supervision. Comprehensive experiments show that our method consistently
outperforms strong offline routers by at least 12.46% and the largest LLM by at
least 2.45%, and generalizes robustly for unseen tasks.

</details>


### [8] [Parameter-Free Federated TD Learning with Markov Noise in Heterogeneous Environments](https://arxiv.org/abs/2510.07436)
*Ankur Naskar,Gugan Thoppe,Utsav Negi,Vijay Gupta*

Main category: cs.LG

TL;DR: 提出了一种无参数的联邦时间差分学习方法，在异质环境中实现最佳的收敛速率。


<details>
  <summary>Details</summary>
Motivation: 现有的TD学习结果需要算法依赖未知的参数，我们提出一个无参数的FTD方法来填补这一空白。

Method: 提出联邦时间差分（FTD）学习方法，采用Polyak-Ruppert均值化，能够在异质环境中实现最佳收敛速率。

Result: 我们的FTD方法在平均回报和折扣设置中以无参数的方式达到了理论上的最佳收敛速率。

Conclusion: 我们提出了一个两时间尺度的联邦时间差分学习方法，采用Polyak-Ruppert均值化，可以在平均回报和折扣设置中达到最佳的收敛速率，无需依赖未知参数。

Abstract: Federated learning (FL) can dramatically speed up reinforcement learning by
distributing exploration and training across multiple agents. It can guarantee
an optimal convergence rate that scales linearly in the number of agents, i.e.,
a rate of $\tilde{O}(1/(NT)),$ where $T$ is the iteration index and $N$ is the
number of agents. However, when the training samples arise from a Markov chain,
existing results on TD learning achieving this rate require the algorithm to
depend on unknown problem parameters. We close this gap by proposing a
two-timescale Federated Temporal Difference (FTD) learning with Polyak-Ruppert
averaging. Our method provably attains the optimal $\tilde{O}(1/NT)$ rate in
both average-reward and discounted settings--offering a parameter-free FTD
approach for Markovian data. Although our results are novel even in the
single-agent setting, they apply to the more realistic and challenging scenario
of FL with heterogeneous environments.

</details>


### [9] [MoGU: Mixture-of-Gaussians with Uncertainty-based Gating for Time Series Forecasting](https://arxiv.org/abs/2510.07459)
*Yoli Shavit,Jacob Goldberger*

Main category: cs.LG

TL;DR: MoGU, a novel MoE framework using Gaussian distributions and uncertainty-based gating, demonstrated superior performance in time series forecasting tasks.


<details>
  <summary>Details</summary>
Motivation: There is a need for regression models that not only provide accurate point estimates but also quantify uncertainty in forecasts, especially in time series forecasting tasks.

Method: The method involves using a novel MoE framework called MoGU, which models each expert's output as a Gaussian distribution to quantify forecast uncertainty and employs an uncertainty-based gating mechanism.

Result: MoGU showed improved performance compared to single-expert models and traditional MoE setups across various time series forecasting benchmarks.

Conclusion: MoGU consistently outperforms single-expert models and traditional MoE setups, providing well-quantified uncertainties that correlate with prediction errors, thus enhancing forecast reliability.

Abstract: We introduce Mixture-of-Gaussians with Uncertainty-based Gating (MoGU), a
novel Mixture-of-Experts (MoE) framework designed for regression tasks and
applied to time series forecasting. Unlike conventional MoEs that provide only
point estimates, MoGU models each expert's output as a Gaussian distribution.
This allows it to directly quantify both the forecast (the mean) and its
inherent uncertainty (variance). MoGU's core innovation is its
uncertainty-based gating mechanism, which replaces the traditional input-based
gating network by using each expert's estimated variance to determine its
contribution to the final prediction. Evaluated across diverse time series
forecasting benchmarks, MoGU consistently outperforms single-expert models and
traditional MoE setups. It also provides well-quantified, informative
uncertainties that directly correlate with prediction errors, enhancing
forecast reliability. Our code is available from:
https://github.com/yolish/moe_unc_tsf

</details>


### [10] [HEMERA: A Human-Explainable Transformer Model for Estimating Lung Cancer Risk using GWAS Data](https://arxiv.org/abs/2510.07477)
*Maria Mahbub,Robert J. Klein,Myvizhi Esai Selvan,Rowena Yip,Claudia Henschke,Providencia Morales,Ian Goethert,Olivera Kotevska,Mayanka Chandra Shekar,Sean R. Wilkinson,Eileen McAllister,Samuel M. Aguayo,Zeynep H. Gümüş,Ioana Danciu,VA Million Veteran Program*

Main category: cs.LG

TL;DR: HEMERA模型通过利用GWAS数据和解释性AI技术，有效提升了肺癌风险预测的准确性，实现了超过99%的AUC评分。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明遗传因素在肺癌风险中有重要作用，该研究试图通过新方法提高对肺癌风险的预测准确性。

Method: 应用解释性Transformer深度学习框架处理GWAS数据，以单核苷酸多态性（SNPs）为基础进行肺癌风险预测。

Result: HEMERA在27,254名参与者的数据上取得了超过99%的AUC评分。

Conclusion: HEMERA模型通过解释性Transformer方法处理GWAS数据，实现了对肺癌风险的个性化评估，支持早期干预。

Abstract: Lung cancer (LC) is the third most common cancer and the leading cause of
cancer deaths in the US. Although smoking is the primary risk factor, the
occurrence of LC in never-smokers and familial aggregation studies highlight a
genetic component. Genetic biomarkers identified through genome-wide
association studies (GWAS) are promising tools for assessing LC risk. We
introduce HEMERA (Human-Explainable Transformer Model for Estimating Lung
Cancer Risk using GWAS Data), a new framework that applies explainable
transformer-based deep learning to GWAS data of single nucleotide polymorphisms
(SNPs) for predicting LC risk. Unlike prior approaches, HEMERA directly
processes raw genotype data without clinical covariates, introducing additive
positional encodings, neural genotype embeddings, and refined variant
filtering. A post hoc explainability module based on Layer-wise Integrated
Gradients enables attribution of model predictions to specific SNPs, aligning
strongly with known LC risk loci. Trained on data from 27,254 Million Veteran
Program participants, HEMERA achieved >99% AUC (area under receiver
characteristics) score. These findings support transparent,
hypothesis-generating models for personalized LC risk assessment and early
intervention.

</details>


### [11] [PEAR: Planner-Executor Agent Robustness Benchmark](https://arxiv.org/abs/2510.07505)
*Shen Dong,Mingxuan Zhang,Pengfei He,Li Ma,Bhavani Thuraisingham,Hui Liu,Yue Xing*

Main category: cs.LG

TL;DR: MAS在处理复杂任务方面性能强大但易受攻击，PEAR基准工具帮助分析其效用和脆弱性，揭示计划器攻击风险及防御方向。


<details>
  <summary>Details</summary>
Motivation: 虽然MAS在处理复杂多步任务方面表现突出，但仍易受到对抗性操控。现有研究多关注孤立的攻击面或特定场景，缺乏对MAS脆弱性的整体认识。为弥补这一空白，研究发布了PEAR基准。

Method: 通过大量实验，研究了计划器和执行器的弱化对任务执行的影响，以及内存模块对于计划器和执行器的重要性。

Result: 实验证明，弱化计划器比弱化执行器更严重地影响任务性能，计划器需要内存模块而执行器不需要，任务性能与鲁棒性之间存在权衡，针对计划器的攻击效果最佳。

Conclusion: 该研究引入了PEAR基准工具，系统评估MAS的效用和脆弱性，发现针对计划器的攻击对系统误导最为有效。这些发现为提升MAS的鲁棒性提供了可行的见解。

Abstract: Large Language Model (LLM)-based Multi-Agent Systems (MAS) have emerged as a
powerful paradigm for tackling complex, multi-step tasks across diverse
domains. However, despite their impressive capabilities, MAS remain susceptible
to adversarial manipulation. Existing studies typically examine isolated attack
surfaces or specific scenarios, leaving a lack of holistic understanding of MAS
vulnerabilities. To bridge this gap, we introduce PEAR, a benchmark for
systematically evaluating both the utility and vulnerability of
planner-executor MAS. While compatible with various MAS architectures, our
benchmark focuses on the planner-executor structure, which is a practical and
widely adopted design. Through extensive experiments, we find that (1) a weak
planner degrades overall clean task performance more severely than a weak
executor; (2) while a memory module is essential for the planner, having a
memory module for the executor does not impact the clean task performance; (3)
there exists a trade-off between task performance and robustness; and (4)
attacks targeting the planner are particularly effective at misleading the
system. These findings offer actionable insights for enhancing the robustness
of MAS and lay the groundwork for principled defenses in multi-agent settings.

</details>


### [12] [Efficient Generalization via Multimodal Co-Training under Data Scarcity and Distribution Shift](https://arxiv.org/abs/2510.07509)
*Tianyu Bell Pan,Damon L. Woodard*

Main category: cs.LG

TL;DR: 提出一种多模态协同训练框架，利用未标记数据及促进模态间一致性来增强模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机是应对数据标记有限且分布变化的挑战，通过多模态协同训练框架提高模型的泛化能力，促进模态间分类器的协同。

Method: 本文进行了理论分析，推导出在利用未标记数据及促进模态间分类器协同时显著改进泛化能力的条件。我们还进行了收敛性分析，验证了迭代协同训练在减少分类错误方面的有效性。此外，建立了一种新的泛化界，将利用未标记的多模态数据、促进模态间一致性以及保持条件独立性的优势进行分解和量化。

Result: 研究结果确认了迭代协同训练在减少分类错误方面的有效性，并首次在多模态协同训练环境中建立了新的泛化界能够量化不同优势。

Conclusion: 本研究提出了一种多模态协同训练框架，能够在标记数据有限且分布发生变化的情况下增强模型泛化能力。这一框架通过利用未标记数据以及促进不同模态分类器间的协同来显著提高模型的泛化能力。研究结果显示，该方法在现实环境中具有实用价值，可作为开发数据高效且稳健的AI系统的结构化方法。

Abstract: This paper explores a multimodal co-training framework designed to enhance
model generalization in situations where labeled data is limited and
distribution shifts occur. We thoroughly examine the theoretical foundations of
this framework, deriving conditions under which the use of unlabeled data and
the promotion of agreement between classifiers for different modalities lead to
significant improvements in generalization. We also present a convergence
analysis that confirms the effectiveness of iterative co-training in reducing
classification errors. In addition, we establish a novel generalization bound
that, for the first time in a multimodal co-training context, decomposes and
quantifies the distinct advantages gained from leveraging unlabeled multimodal
data, promoting inter-view agreement, and maintaining conditional view
independence. Our findings highlight the practical benefits of multimodal
co-training as a structured approach to developing data-efficient and robust AI
systems that can effectively generalize in dynamic, real-world environments.
The theoretical foundations are examined in dialogue with, and in advance of,
established co-training principles.

</details>


### [13] [Investigating Thematic Patterns and User Preferences in LLM Interactions using BERTopic](https://arxiv.org/abs/2510.07557)
*Abhay Bhandarkar,Gaurav Mishra,Khushi Juchani,Harsh Singhal*

Main category: cs.LG

TL;DR: 该研究应用BERTopic技术分析lmsys-chat-1m数据集揭示了用户对LLM在特定主题中偏好的趋势，为领域特定的微调和优化提供了策略。


<details>
  <summary>Details</summary>
Motivation: 该研究的主要目的是揭示会话中的主题模式并分析它们与用户偏好的关联，特别是某些LLM是否在特定主题内被一致偏好。

Method: 研究采用BERTopic技术进行主题建模，通过一套用于处理多语言变体、平衡对话轮次、清理噪音和编辑数据的预处理管道，在lmsys-chat-1m数据集上进行分析。

Result: BERTopic提取了包括人工智能、编程、伦理和云基础设施在内的29个一致主题，并通过分析主题与模型偏好之间的关系以识别模型主题一致性趋势。可视化技术包括主题间距离地图、主题概率分布和模型对比主题矩阵。

Conclusion: 该研究经过对多语言会话语料库的主题分析后，揭示了LLMs在特定主题中的用户偏好趋势，提供了领域特定的微调和优化策略，以提升LLM的真实世界性能和用户满意度。

Abstract: This study applies BERTopic, a transformer-based topic modeling technique, to
the lmsys-chat-1m dataset, a multilingual conversational corpus built from
head-to-head evaluations of large language models (LLMs). Each user prompt is
paired with two anonymized LLM responses and a human preference label, used to
assess user evaluation of competing model outputs. The main objective is
uncovering thematic patterns in these conversations and examining their
relation to user preferences, particularly if certain LLMs are consistently
preferred within specific topics. A robust preprocessing pipeline was designed
for multilingual variation, balancing dialogue turns, and cleaning noisy or
redacted data. BERTopic extracted over 29 coherent topics including artificial
intelligence, programming, ethics, and cloud infrastructure. We analysed
relationships between topics and model preferences to identify trends in
model-topic alignment. Visualization techniques included inter-topic distance
maps, topic probability distributions, and model-versus-topic matrices. Our
findings inform domain-specific fine-tuning and optimization strategies for
improving real-world LLM performance and user satisfaction.

</details>


### [14] [MLLM4TS: Leveraging Vision and Multimodal Language Models for General Time-Series Analysis](https://arxiv.org/abs/2510.07513)
*Qinghua Liu,Sam Heshmati,Zheda Mai,Zubin Abraham,John Paparrizos,Liu Ren*

Main category: cs.LG

TL;DR: MLLM4TS通过将视觉表示整合到多模态大语言模型中，有效地提升了时间序列分析性能，尤其是在预测和生成任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型展现了令人印象深刻的泛化和视觉理解能力，但应用于时间序列仍受限于连续数值数据与离散自然语言之间的模态差距。为了弥合这一差距，提出了将视觉表示融合到自动时间序列分析中的设想，并引入了新框架MLLM4TS。

Method: 提出了名为MLLM4TS的新框架，通过专门的视觉分支集成多模态大语言模型进行时间序列分析。每个时间序列通道在一个复合图像中呈现为水平堆叠的彩色线图，以捕获跨通道的空间依赖关系，然后用时间感知视觉补丁对齐策略将视觉补丁与其对应的时间段对齐。MLLM4TS融合了来自数值数据的精细时间细节和从视觉表示中派生的全局上下文信息。

Result: 在标准基准上的大量实验证明了MLLM4TS在预测任务（如分类）和生成任务（如异常检测和预测）中的有效性。

Conclusion: MLLM4TS能够在预测任务和生成任务中表现出色，证明了将视觉模式与预训练语言模型结合以实现稳健且可推广的时间序列分析的潜力。

Abstract: Effective analysis of time series data presents significant challenges due to
the complex temporal dependencies and cross-channel interactions in
multivariate data. Inspired by the way human analysts visually inspect time
series to uncover hidden patterns, we ask: can incorporating visual
representations enhance automated time-series analysis? Recent advances in
multimodal large language models have demonstrated impressive generalization
and visual understanding capability, yet their application to time series
remains constrained by the modality gap between continuous numerical data and
discrete natural language. To bridge this gap, we introduce MLLM4TS, a novel
framework that leverages multimodal large language models for general
time-series analysis by integrating a dedicated vision branch. Each time-series
channel is rendered as a horizontally stacked color-coded line plot in one
composite image to capture spatial dependencies across channels, and a
temporal-aware visual patch alignment strategy then aligns visual patches with
their corresponding time segments. MLLM4TS fuses fine-grained temporal details
from the numerical data with global contextual information derived from the
visual representation, providing a unified foundation for multimodal
time-series analysis. Extensive experiments on standard benchmarks demonstrate
the effectiveness of MLLM4TS across both predictive tasks (e.g.,
classification) and generative tasks (e.g., anomaly detection and forecasting).
These results underscore the potential of integrating visual modalities with
pretrained language models to achieve robust and generalizable time-series
analysis.

</details>


### [15] [EEG Sleep Stage Classification with Continuous Wavelet Transform and Deep Learning](https://arxiv.org/abs/2510.07524)
*Mehdi Zekriyapanah Gashti,Ghasem Farjamnia*

Main category: cs.LG

TL;DR: 提出了一种基于小波变换的自动睡眠阶段评分框架，通过时频分析结合集成学习实现高效分类，实验结果优秀。


<details>
  <summary>Details</summary>
Motivation: 准确的睡眠阶段分类对于睡眠障碍的诊断和管理至关重要。

Method: 使用连续小波变换（CWT）生成时频图，并结合集成学习进行睡眠阶段分类。

Result: 整体准确率为88.37%，宏平均F1得分为73.15，优于传统机器学习方法，并与最新的深度学习方法性能相当或更优。

Conclusion: 提出了一种新的基于小波变换的自动睡眠阶段评分框架，实验结果表明其性能优于传统和最新的深度学习方法。

Abstract: Accurate classification of sleep stages is crucial for the diagnosis and
management of sleep disorders. Conventional approaches for sleep scoring rely
on manual annotation or features extracted from EEG signals in the time or
frequency domain. This study proposes a novel framework for automated sleep
stage scoring using time-frequency analysis based on the wavelet transform. The
Sleep-EDF Expanded Database (sleep-cassette recordings) was used for
evaluation. The continuous wavelet transform (CWT) generated time-frequency
maps that capture both transient and oscillatory patterns across frequency
bands relevant to sleep staging. Experimental results demonstrate that the
proposed wavelet-based representation, combined with ensemble learning,
achieves an overall accuracy of 88.37 percent and a macro-averaged F1 score of
73.15, outperforming conventional machine learning methods and exhibiting
comparable or superior performance to recent deep learning approaches. These
findings highlight the potential of wavelet analysis for robust, interpretable,
and clinically applicable sleep stage classification.

</details>


### [16] [Estimating Fair Graphs from Graph-Stationary Data](https://arxiv.org/abs/2510.07536)
*Madeline Navarro,Andrei Buciulea,Samuel Rey,Antonio G. Marques,Santiago Segarra*

Main category: cs.LG

TL;DR: 这篇论文提出了一种估计公平图的方法，通过优化方法Fair Spectral Templates，在实现公平性的同时不牺牲准确性。


<details>
  <summary>Details</summary>
Motivation: 为了估计公平图，使得连接在敏感属性上没有偏差。现实世界中的图往往表现出连接某些群体对偏好的偏见，这不仅可能加剧而且可能引发基于图的下游任务的不公平待遇。

Method: 我们提出了一种基于优化的Fair Spectral Templates (FairSpecTemp) 方法，用于从静止图信号中估计公平图。这种方法有两个变体，一个变体利用图静止性的交换性质并直接限制偏差；另一个变体通过限制图谱中的偏差隐式鼓励公平估计，因此更加灵活。

Result: FairSpecTemp方法在合成和真实世界数据集上进行了评估，以展示其有效性并突出两个变体的优势。结果表明在恢复公平图时不需要牺牲准确性。

Conclusion: 我们的方法能够以高概率实现性能边界，并在公平性和准确性之间提供条件折衷。特别是，分析表明，恢复公平图不需要牺牲准确性。通过在合成和真实世界数据集上评估FairSpecTemp以展示其有效性，并突出FairSpecTemp两个变体的优势。

Abstract: We estimate fair graphs from graph-stationary nodal observations such that
connections are not biased with respect to sensitive attributes. Edges in
real-world graphs often exhibit preferences for connecting certain pairs of
groups. Biased connections can not only exacerbate but even induce unfair
treatment for downstream graph-based tasks. We therefore consider group and
individual fairness for graphs corresponding to group- and node-level
definitions, respectively. To evaluate the fairness of a given graph, we
provide multiple bias metrics, including novel measurements in the spectral
domain. Furthermore, we propose Fair Spectral Templates (FairSpecTemp), an
optimization-based method with two variants for estimating fair graphs from
stationary graph signals, a general model for graph data subsuming many
existing ones. One variant of FairSpecTemp exploits commutativity properties of
graph stationarity while directly constraining bias, while the other implicitly
encourages fair estimates by restricting bias in the graph spectrum and is thus
more flexible. Our methods enjoy high probability performance bounds, yielding
a conditional tradeoff between fairness and accuracy. In particular, our
analysis reveals that accuracy need not be sacrificed to recover fair graphs.
We evaluate FairSpecTemp on synthetic and real-world data sets to illustrate
its effectiveness and highlight the advantages of both variants of
FairSpecTemp.

</details>


### [17] [Targeted Digital Twin via Flow Map Learning and Its Application to Fluid Dynamics](https://arxiv.org/abs/2510.07549)
*Qifan Chen,Zhongshu Xu,Jinjin Zhang,Dongbin Xiu*

Main category: cs.LG

TL;DR: 一个基于记忆的流映射学习方法用于构建目标数字孪生，实现高效的长期动态预测，并在二维流体动力学中展示了其准确性和计算节省。


<details>
  <summary>Details</summary>
Motivation: 通过构建目标数字孪生（tDT）直接对完整数字孪生（DT）中的关键量（QoIs）进行动态建模，以实现高效预测和模拟。

Method: 使用基于记忆的流映射学习（FML）开发数据驱动的QoIs模型，并通过多次执行完整DT来生成轨迹数据短突发，以完全离线的计算过程构建基于FML的tDT。

Result: 在计算流体动力学（CFD）案例中展示了tDT的构建和预测能力，结果显示tDT能够准确预测长期作用力，同时完全避免了完整流场模拟。

Conclusion: 基于FML的tDT能在不进行完整数字孪生系统模拟的情况下，有效预测和分析QoIs的长期动态，从而节省大量计算资源。

Abstract: We present a numerical framework for constructing a targeted digital twin
(tDT) that directly models the dynamics of quantities of interest (QoIs) in a
full digital twin (DT). The proposed approach employs memory-based flow map
learning (FML) to develop a data-driven model of the QoIs using short bursts of
trajectory data generated through repeated executions of the full DT. This
renders the construction of the FML-based tDT an entirely offline computational
process. During online simulation, the learned tDT can efficiently predict and
analyze the long-term dynamics of the QoIs without requiring simulations of the
full DT system, thereby achieving substantial computational savings. After
introducing the general numerical procedure, we demonstrate the construction
and predictive capability of the tDT in a computational fluid dynamics (CFD)
example: two-dimensional incompressible flow past a cylinder. The QoIs in this
problem are the hydrodynamic forces exerted on the cylinder. The resulting tDTs
are compact dynamical systems that evolve these forces without explicit
knowledge of the underlying flow field. Numerical results show that the tDTs
yield accurate long-term predictions of the forces while entirely bypassing
full flow simulations.

</details>


### [18] [Beyond Sub-6 GHz: Leveraging mmWave Wi-Fi for Gait-Based Person Identification](https://arxiv.org/abs/2510.08160)
*Nabeel Nisar Bhat,Maksim Karnaukh,Jakob Struye,Rafael Berkvens,Jeroen Famaey*

Main category: cs.LG

TL;DR: 该研究首次对比了sub-6 GHz和毫米波Wi-Fi信号在人员识别中的性能，使用最新的同步测量数据集。


<details>
  <summary>Details</summary>
Motivation: 探索毫米波Wi-Fi在人员识别中的优势，并进行与sub-6 GHz频段的比较研究。

Method: 使用端到端深度学习方法对两种频段的Wi-Fi信号进行比较，采用相同的训练流程和模型配置来确保公平性。

Result: 通过端到端深度学习，毫米波Wi-Fi信号在低采样率（10 Hz）下结合有效的背景减除，可以达到91.2%的人员识别准确率（针对20个人）。

Conclusion: 毫米波Wi-Fi信号在低采样率下结合有效的背景减除技术，可以实现较高的人员识别准确率。

Abstract: Person identification plays a vital role in enabling intelligent,
personalized, and secure human-computer interaction. Recent research has
demonstrated the feasibility of leveraging Wi-Fi signals for passive person
identification using a person's unique gait pattern. Although most existing
work focuses on sub-6 GHz frequencies, the emergence of mmWave offers new
opportunities through its finer spatial resolution, though its comparative
advantages for person identification remain unexplored. This work presents the
first comparative study between sub-6 GHz and mmWave Wi-Fi signals for person
identification with commercial off-the-shelf (COTS) Wi-Fi, using a novel
dataset of synchronized measurements from the two frequency bands in an indoor
environment. To ensure a fair comparison, we apply identical training pipelines
and model configurations across both frequency bands. Leveraging end-to-end
deep learning, we show that even at low sampling rates (10 Hz), mmWave Wi-Fi
signals can achieve high identification accuracy (91.2% on 20 individuals) when
combined with effective background subtraction.

</details>


### [19] [To Ask or Not to Ask: Learning to Require Human Feedback](https://arxiv.org/abs/2510.08314)
*Andrea Pugnana,Giovanni De Toni,Cesare Barbera,Roberto Pellungrini,Bruno Lepri,Andrea Passerini*

Main category: cs.LG

TL;DR: LtA improves human-AI collaboration by integrating expert feedback more effectively than traditional methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of Learning to Defer in human-AI collaborative systems by proposing a framework that better integrates expert input.

Method: Proposes a two-part architecture combining a standard ML model with an enriched model incorporating expert feedback, and develops two practical implementation approaches: sequential and joint training.

Result: LtA showed superior performance in experiments with synthetic and real expert data, enhancing flexibility and collaboration efficiency.

Conclusion: LtA offers a more robust framework for human-AI collaboration compared to traditional LtD approaches.

Abstract: Developing decision-support systems that complement human performance in
classification tasks remains an open challenge. A popular approach, Learning to
Defer (LtD), allows a Machine Learning (ML) model to pass difficult cases to a
human expert. However, LtD treats humans and ML models as mutually exclusive
decision-makers, restricting the expert contribution to mere predictions. To
address this limitation, we propose Learning to Ask (LtA), a new framework that
handles both when and how to incorporate expert input in an ML model. LtA is
based on a two-part architecture: a standard ML model and an enriched model
trained with additional expert human feedback, with a formally optimal strategy
for selecting when to query the enriched model. We provide two practical
implementations of LtA: a sequential approach, which trains the models in
stages, and a joint approach, which optimises them simultaneously. For the
latter, we design surrogate losses with realisable-consistency guarantees. Our
experiments with synthetic and real expert data demonstrate that LtA provides a
more flexible and powerful foundation for effective human-AI collaboration.

</details>


### [20] [Automated Machine Learning for Unsupervised Tabular Tasks](https://arxiv.org/abs/2510.07569)
*Prabhant Singh,Pieter Gijsbers,Elif Ceren Gok Yildirim,Murat Onur Yildirim,Joaquin Vanschoren*

Main category: cs.LG

TL;DR: 提出了用于无监督任务模型选择的LOTUS方法，通过评估数据集之间的相似性来推荐合适的机器学习管道。


<details>
  <summary>Details</summary>
Motivation: 希望找到一种能够对多种无监督机器学习任务进行模型选择的统一方法。

Method: 利用最佳传输距离来评估不同数据集的相似性，并推荐机器学习管道处理无监督任务。

Result: 通过与强基线进行实验对比，证明了LOTUS在进行模型选择方面的有效性。

Conclusion: LOTUS展示出在多种无监督任务中的模型选择能力，是针对多种无监督机器学习任务进行模型选择的一个很有前景的初步方案。

Abstract: In this work, we present LOTUS (Learning to Learn with Optimal Transport for
Unsupervised Scenarios), a simple yet effective method to perform model
selection for multiple unsupervised machine learning(ML) tasks such as outlier
detection and clustering. Our intuition behind this work is that a machine
learning pipeline will perform well in a new dataset if it previously worked
well on datasets with a similar underlying data distribution. We use Optimal
Transport distances to find this similarity between unlabeled tabular datasets
and recommend machine learning pipelines with one unified single method on two
downstream unsupervised tasks: outlier detection and clustering. We present the
effectiveness of our approach with experiments against strong baselines and
show that LOTUS is a very promising first step toward model selection for
multiple unsupervised ML tasks.

</details>


### [21] [Expanding the Action Space of LLMs to Reason Beyond Language](https://arxiv.org/abs/2510.07581)
*Zhongqi Yue,Weishi Wang,Yundaichuan Zhan,Juncheng Li,Daniel Dahlmeier,Fredrik D. Johansson*

Main category: cs.LG

TL;DR: EARL enhances LLM interaction capabilities with external environments, outperforming traditional methods in challenging tasks.


<details>
  <summary>Details</summary>
Motivation: To improve LLM interaction with external environments by expanding the action space beyond vocabulary tokens.

Method: ExpA Reinforcement Learning (EARL) with counterfactual policy optimization.

Result: EARL shows superior performance in calculator-based multitask learning and achieves perfect accuracy in sorting tasks, self-discovering efficient algorithms.

Conclusion: EARL outperforms traditional LLM actions confined to vocabulary tokens, achieving robust performance in tasks requiring multi-turn interactions and planning.

Abstract: Large Language Models (LLMs) are powerful reasoners in natural language, but
their actions are typically confined to outputting vocabulary tokens. As a
result, interactions with external environments -- such as symbolic operators
or simulators -- must be expressed through text in predefined formats, parsed,
and routed to external interfaces. This overloads the model's language with
both reasoning and control duties, and requires a hand-crafted parser, external
to the LLM. To address this, we decouple environment interactions from language
by internalizing them in an Expanded Action space (ExpA), beyond the
vocabulary. The model starts reasoning in the default language environment, but
may trigger routing actions and switch to an external environment at any time.
From there, the model can only invoke environment-specific actions, receive
feedback from the environment, and potentially route back to language as a
result. To promote effective exploration of the expanded action space and new
environments, we introduce ExpA Reinforcement Learning (EARL) with
counterfactual policy optimization. On tasks requiring multi-turn interactions
and contingent planning, EARL outperforms strong baselines with
vocabulary-constrained actions. It performs robustly across calculator-based
multi-task learning and, in the partially observed sorting problem, achieves
perfect Sort-4 accuracy while self-discovering an efficient algorithm
competitive with classical designs.

</details>


### [22] [TGM: a Modular and Efficient Library for Machine Learning on Temporal Graphs](https://arxiv.org/abs/2510.07586)
*Jacob Chmura,Shenyang Huang,Tran Gia Bao Ngo,Ali Parviz,Farimah Poursafaei,Jure Leskovec,Michael Bronstein,Guillaume Rabusseau,Matthias Fey,Reihaneh Rabbany*

Main category: cs.LG

TL;DR: TGM是一个研究导向的时序图机器学习库，统一了CTDG和DTDG方法，显著提升了性能并开辟了新的研究可能性。


<details>
  <summary>Details</summary>
Motivation: 现有的时序图机器学习库存在架构特定化的问题，阻碍了多样化模型的支持。同时缺乏将连续和离散时间动态图方法统一起来的平台。

Method: 使用TGM库进行实验，比较不同模型、数据集和任务的运行速度，特别是与DyGLib和现有实现进行比较。

Result: TGM实现了平均7.8倍的速度提升，相较于DyGLib，实现了平均175倍的图离散化速度提升。同时在实验中展示了TGM如何能够解锁新的研究可能性。

Conclusion: 实验结果表明，TGM可以实现显著的性能提升，并且开辟了新的研究可能性。TGM不仅效率高，还能解决以前不可能研究的问题。

Abstract: Well-designed open-source software drives progress in Machine Learning (ML)
research. While static graph ML enjoys mature frameworks like PyTorch Geometric
and DGL, ML for temporal graphs (TG), networks that evolve over time, lacks
comparable infrastructure. Existing TG libraries are often tailored to specific
architectures, hindering support for diverse models in this rapidly evolving
field. Additionally, the divide between continuous- and discrete-time dynamic
graph methods (CTDG and DTDG) limits direct comparisons and idea transfer. To
address these gaps, we introduce Temporal Graph Modelling (TGM), a
research-oriented library for ML on temporal graphs, the first to unify CTDG
and DTDG approaches. TGM offers first-class support for dynamic node features,
time-granularity conversions, and native handling of link-, node-, and
graph-level tasks. Empirically, TGM achieves an average 7.8x speedup across
multiple models, datasets, and tasks compared to the widely used DyGLib, and an
average 175x speedup on graph discretization relative to available
implementations. Beyond efficiency, we show in our experiments how TGM unlocks
entirely new research possibilities by enabling dynamic graph property
prediction and time-driven training paradigms, opening the door to questions
previously impractical to study. TGM is available at
https://github.com/tgm-team/tgm

</details>


### [23] [Transformer-Based Indirect Structural Health Monitoring of Rail Infrastructure with Attention-Driven Detection and Localization of Transient Defects](https://arxiv.org/abs/2510.07606)
*Sizhe Ma,Katherine A. Flanigan,Mario Bergés,James D. Brooks*

Main category: cs.LG

TL;DR: 研究通过车载传感器进行断轨检测，并提出了一种新的注意力集中的Transformer模型，通过处理噪声问题，在保持准确度的同时实现更高的推理速度。


<details>
  <summary>Details</summary>
Motivation: 间接结构健康监测（iSHM）通过车载传感器进行断轨检测是一种成本效益高的轨道评估范式，但由于复杂的车辆动力学、信号噪声以及标记数据的稀缺限制了监督方法，使得可靠检测小而短暂的异常（2-10 cm）仍然是一个重要挑战。

Method: 研究提出了一种注意力集中的Transformer模型，该模型通过重构进行训练，并主要通过学习得出的注意力权重偏差来创新地导出异常得分。

Result: 基准测试结果表明，尽管基于Transformer的模型普遍表现优于其他模型，但所有测试模型在面对高频局部噪声时均展示出了显著的脆弱性，这被认为是实际部署的关键瓶颈。

Conclusion: 虽然目前的模型在高频局部噪声上存在明显缺陷，但所提出的注意力集中的Transformer模型在达到与最先进解决方案相当的准确度的同时，表现出更佳的推理速度。

Abstract: Indirect structural health monitoring (iSHM) for broken rail detection using
onboard sensors presents a cost-effective paradigm for railway track
assessment, yet reliably detecting small, transient anomalies (2-10 cm) remains
a significant challenge due to complex vehicle dynamics, signal noise, and the
scarcity of labeled data limiting supervised approaches. This study addresses
these issues through unsupervised deep learning. We introduce an incremental
synthetic data benchmark designed to systematically evaluate model robustness
against progressively complex challenges like speed variations, multi-channel
inputs, and realistic noise patterns encountered in iSHM. Using this benchmark,
we evaluate several established unsupervised models alongside our proposed
Attention-Focused Transformer. Our model employs a self-attention mechanism,
trained via reconstruction but innovatively deriving anomaly scores primarily
from deviations in learned attention weights, aiming for both effectiveness and
computational efficiency. Benchmarking results reveal that while
transformer-based models generally outperform others, all tested models exhibit
significant vulnerability to high-frequency localized noise, identifying this
as a critical bottleneck for practical deployment. Notably, our proposed model
achieves accuracy comparable to the state-of-the-art solution while
demonstrating better inference speed. This highlights the crucial need for
enhanced noise robustness in future iSHM models and positions our more
efficient attention-based approach as a promising foundation for developing
practical onboard anomaly detection systems.

</details>


### [24] [LLM Unlearning Under the Microscope: A Full-Stack View on Methods and Metrics](https://arxiv.org/abs/2510.07626)
*Chongyu Fan,Changsheng Wang,Yancheng Huang,Soumyadeep Pal,Sijia Liu*

Main category: cs.LG

TL;DR: 本文对大型语言模型反学习进行了全面的回顾，提出了一种新的分类法，并引入更能反映生成性能的开放问答指标。


<details>
  <summary>Details</summary>
Motivation: 尽管在过去两年中进展迅速，大型语言模型反学习的研究仍然分散，缺乏对有效反学习的定义和评估标准的明确性。

Method: 提出了一个原则性的分类法，涵盖了十二种最近的有状态反学习方法，分为三类：基于偏差驱动的优化、表示失配和基于拒绝的有针对性的反学习。同时引入开放式问答(Open-QA)指标，以更好地捕捉生成性能。

Result: 分析显示当前评估方法多以多项选择题准确性为主，导致对成功的狭窄视角，并常常忽视了模型的实际生成行为。引入的开放式问答指标揭示了方法家族之间固有的UE-UT权衡关系，且在领域内重新学习与领域外微调间的鲁棒性存在显著差异。

Conclusion: 本文旨在为大型语言模型的机器反学习提供一个完整的回顾，并为未来的方法设计和评估提供可操作的指导。

Abstract: Machine unlearning for large language models (LLMs) aims to remove undesired
data, knowledge, and behaviors (e.g., for safety, privacy, or copyright) while
preserving useful model capabilities. Despite rapid progress over the past two
years, research in LLM unlearning remains fragmented, with limited clarity on
what constitutes effective unlearning and how it should be rigorously
evaluated. In this work, we present a principled taxonomy of twelve recent
stateful unlearning methods, grouped into three methodological families:
divergence-driven optimization, representation misalignment, and
rejection-based targeted unlearning. Building on this taxonomy, we revisit the
evaluation of unlearning effectiveness (UE), utility retention (UT), and
robustness (Rob), focusing on the WMDP benchmark. Our analysis shows that
current evaluations, dominated by multiple-choice question (MCQ) accuracy,
offer only a narrow perspective, often overstating success while overlooking
the model's actual generation behavior. To address this gap, we introduce open
question-answering (Open-QA) metrics that better capture generative performance
and reveal the inherent UE-UT tradeoff across method families. Furthermore, we
demonstrate that robustness requires finer-grained analysis: for example,
vulnerabilities differ substantially between in-domain relearning and
out-of-domain fine-tuning, even though both fall under model-level attacks.
Through this study, we hope to deliver a full-stack revisit of LLM unlearning
and actionable guidance for designing and evaluating future methods.

</details>


### [25] [Property Classification of Vacation Rental Properties during Covid-19](https://arxiv.org/abs/2510.07639)
*Favour Yahdii Aghaebe,Dustin Foley,Eric Atwell,Stephen Clark*

Main category: cs.LG

TL;DR: 研究利用聚类技术分类疫情期间活跃的度假租赁物业，以识别其模式和行为，并可能用于制定针对特定群体的政策。


<details>
  <summary>Details</summary>
Motivation: 使用聚类技术对疫情期间活跃的度假租赁物业进行分类，以识别其固有模式和行为。

Method: 使用K-means和K-medoids聚类技术来识别同质群组及其共同特征。

Result: 通过聚类分析，可以更好地理解度假租赁评价的复杂性。

Conclusion: 通过聚类技术识别度假租赁属性的内在模式和行为，增强对度假租赁评价复杂性的理解，有助于制定针对特定群体的政策。

Abstract: This study advocates for employing clustering techniques to classify vacation
rental properties active during the Covid pandemic to identify inherent
patterns and behaviours. The dataset, a collaboration between the ESRC funded
Consumer Data Research Centre (CDRC) and AirDNA, encompasses data for over a
million properties and hosts. Utilising K-means and K-medoids clustering
techniques, we identify homogenous groups and their common characteristics. Our
findings enhance comprehension of the intricacies of vacation rental
evaluations and could potentially be utilised in the creation of targeted,
cluster-specific policies.

</details>


### [26] [Design-Based Bandits Under Network Interference: Trade-Off Between Regret and Statistical Inference](https://arxiv.org/abs/2510.07646)
*Zichen Wang,Haoyang Hong,Chuanhao Li,Haoxuan Li,Zhiheng Zhang,Huazheng Wang*

Main category: cs.LG

TL;DR: 研究了MABNI中的遗憾和推理准确性的权衡，提出了新的理论和算法。


<details>
  <summary>Details</summary>
Motivation: 现有研究过于强调最佳选择，可能影响次佳选择的推理准确性，尤其在MABNI情况下更为明显。

Method: 引入了随时有效的渐近置信序列，并设计了算法\texttt{EXP3-N-CS}。

Result: 成功平衡了在对抗性MABNI下遗憾最小化与推理准确性之间的权衡。

Conclusion: 首次建立了MABNI中遗憾最小化与推理准确性之间的理论帕累托前沿，并提出了相应的算法。

Abstract: In multi-armed bandits with network interference (MABNI), the action taken by
one node can influence the rewards of others, creating complex interdependence.
While existing research on MABNI largely concentrates on minimizing regret, it
often overlooks the crucial concern that an excessive emphasis on the optimal
arm can undermine the inference accuracy for sub-optimal arms. Although initial
efforts have been made to address this trade-off in single-unit scenarios,
these challenges have become more pronounced in the context of MABNI. In this
paper, we establish, for the first time, a theoretical Pareto frontier
characterizing the trade-off between regret minimization and inference accuracy
in adversarial (design-based) MABNI. We further introduce an anytime-valid
asymptotic confidence sequence along with a corresponding algorithm,
$\texttt{EXP3-N-CS}$, specifically designed to balance the trade-off between
regret minimization and inference accuracy in this setting.

</details>


### [27] [Value Flows](https://arxiv.org/abs/2510.07650)
*Perry Dong,Chongyi Zheng,Chelsea Finn,Dorsa Sadigh,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: 该论文提出了一种基于流模型的分布式RL方法，通过细粒度的回报分布估计和不确定性识别，提高了学习效果和决策的准确性。实验结果显示成功率平均提高了1.3倍。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习方法通常将未来回报的分布简化为一个单一标量值，而分布式RL方法则利用回报分布提供更强的学习信号，并在探索和安全RL中有应用潜力。现有的估计方法无法回答回报分布的细粒度结构问题及如何识别具有高回报不确定性的状态。因此本研究提出使用流模型进行更精细的估计。

Method: 使用现代流模型估计完整的未来回报分布，并通过新的流匹配目标生成满足Bellman方程的概率密度路径。利用流衍生ODE计算不同状态的回报不确定性，并根据不确定性信息优先学习更准确的回报估计。

Result: 实验表明，在37个基于状态和25个基于图像的基准任务中，该方法（Value Flows）的成功率平均提高了1.3倍。

Conclusion: 该研究提出了一种创新的方法，通过现代灵活的流模型估计未来回报分布，并识别回报方差高的状态。这些方法比现有的离散分类和有限分位数估计方法提供了更细粒度的回报分布结构，为决策提供了更强的学习信号。通过实验表明，该方法在离线和在线设置中均表现优异。

Abstract: While most reinforcement learning methods today flatten the distribution of
future returns to a single scalar value, distributional RL methods exploit the
return distribution to provide stronger learning signals and to enable
applications in exploration and safe RL. While the predominant method for
estimating the return distribution is by modeling it as a categorical
distribution over discrete bins or estimating a finite number of quantiles,
such approaches leave unanswered questions about the fine-grained structure of
the return distribution and about how to distinguish states with high return
uncertainty for decision-making. The key idea in this paper is to use modern,
flexible flow-based models to estimate the full future return distributions and
identify those states with high return variance. We do so by formulating a new
flow-matching objective that generates probability density paths satisfying the
distributional Bellman equation. Building upon the learned flow models, we
estimate the return uncertainty of distinct states using a new flow derivative
ODE. We additionally use this uncertainty information to prioritize learning a
more accurate return estimation on certain transitions. We compare our method
(Value Flows) with prior methods in the offline and online-to-online settings.
Experiments on $37$ state-based and $25$ image-based benchmark tasks
demonstrate that Value Flows achieves a $1.3\times$ improvement on average in
success rates. Website: https://pd-perry.github.io/value-flows Code:
https://github.com/chongyi-zheng/value-flows

</details>


### [28] [Incremental Hybrid Ensemble with Graph Attention and Frequency-Domain Features for Stable Long-Term Credit Risk Modeling](https://arxiv.org/abs/2510.07663)
*Jiajing Wang*

Main category: cs.LG

TL;DR: HYDRA-EI是一种混合集成增量学习框架，通过多阶段特征处理和多模型结合，提高了长期贷款违约预测的稳定性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 长期贷款违约预测困难是因为借款人行为经常变化且数据分布随时间发生变化。

Method: HYDRA-EI使用图注意力机制、自动交叉特征创建以及频域转换进行特征处理，结合多种模型进行增量学习，以每周更新数据并通过简单的性能评估方法调整模型权重。

Result: HYDRA-EI通过增加模型稳定性和泛化能力，改善了长期信用风险任务的效果。

Conclusion: HYDRA-EI提高了模型的稳定性和泛化能力，使其可以用于长期信用风险任务。

Abstract: Predicting long-term loan defaults is hard because borrower behavior often
changes and data distributions shift over time. This paper presents HYDRA-EI, a
hybrid ensemble incremental learning framework. It uses several stages of
feature processing and combines multiple models. The framework builds
relational, cross, and frequency-based features. It uses graph attention,
automatic cross-feature creation, and transformations from the frequency
domain. HYDRA-EI updates weekly using new data and adjusts the model weights
with a simple performance-based method. It works without frequent manual
changes or fixed retraining. HYDRA-EI improves model stability and
generalization, which makes it useful for long-term credit risk tasks.

</details>


### [29] [FedQS: Optimizing Gradient and Model Aggregation for Semi-Asynchronous Federated Learning](https://arxiv.org/abs/2510.07664)
*Yunbo Li,Jiaping Gui,Zhihang Deng,Fanchao Meng,Yue Wu*

Main category: cs.LG

TL;DR: 大纲介绍了FedQS框架，它通过分析差异并采用分而治之策略，解决了半异步联邦学习中的聚合挑战，实现了高准确性、低损失和快收敛速度的解决方案。


<details>
  <summary>Details</summary>
Motivation: 半异步联邦学习（SAFL）在同步和异步联邦学习之间提供了一种平衡的方法，但在优化梯度聚合（如FedSGD）和模型聚合（如FedAvg）策略上面临显著挑战，这两者在准确性、收敛速度和稳定性上具有不同的权衡。虽然梯度聚合能够实现更快的收敛和更高的准确性，但却存在明显波动，而模型聚合则提供更大的稳定性但收敛速度较慢且精度次优。因此，本文提出FedQS框架来分析和解决这些差异。

Method: FedQS框架采用一种分而治之的策略，以应对客户端异质性，通过将客户端分为四种不同类型，并根据数据分布特征和可用计算资源来自适应地优化它们的本地训练。

Result: FedQS在计算机视觉、自然语言处理和真实世界任务上的广泛实验表明，其实现了最高的准确度、最低的损失，并在收敛速度上位列最快之列，超越了最先进的基线。

Conclusion: 我们的工作弥合了SAFL中的聚合策略之间的差距，提供了一种统一的解决方案，实现稳定、准确和高效的联邦学习。

Abstract: Federated learning (FL) enables collaborative model training across multiple
parties without sharing raw data, with semi-asynchronous FL (SAFL) emerging as
a balanced approach between synchronous and asynchronous FL. However, SAFL
faces significant challenges in optimizing both gradient-based (e.g., FedSGD)
and model-based (e.g., FedAvg) aggregation strategies, which exhibit distinct
trade-offs in accuracy, convergence speed, and stability. While gradient
aggregation achieves faster convergence and higher accuracy, it suffers from
pronounced fluctuations, whereas model aggregation offers greater stability but
slower convergence and suboptimal accuracy. This paper presents FedQS, the
first framework to theoretically analyze and address these disparities in SAFL.
FedQS introduces a divide-and-conquer strategy to handle client heterogeneity
by classifying clients into four distinct types and adaptively optimizing their
local training based on data distribution characteristics and available
computational resources. Extensive experiments on computer vision, natural
language processing, and real-world tasks demonstrate that FedQS achieves the
highest accuracy, attains the lowest loss, and ranks among the fastest in
convergence speed, outperforming state-of-the-art baselines. Our work bridges
the gap between aggregation strategies in SAFL, offering a unified solution for
stable, accurate, and efficient federated learning. The code and datasets are
available at https://anonymous.4open.science/r/FedQS-EDD6.

</details>


### [30] [Computationally-efficient Graph Modeling with Refined Graph Random Features](https://arxiv.org/abs/2510.07716)
*Krzysztof Choromanski,Avinava Dubey,Arijit Sehanobish,Isaac Reid*

Main category: cs.LG

TL;DR: GRFs++通过连接短游走和改进停止机制提高计算效率和准确性，解决了传统GRFs存在的问题。


<details>
  <summary>Details</summary>
Motivation: 解决常规GRFs对较远节点建模困难的问题，并减少对长随机游走的依赖。

Method: 通过步行缝合技术，GRFs++可以连接较短的随机游走而不破坏无偏性。此外，还扩展了停止机制，提高了图核的近似准确性。

Result: GRFs++具有长游走提供的高质量近似但效率更高，并通过并行计算和矩阵乘法提高计算效率。同时采用更广泛的游走长度分布策略增强近似准确性，且不增加计算成本。

Conclusion: GRFs++改进了传统GRFs，通过新的技术提高了图随机特征方法的效率和准确性，特别是在图节点上定义的核计算方面。

Abstract: We propose refined GRFs (GRFs++), a new class of Graph Random Features (GRFs)
for efficient and accurate computations involving kernels defined on the nodes
of a graph. GRFs++ resolve some of the long-standing limitations of regular
GRFs, including difficulty modeling relationships between more distant nodes.
They reduce dependence on sampling long graph random walks via a novel
walk-stitching technique, concatenating several shorter walks without breaking
unbiasedness. By applying these techniques, GRFs++ inherit the approximation
quality provided by longer walks but with greater efficiency, trading
sequential, inefficient sampling of a long walk for parallel computation of
short walks and matrix-matrix multiplication. Furthermore, GRFs++ extend the
simplistic GRFs walk termination mechanism (Bernoulli schemes with fixed
halting probabilities) to a broader class of strategies, applying general
distributions on the walks' lengths. This improves the approximation accuracy
of graph kernels, without incurring extra computational cost. We provide
empirical evaluations to showcase all our claims and complement our results
with theoretical analysis.

</details>


### [31] [DEAS: DEtached value learning with Action Sequence for Scalable Offline RL](https://arxiv.org/abs/2510.07730)
*Changyeon Kim,Haeone Lee,Younggyo Seo,Kimin Lee,Yuke Zhu*

Main category: cs.LG

TL;DR: DEAS是一个利用行动序列进行价值学习的离线强化学习框架，在复杂任务中超越基准模型，并改善实际任务表现。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习可以避免昂贵的在线交互，但在处理复杂的长时间跨度的决策问题时仍存在困难。

Method: 提出了DEAS，一个离线强化学习框架，利用行动序列进行价值学习，并通过半马尔可夫决策过程Q学习减少规划时间跨度。

Result: DEAS在OGBench的复杂长时间跨度任务中优于基准，并提升大规模视听行为模型的性能，在模拟与实际任务中表现出色。

Conclusion: DEAS框架能够优化长时间跨度任务中的价值估算，并有效提升离线强化学习及大规模模型应用的性能。

Abstract: Offline reinforcement learning (RL) presents an attractive paradigm for
training intelligent agents without expensive online interactions. However,
current approaches still struggle with complex, long-horizon sequential
decision making. In this work, we introduce DEtached value learning with Action
Sequence (DEAS), a simple yet effective offline RL framework that leverages
action sequences for value learning. These temporally extended actions provide
richer information than single-step actions and can be interpreted through the
options framework via semi-Markov decision process Q-learning, enabling
reduction of the effective planning horizon by considering longer sequences at
once. However, directly adopting such sequences in actor-critic algorithms
introduces excessive value overestimation, which we address through detached
value learning that steers value estimates toward in-distribution actions that
achieve high return in the offline dataset. We demonstrate that DEAS
consistently outperforms baselines on complex, long-horizon tasks from OGBench
and can be applied to enhance the performance of large-scale
Vision-Language-Action models that predict action sequences, significantly
boosting performance in both RoboCasa Kitchen simulation tasks and real-world
manipulation tasks.

</details>


### [32] [GeoGen: A Two-stage Coarse-to-Fine Framework for Fine-grained Synthetic Location-based Social Network Trajectory Generation](https://arxiv.org/abs/2510.07735)
*Rongchao Xu,Kunlin Cai,Lin Jiang,Dahai Yu,Zhiqing Hong,Yuan Tian,Guang Wang*

Main category: cs.LG

TL;DR: GeoGen通过两阶段框架生成合成LBSN轨迹，在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 由于收集轨迹数据的高成本和日益增长的隐私担忧，获取大规模LBSN轨迹数据存在困难。最近合成数据生成技术的进步给我们提供了新的机会，可以利用生成式AI生成保持真实数据特征但同时保护隐私的合成数据。

Method: 本文提出了一种名为GeoGen的两阶段粗到细生成框架来生成大规模LBSN签到轨迹，第一阶段重建空间连续、时间上规律的潜在移动序列，然后设计稀疏感知时空扩散模型（S$^2$TDiff）；第二阶段设计了基于Transformer的Seq2Seq架构，称为Coarse2FineNet。

Result: GeoGen在四个真实数据集上的广泛实验表明，它在忠实度和实用性评估方面优于最先进的模型，例如，在FS-TKY数据集上的距离和半径指标方面分别提升了超过69%和55%。

Conclusion: GeoGen模型在生成合成LBSN签到轨迹方面优于现有的先进模型，尤其是在忠实度和实用性评估方面表现卓越。

Abstract: Location-Based Social Network (LBSN) check-in trajectory data are important
for many practical applications, like POI recommendation, advertising, and
pandemic intervention. However, the high collection costs and ever-increasing
privacy concerns prevent us from accessing large-scale LBSN trajectory data.
The recent advances in synthetic data generation provide us with a new
opportunity to achieve this, which utilizes generative AI to generate synthetic
data that preserves the characteristics of real data while ensuring privacy
protection. However, generating synthetic LBSN check-in trajectories remains
challenging due to their spatially discrete, temporally irregular nature and
the complex spatio-temporal patterns caused by sparse activities and uncertain
human mobility. To address this challenge, we propose GeoGen, a two-stage
coarse-to-fine framework for large-scale LBSN check-in trajectory generation.
In the first stage, we reconstruct spatially continuous, temporally regular
latent movement sequences from the original LBSN check-in trajectories and then
design a Sparsity-aware Spatio-temporal Diffusion model (S$^2$TDiff) with an
efficient denosing network to learn their underlying behavioral patterns. In
the second stage, we design Coarse2FineNet, a Transformer-based Seq2Seq
architecture equipped with a dynamic context fusion mechanism in the encoder
and a multi-task hybrid-head decoder, which generates fine-grained LBSN
trajectories based on coarse-grained latent movement sequences by modeling
semantic relevance and behavioral uncertainty. Extensive experiments on four
real-world datasets show that GeoGen excels state-of-the-art models for both
fidelity and utility evaluation, e.g., it increases over 69% and 55% in
distance and radius metrics on the FS-TKY dataset.

</details>


### [33] [MeSH: Memory-as-State-Highways for Recursive Transformers](https://arxiv.org/abs/2510.07739)
*Chengting Yu,Xiaobo Shu,Yadao Wang,Yizhen Zhang,Haoyi Wu,Jiaang Li,Rujiao Long,Ziheng Chen,Yuchi Xu,Wenbo Su,Bo Zheng*

Main category: cs.LG

TL;DR: 本文提出MeSH方案，通过增强递归变压器来解决未分化计算和信息过载问题，显著提升模型性能并减少参数。


<details>
  <summary>Details</summary>
Motivation: 探测隐藏状态以追踪递归模型性能落后的原因，并解决未分化计算和信息过载问题。

Method: 引入内存状态高速公路（MeSH）方案，将状态管理外化为显式内存缓冲区，并使用轻量级路由器在各次迭代间动态丰富计算。

Result: 在Pythia套件（160M-1.4B）上的测试显示，MeSH增强的递归变压器在1.4B规模上性能超越其更大的非递归对手，平均下游准确性提高1.06%，非嵌入参数减少33%。

Conclusion: MeSH作为一个可扩展和有原则的架构，能够构建更强大的递归模型。

Abstract: Recursive transformers reuse parameters and iterate over hidden states
multiple times, decoupling compute depth from parameter depth. However, under
matched compute, recursive models with fewer parameters often lag behind
non-recursive counterparts. By probing hidden states, we trace this performance
gap to two primary bottlenecks: undifferentiated computation, where the core is
forced to adopt a similar computational pattern at every iteration, and
information overload, where long-lived and transient information must coexist
in a single hidden state. To address the issues, we introduce a
Memory-as-State-Highways (MeSH) scheme, which externalizes state management
into an explicit memory buffer and employs lightweight routers to dynamically
diversify computation across iterations. Probing visualizations confirm that
MeSH successfully resolves the pathologies by inducing functional
specialization across iterations. On the Pythia suite (160M-1.4B),
MeSH-enhanced recursive transformers consistently improve over recursive
baselines and outperforms its larger non-recursive counterpart at the 1.4B
scale, improving average downstream accuracy by +1.06% with 33% fewer
non-embedding parameters. Our analysis establishes MeSH as a scalable and
principled architecture for building stronger recursive models.

</details>


### [34] [t-SNE Exaggerates Clusters, Provably](https://arxiv.org/abs/2510.07746)
*Noah Bergam,Szymon Snoeck,Nakul Verma*

Main category: cs.LG

TL;DR: t-SNE不能可靠地反映输入数据的聚类强度和离群点特征。


<details>
  <summary>Details</summary>
Motivation: 研究t-SNE在可视化与输入数据结构匹配方面的可靠性。

Method: 证明并演示t-SNE在输入聚类强度和离群点极端性方面失败的情况。

Result: 在实践中也证明了t-SNE的失败模式。

Conclusion: t-SNE的输出无法可靠地推断输入聚类的强度和离群点的极端性。

Abstract: Central to the widespread use of t-distributed stochastic neighbor embedding
(t-SNE) is the conviction that it produces visualizations whose structure
roughly matches that of the input. To the contrary, we prove that (1) the
strength of the input clustering, and (2) the extremity of outlier points,
cannot be reliably inferred from the t-SNE output. We demonstrate the
prevalence of these failure modes in practice as well.

</details>


### [35] [FedBook: A Unified Federated Graph Foundation Codebook with Intra-domain and Inter-domain Knowledge Modeling](https://arxiv.org/abs/2510.07755)
*Zhengyu Wu,Yinlin Zhu,Xunkai Li,Ziang Qiu,Rong-Hua Li,Guoren Wang,Chenghu Zhou*

Main category: cs.LG

TL;DR: FedBook通过两阶段过程构建统一的联邦图基础代码本，显著改善了图基础模型在跨域和多任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有GFMs通常假设对多域图的集中访问，这由于隐私和制度限制通常是不可行的。联邦图基础模型（FedGFMs）解决了这一限制，但其有效性基本上依赖于构建一个稳健的全球代码本，该代码本通过巩固每个领域内相互强化的语义来实现域内一致性，同时通过保留跨领域的异构知识来保持域间多样性。

Method: FedBook遵循两个阶段的过程：阶段一是域内协作（Intra-domain Collaboration），在这一阶段中，通过参照跨客户端语义上更可靠的高频词来改进低频词，以增强域特定的一致性；阶段二是域间整合（Inter-domain Integration），在这一阶段中，全球GFM的汇总过程里按客户端代码本的语义独特性对其贡献进行加权，从而保留跨领域的多样性。

Result: FedBook在多个领域和任务的8个基准上进行了广泛实验，结果显示其性能始终优于包括孤立的监督学习、FL/FGL、集中GFMs的联邦适配和FedGFM技术在内的21个基准。

Conclusion: FedBook在多个领域和任务的8个基准上进行的广泛实验中，始终优于包括单独监督学习、FL/FGL、集中GFMs的联邦适配和FedGFM技术在内的21个基准。

Abstract: Foundation models have shown remarkable cross-domain generalization in
language and vision, inspiring the development of graph foundation models
(GFMs). However, existing GFMs typically assume centralized access to
multi-domain graphs, which is often infeasible due to privacy and institutional
constraints. Federated Graph Foundation Models (FedGFMs) address this
limitation, but their effectiveness fundamentally hinges on constructing a
robust global codebook that achieves intra-domain coherence by consolidating
mutually reinforcing semantics within each domain, while also maintaining
inter-domain diversity by retaining heterogeneous knowledge across domains. To
this end, we propose FedBook, a unified federated graph foundation codebook
that systematically aggregates clients' local codebooks during server-side
federated pre-training. FedBook follows a two-phase process: (1) Intra-domain
Collaboration, where low-frequency tokens are refined by referencing more
semantically reliable high-frequency tokens across clients to enhance
domain-specific coherence; and (2) Inter-domain Integration, where client
contributions are weighted by the semantic distinctiveness of their codebooks
during the aggregation of the global GFM, thereby preserving cross-domain
diversity. Extensive experiments on 8 benchmarks across multiple domains and
tasks demonstrate that FedBook consistently outperforms 21 baselines, including
isolated supervised learning, FL/FGL, federated adaptations of centralized
GFMs, and FedGFM techniques.

</details>


### [36] [A Unified Multi-Task Learning Framework for Generative Auto-Bidding with Validation-Aligned Optimization](https://arxiv.org/abs/2510.07760)
*Yiqin Lv,Zhiyu Mou,Miao Xu,Jinghao Chen,Qi Wang,Yixiu Mao,Yun Qu,Rongquan Bai,Chuan Yu,Jian Xu,Bo Zheng,Xiangyang Ji*

Main category: cs.LG

TL;DR: 提出了一种新的验证对齐多任务优化方法（VAMO），通过自适应任务权重分配和先进的竞价生成框架显著提升广告系统表现。


<details>
  <summary>Details</summary>
Motivation: 现有的多任务优化策略主要依赖于训练动态，在波动的竞价环境中表现较差。为了改善这一问题，提出了新的优化方法以提升验证改善和更好地匹配部署目标。

Method: 提出了一种新的验证对齐多任务优化方法（VAMO），通过根据每个任务训练梯度与验证梯度之间的对齐情况来自适应地分配任务权重，并结合周期性意识的时间模块和高级生成自我竞价框架来增强跨任务的季节性结构转移和竞价性能。

Result: 在模拟和大规模真实广告系统上的实验一致表明，与典型的基线模型相比，所提出的方法表现出显著的改进。

Conclusion: 实验结果表明，与典型的基线模型相比，VAMO方法在广告系统中表现出显著的改进，证明了其有效性。

Abstract: In online advertising, heterogeneous advertiser requirements give rise to
numerous customized bidding tasks that are typically optimized independently,
resulting in extensive computation and limited data efficiency. Multi-task
learning offers a principled framework to train these tasks jointly through
shared representations. However, existing multi-task optimization strategies
are primarily guided by training dynamics and often generalize poorly in
volatile bidding environments. To this end, we present Validation-Aligned
Multi-task Optimization (VAMO), which adaptively assigns task weights based on
the alignment between per-task training gradients and a held-out validation
gradient, thereby steering updates toward validation improvement and better
matching deployment objectives. We further equip the framework with a
periodicity-aware temporal module and couple it with an advanced generative
auto-bidding backbone to enhance cross-task transfer of seasonal structure and
strengthen bidding performance. Meanwhile, we provide theoretical insights into
the proposed method, e.g., convergence guarantee and alignment analysis.
Extensive experiments on both simulated and large-scale real-world advertising
systems consistently demonstrate significant improvements over typical
baselines, illuminating the effectiveness of the proposed approach.

</details>


### [37] [Weak Form Learning for Mean-Field Partial Differential Equations: an Application to Insect Movement](https://arxiv.org/abs/2510.07786)
*Seth Minor,Bret D. Elderd,Benjamin Van Allen,David M. Bortz,Vanja Dukic*

Main category: cs.LG

TL;DR: 研究从稀疏数据中使用弱形式方程学习技术，预测秋粘虫等害虫动态，改善虫害管理。


<details>
  <summary>Details</summary>
Motivation: 理解作物及林业害虫的扩散动态，能更好地预测其爆发强度和位置，从而促进害虫管理。

Method: 本文利用弱形式稀疏非线性动力学识别算法（WSINDy）等Galerkin方法，还结合密度估计技术，学习昆虫迁移的Fokker-Planck方程。

Result: 在模拟农业条件下，我们验证了在秋粘虫位置测量上的方法效用。

Conclusion: 我们通过扩展弱形式方程学习技术，结合核密度估计，从稀疏的实验数据中学习有效的模型，用于预测作物害虫的扩散动态。这不仅提高了害虫管理，还能更好地预测爆发强度和位置。

Abstract: Insect species subject to infection, predation, and anisotropic environmental
conditions may exhibit preferential movement patterns. Given the innate
stochasticity of exogenous factors driving these patterns over short
timescales, individual insect trajectories typically obey overdamped stochastic
dynamics. In practice, data-driven modeling approaches designed to learn the
underlying Fokker-Planck equations from observed insect distributions serve as
ideal tools for understanding and predicting such behavior. Understanding
dispersal dynamics of crop and silvicultural pests can lead to a better
forecasting of outbreak intensity and location, which can result in better pest
management. In this work, we extend weak-form equation learning techniques,
coupled with kernel density estimation, to learn effective models for
lepidopteran larval population movement from highly sparse experimental data.
Galerkin methods such as the Weak form Sparse Identification of Nonlinear
Dynamics (WSINDy) algorithm have recently proven useful for learning governing
equations in several scientific contexts. We demonstrate the utility of the
method on a sparse dataset of position measurements of fall armyworms
(Spodoptera frugiperda) obtained in simulated agricultural conditions with
varied plant resources and infection status.

</details>


### [38] [HySim-LLM: Embedding-Weighted Fine-Tuning Bounds and Manifold Denoising for Domain-Adapted LLMs](https://arxiv.org/abs/2510.07796)
*Majid Jaberi-Douraki,Hossein Sholehrasa,Xuan Xu,Remya Ampadi Ramachandran*

Main category: cs.LG

TL;DR: 本文提出HySim-LLM框架，通过嵌入加权微调与流形感知去噪改善大型语言模型对药代动力学信息处理的适应性和性能。


<details>
  <summary>Details</summary>
Motivation: 药代动力学信息的提取和标准化是计算药理学中亟待解决的问题，这限制了药物开发中数据驱动模型的可靠性，因此需找到一种方法处理结构化生物医学数据中的异质性、噪声和领域转移问题。

Method: HySim-LLM框架结合了嵌入加权微调与流形感知去噪技术，用以增强大型语言模型在药代动力学信息提取中的适应性和性能。

Result: 通过建立相似性加权的泛化界限和基于流形的去噪保障来量化嵌入分歧下的适应性能，HySim-LLM框架显著增强了大型语言模型在结构化生物医学数据处理中的鲁棒性和可解释性。

Conclusion: 本文提出的HySim-LLM框架有效地增强了大型语言模型在结构化生物医学数据处理中的鲁棒性和可解释性，为药物开发中的数据驱动模型提供了可靠支持。该框架通过嵌入加权微调和流形感知去噪，克服了异质性、噪声和领域转换导致的限制。

Abstract: The extraction and standardization of pharmacokinetic (PK) information from
scientific literature remain significant challenges in computational
pharmacology, which limits the reliability of data-driven models in drug
development. Large language models (LLMs) have achieved remarkable progress in
text understanding and reasoning, yet their adaptation to structured biomedical
data, such as PK tables, remains constrained by heterogeneity, noise, and
domain shift. To address these limitations, we propose HySim-LLM, a unified
mathematical and computational framework that integrates embedding-weighted
fine-tuning and manifold-aware denoising to enhance the robustness and
interpretability of LLMs. We establish two theoretical results: (1) a
similarity-weighted generalization bound that quantifies adaptation performance
under embedding divergence, and (2) a manifold-based denoising guarantee that
bounds loss contributions from noisy or off-manifold samples. These theorems
provide a principled foundation for fine-tuning LLMs in structured biomedical
settings. The framework offers a mathematically grounded pathway toward
reliable and interpretable LLM adaptation for biomedical and data-intensive
scientific domains.

</details>


### [39] [SIMU: Selective Influence Machine Unlearning](https://arxiv.org/abs/2510.07822)
*Anu Agarwal,Mihir Pamnani,Dilek Hakkani-Tur*

Main category: cs.LG

TL;DR: 提出了选择性影响机器遗忘（SIMU）框架，通过选择性更新关键神经元实现对敏感信息的遗忘，同时保留模型的原始能力。


<details>
  <summary>Details</summary>
Motivation: 当前机器遗忘方法在遗忘目标信息时，常常导致模型能力的损失。因此需要一种能够在遗忘信息的同时保留模型原始能力的方法。

Method: 提出了一种名为选择性影响机器遗忘（SIMU）的两步框架，通过选择性地更新负责编码遗忘集的关键神经元，实现优化器驱动遗忘方法。

Result: SIMU框架显著优于当前方法，能够保留模型原始知识的同时实现针对性遗忘。

Conclusion: 通过选择性地更新负责编码遗忘集的关键神经元，SIMU框架在不影响模型原始能力的情况下，实现了与当前方法相当的遗忘效果。

Abstract: The undesired memorization of sensitive information by Large Language Models
(LLMs) has emphasized the need for safety mechanisms that can regulate model
behavior. This has led to the development of machine unlearning techniques that
enable models to precisely forget sensitive and unwanted information. For
machine unlearning, first-order and second-order optimizer-based methods have
shown significant progress in enabling LLMs to forget targeted information.
However, in doing so, these approaches often compromise the model's original
capabilities, resulting in unlearned models that struggle to retain their prior
knowledge and overall utility. To address this, we propose Selective Influence
Machine Unlearning (SIMU), a two-step framework that enhances second-order
optimizer-based unlearning by selectively updating only the critical neurons
responsible for encoding the forget-set. By constraining updates to these
targeted neurons, SIMU achieves comparable unlearning efficacy while
substantially outperforming current methods in retaining the model's original
knowledge.

</details>


### [40] [MetaDefense: Defending Finetuning-based Jailbreak Attack Before and During Generation](https://arxiv.org/abs/2510.07835)
*Weisen Jiang,Sinno Jialin Pan*

Main category: cs.LG

TL;DR: MetaDefense是用于防御大型语言模型微调攻击的新框架，利用两阶段防御机制显著提升了抗有害查询能力。


<details>
  <summary>Details</summary>
Motivation: 现有防御机制无法泛化到由未见攻击模板伪装的有害查询，然而LLMs能够在嵌入空间中区分伪装的有害查询。

Method: 提出两阶段防御方法：生成前防御检测有害查询，以及生成中防御监控部分响应以防止生成更多有害内容。

Result: MetaDefense在多种LLM架构上通过广泛实验展示出显著优于现有防御机制，应对有害查询及隐藏攻击模板的稳健性，同时在良性任务中表现优异。

Conclusion: MetaDefense显著优于现有防御机制，在处理已知与未知攻击模板的有害查询方面实现了稳健防御，同时在良性任务上保持竞争力。

Abstract: This paper introduces MetaDefense, a novel framework for defending against
finetuning-based jailbreak attacks in large language models (LLMs). We observe
that existing defense mechanisms fail to generalize to harmful queries
disguised by unseen attack templates, despite LLMs being capable of
distinguishing disguised harmful queries in the embedding space. Based on these
insights, we propose a two-stage defense approach: (i) pre-generation defense
that detects harmful queries before response generation begins, and (ii)
mid-generation defense that monitors partial responses during generation to
prevent outputting more harmful content. Our MetaDefense trains the LLM to
predict the harmfulness of both queries and partial responses using specialized
prompts, enabling early termination of potentially harmful interactions.
Extensive experiments across multiple LLM architectures (LLaMA-2-7B,
Qwen-2.5-3B-Instruct, and LLaMA-3.2-3B-Instruct) demonstrate that MetaDefense
significantly outperforms existing defense mechanisms, achieving robust defense
against harmful queries with seen and unseen attack templates while maintaining
competitive performance on benign tasks. Code is available at
https://github.com/ws-jiang/MetaDefense.

</details>


### [41] [Signal-to-Noise Ratio in Scanning Electron Microscopy: A Comprehensive Review](https://arxiv.org/abs/2510.07886)
*K. S. Sim,I. Bukhori,D. C. Y. Ong,K. B. Gan*

Main category: cs.LG

TL;DR: 本文综述了扫描电子显微镜（SEM）中信噪比（SNR）的优化方法，包括操作原理、噪声来源、测量与估计方法，以及提升图像质量的硬件与软件技术。


<details>
  <summary>Details</summary>
Motivation: 由于噪声影响了扫描电子显微镜（SEM）图像的质量和可解释性，提高信噪比（SNR）是解决这一问题的关键。因此，需要对SNR的优化进行深入理解和研究。

Method: 本文采用综述的形式，分析和总结了SEM的操作原理、噪声来源、SNR的测量与估计方法，以及硬件和软件方面提高SNR的不同方法。

Result: 本文综合分析了扫描电子显微镜（SEM）中与信噪比（SNR）相关的多个方面，为研究人员与实践者提供了清晰详尽的指导，推动该领域的进一步研究。

Conclusion: 本文综述了几种提高扫描电子显微镜（SEM）的方法，着重分析了信噪比（SNR）的优化，以提高图像质量。

Abstract: Scanning Electron Microscopy (SEM) is critical in nanotechnology, materials
science, and biological imaging due to its high spatial resolution and depth of
focus. Signal-to-noise ratio (SNR) is an essential parameter in SEM because it
directly impacts the quality and interpretability of the images. SEM is widely
used in various scientific disciplines, but its utility can be compromised by
noise, which degrades image clarity. This review explores multiple aspects of
the SEM imaging process, from the principal operation of SEM, sources of noise
in SEM, methods for SNR measurement and estimations, to various aspects that
affect the SNR measurement and approaches to enhance SNR, both from a hardware
and software standpoint. We review traditional and emerging techniques,
focusing on their applications, advantages, and limitations. The paper aims to
provide a comprehensive understanding of SNR optimization in SEM for
researchers and practitioners and to encourage further research in the field.

</details>


### [42] [Adaptive Optimizable Gaussian Process Regression Linear Least Squares Regression Filtering Method for SEM Images](https://arxiv.org/abs/2510.07895)
*D. Chee Yong Ong,I. Bukhori,K. S. Sim,K. Beng Gan*

Main category: cs.LG

TL;DR: 提出了一种新的AO-GPRLLSR滤波管线，通过优化的噪声方差估测和维纳滤波器提高了SEM图像的质量。


<details>
  <summary>Details</summary>
Motivation: SEM图像容易受到噪声污染，影响图像质量和后续分析，需要一种方法来有效估计信噪比和噪声方差并提高图像质量。

Method: 采用线性最小二乘回归（LSR）和优化的高斯过程回归（GPR）进行噪声方差估测，结合SNR估测技术和机器学习模型指导维纳滤波器。

Result: 所提出的AO-GPRLLSR滤波管线成功降低了过滤过程后的均方误差（MSE），并提高了SEM图像的质量。

Conclusion: 提出了一种新的方法来改进SEM图像的信噪比估测和噪声方差估测，并通过NV引导的维纳滤波器提高图像质量。

Abstract: Scanning Electron Microscopy (SEM) images often suffer from noise
contamination, which degrades image quality and affects further analysis. This
research presents a complete approach to estimate their Signal-to-Noise Ratio
(SNR) and noise variance (NV), and enhance image quality using NV-guided Wiener
filter. The main idea of this study is to use a good SNR estimation technique
and infuse a machine learning model to estimate NV of the SEM image, which then
guides the wiener filter to remove the noise, providing a more robust and
accurate SEM image filtering pipeline. First, we investigate five different SNR
estimation techniques, namely Nearest Neighbourhood (NN) method, First-Order
Linear Interpolation (FOL) method, Nearest Neighbourhood with First-Order
Linear Interpolation (NN+FOL) method, Non-Linear Least Squares Regression
(NLLSR) method, and Linear Least Squares Regression (LSR) method. It is shown
that LSR method to perform better than the rest. Then, Support Vector Machines
(SVM) and Gaussian Process Regression (GPR) are tested by pairing it with LSR.
In this test, the Optimizable GPR model shows the highest accuracy and it
stands as the most effective solution for NV estimation. Combining these
results lead to the proposed Adaptive Optimizable Gaussian Process Regression
Linear Least Squares Regression (AO-GPRLLSR) Filtering pipeline. The AO-GPRLLSR
method generated an estimated noise variance which served as input to NV-guided
Wiener filter for improving the quality of SEM images. The proposed method is
shown to achieve notable success in estimating SNR and NV of SEM images and
leads to lower Mean Squared Error (MSE) after the filtering process.

</details>


### [43] [MMM: Quantum-Chemical Molecular Representation Learning for Combinatorial Drug Recommendation](https://arxiv.org/abs/2510.07910)
*Chongmyung Kwon,Yujin Kim,Seoeun Park,Yunji Lee,Charmgil Hong*

Main category: cs.LG

TL;DR: 研究提出了结合三维量子化学信息的MMM框架，提升药物推荐准确性及药物相互作用的预测能力。


<details>
  <summary>Details</summary>
Motivation: 现有药物推荐系统未能充分捕获分子结合亲和力和反应性，尤其在药物间相互作用方面存在局限。

Method: MMM框架利用分子电子定位函数图生成三维电子密度图，并结合双分图编码器实现药物分子表示学习。

Result: 在MIMIC-III数据集上，与GNN-based SafeDrug模型相比，MMM显著提高了F1-score、Jaccard和DDI rate，证明了其预测准确性。

Conclusion: 多模态DDI预测框架MMM，通过结合三维量子化学信息和双分图编码器，提高了药物推荐的准确性和安全性。

Abstract: Drug recommendation is an essential task in machine learning-based clinical
decision support systems. However, the risk of drug-drug interactions (DDI)
between co-prescribed medications remains a significant challenge. Previous
studies have used graph neural networks (GNNs) to represent drug structures.
Regardless, their simplified discrete forms cannot fully capture the molecular
binding affinity and reactivity. Therefore, we propose Multimodal DDI
Prediction with Molecular Electron Localization Function (ELF) Maps (MMM), a
novel framework that integrates three-dimensional (3D) quantum-chemical
information into drug representation learning. It generates 3D electron density
maps using the ELF. To capture both therapeutic relevance and interaction
risks, MMM combines ELF-derived features that encode global electronic
properties with a bipartite graph encoder that models local substructure
interactions. This design enables learning complementary characteristics of
drug molecules. We evaluate MMM in the MIMIC-III dataset (250 drugs, 442
substructures), comparing it with several baseline models. In particular, a
comparison with the GNN-based SafeDrug model demonstrates statistically
significant improvements in the F1-score (p = 0.0387), Jaccard (p = 0.0112),
and the DDI rate (p = 0.0386). These results demonstrate the potential of
ELF-based 3D representations to enhance prediction accuracy and support safer
combinatorial drug prescribing in clinical practice.

</details>


### [44] [GRADE: Personalized Multi-Task Fusion via Group-relative Reinforcement Learning with Adaptive Dirichlet Exploratio](https://arxiv.org/abs/2510.07919)
*Tingfeng Hong,Pingye Ren,Xinlong Xiao,Chao Wang,Chenyi Lei,Wenwu Ou,Han Li*

Main category: cs.LG

TL;DR: 提出了一种个性化多目标排序系统，通过多任务学习和多任务融合提高排序精度。


<details>
  <summary>Details</summary>
Motivation: 为了提高排序系统的个性化和精度，解决多种用户反馈信号的优化问题。

Method: 该系统使用多任务学习模型进行用户反馈信号预测，并通过多任务融合模块计算个性化权重，最终生成混合排序。

Result: 通过使用个性化权重和多任务学习，系统能够更准确地预测用户行为，从而改善用户体验。

Conclusion: 该系统通过融合多任务学习模型的预测结果，以个性化的方式提高排序系统的精度和用户满意度。

Abstract: Overall architecture of the personalized multi-objective ranking system. It
comprises: (1) a Feature Center and Prerank Model for initial feature
processing and candidate generation; (2) a Multi-Task Learning (MTL) model
predicting various user feedback signals; (3) a Multi-Task Fusion (MTF) module
(our proposed GRADE framework) that learns personalized weights ($w_1, \dots,
w_n$); these weights are then applied to calculate final scores and sorted to
generate a blended ranking by the Blended Ranking Model, which ultimately
delivers results to users.

</details>


### [45] [Synergy Between the Strong and the Weak: Spiking Neural Networks are Inherently Self-Distillers](https://arxiv.org/abs/2510.07924)
*Yongqi Ding,Lin Zuo,Mengmeng Jing,Kunshan Yang,Pei He,Tonglan Xie*

Main category: cs.LG

TL;DR: 本文提出利用时间特性进行自蒸馏的方法，有效提升了SNN性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 改进SNN性能，通过自蒸馏方法克服依赖大规模教师模型或额外训练开销的问题。

Method: 将每个时间步的SNN实例视为子模型，评估其输出置信度，并基于强弱关系提出Strong2Weak和Weak2Strong两种自蒸馏方案，提供了集合、同时和级联蒸馏等灵活实现。

Result: 实验显示所提方法能有效提升SNN的辨识能力和整体性能，并增强其对抗鲁棒性。

Conclusion: 本文提出了两种高效自蒸馏方案：Strong2Weak和Weak2Strong，能有效提高SNN的辨识能力和整体性能，并增强其对抗鲁棒性。

Abstract: Brain-inspired spiking neural networks (SNNs) promise to be a low-power
alternative to computationally intensive artificial neural networks (ANNs),
although performance gaps persist. Recent studies have improved the performance
of SNNs through knowledge distillation, but rely on large teacher models or
introduce additional training overhead. In this paper, we show that SNNs can be
naturally deconstructed into multiple submodels for efficient
self-distillation. We treat each timestep instance of the SNN as a submodel and
evaluate its output confidence, thus efficiently identifying the strong and the
weak. Based on this strong and weak relationship, we propose two efficient
self-distillation schemes: (1) \textbf{Strong2Weak}: During training, the
stronger "teacher" guides the weaker "student", effectively improving overall
performance. (2) \textbf{Weak2Strong}: The weak serve as the "teacher",
distilling the strong in reverse with underlying dark knowledge, again yielding
significant performance gains. For both distillation schemes, we offer flexible
implementations such as ensemble, simultaneous, and cascade distillation.
Experiments show that our method effectively improves the discriminability and
overall performance of the SNN, while its adversarial robustness is also
enhanced, benefiting from the stability brought by self-distillation. This
ingeniously exploits the temporal properties of SNNs and provides insight into
how to efficiently train high-performance SNNs.

</details>


### [46] [DISCO: Diversifying Sample Condensation for Efficient Model Evaluation](https://arxiv.org/abs/2510.07959)
*Alexander Rubinstein,Benjamin Raible,Martin Gubri,Seong Joon Oh*

Main category: cs.LG

TL;DR: DISCO方法通过选择模型予测结果分歧最大的样本，在性能预测中表现优越，可降低现代模型评估的成本和复杂度。


<details>
  <summary>Details</summary>
Motivation: 目前的机器学习模型评估成本高昂，影响创新速度和环境，同时复杂的锚点选择可能不必要。DISCO旨在通过优化样本选择简化评估过程。

Method: DISCO方法使用贪婪的样本统计而不是全局聚类来选择预测分歧最大的样本，并利用模型间分歧作为信息理论上最优的贪婪选择规则。

Result: DISCO在MMLU、Hellaswag、Winogrande和ARC等基准测试中达到了性能预测的最先进结果。

Conclusion: DISCO方法通过选择模型预测分歧较大的样本，提供了一种简单而有效的性能评估策略。

Abstract: Evaluating modern machine learning models has become prohibitively expensive.
Benchmarks such as LMMs-Eval and HELM demand thousands of GPU hours per model.
Costly evaluation reduces inclusivity, slows the cycle of innovation, and
worsens environmental impact. The typical approach follows two steps. First,
select an anchor subset of data. Second, train a mapping from the accuracy on
this subset to the final test result. The drawback is that anchor selection
depends on clustering, which can be complex and sensitive to design choices. We
argue that promoting diversity among samples is not essential; what matters is
to select samples that $\textit{maximise diversity in model responses}$. Our
method, $\textbf{Diversifying Sample Condensation (DISCO)}$, selects the top-k
samples with the greatest model disagreements. This uses greedy, sample-wise
statistics rather than global clustering. The approach is conceptually simpler.
From a theoretical view, inter-model disagreement provides an
information-theoretically optimal rule for such greedy selection.
$\textbf{DISCO}$ shows empirical gains over prior methods, achieving
state-of-the-art results in performance prediction across MMLU, Hellaswag,
Winogrande, and ARC. Code is available here:
https://github.com/arubique/disco-public.

</details>


### [47] [PRESCRIBE: Predicting Single-Cell Responses with Bayesian Estimation](https://arxiv.org/abs/2510.07964)
*Jiabei Cheng,Changxi Chi,Jingbo Zhou,Hongyi Xin,Jun Xia*

Main category: cs.LG

TL;DR: 本文提出了一种名为PRESCRIBE的新方法，能够在单细胞基因扰动预测中有效估计不确定性，提升准确性超过3%。


<details>
  <summary>Details</summary>
Motivation: 预测基因扰动效应的可靠性取决于模型不确定性和数据不确定性，本文提出方法来联合测量这两者的影响。

Method: 使用多变量深度证据回归框架来同时测量模型不确定性和数据不确定性。

Result: PRESCRIBE通过估计每个预测的置信度实现了结果过滤，并在实验中比基线方法取得了超过3%的稳定准确性提升。

Conclusion: 本文提出的PRESCRIBE框架能够有效地估计预测的置信度，并与实际准确性高度相关，从而提高预测的可靠性。

Abstract: In single-cell perturbation prediction, a central task is to forecast the
effects of perturbing a gene unseen in the training data. The efficacy of such
predictions depends on two factors: (1) the similarity of the target gene to
those covered in the training data, which informs model (epistemic)
uncertainty, and (2) the quality of the corresponding training data, which
reflects data (aleatoric) uncertainty. Both factors are critical for
determining the reliability of a prediction, particularly as gene perturbation
is an inherently stochastic biochemical process. In this paper, we propose
PRESCRIBE (PREdicting Single-Cell Response wIth Bayesian Estimation), a
multivariate deep evidential regression framework designed to measure both
sources of uncertainty jointly. Our analysis demonstrates that PRESCRIBE
effectively estimates a confidence score for each prediction, which strongly
correlates with its empirical accuracy. This capability enables the filtering
of untrustworthy results, and in our experiments, it achieves steady accuracy
improvements of over 3% compared to comparable baselines.

</details>


### [48] [Unveiling the Power of Multiple Gossip Steps: A Stability-Based Generalization Analysis in Decentralized Training](https://arxiv.org/abs/2510.07980)
*Qinglun Li,Yingqi Liu,Miao Zhang,Xiaochun Cao,Quanjun Yin,Li Shen*

Main category: cs.LG

TL;DR: 多Gossip步骤显著减少分散与集中培训的性能差距，但理论原因及是否能完全消除差距仍是开放问题，而MGS的上限误差及过剩误差上限得到了解析


<details>
  <summary>Details</summary>
Motivation: 解决理论问题

Method: 稳定性分析

Result: MGS减少优化误差、收敛到较好的解决方案；存在与集中训练的误差差距

Conclusion: 实验支持理论发现

Abstract: Decentralized training removes the centralized server, making it a
communication-efficient approach that can significantly improve training
efficiency, but it often suffers from degraded performance compared to
centralized training. Multi-Gossip Steps (MGS) serve as a simple yet effective
bridge between decentralized and centralized training, significantly reducing
experiment performance gaps. However, the theoretical reasons for its
effectiveness and whether this gap can be fully eliminated by MGS remain open
questions. In this paper, we derive upper bounds on the generalization error
and excess error of MGS using stability analysis, systematically answering
these two key questions. 1). Optimization Error Reduction: MGS reduces the
optimization error bound at an exponential rate, thereby exponentially
tightening the generalization error bound and enabling convergence to better
solutions. 2). Gap to Centralization: Even as MGS approaches infinity, a
non-negligible gap in generalization error remains compared to centralized
mini-batch SGD ($\mathcal{O}(T^{\frac{c\beta}{c\beta +1}}/{n m})$ in
centralized and $\mathcal{O}(T^{\frac{2c\beta}{2c\beta +2}}/{n
m^{\frac{1}{2c\beta +2}}})$ in decentralized). Furthermore, we provide the
first unified analysis of how factors like learning rate, data heterogeneity,
node count, per-node sample size, and communication topology impact the
generalization of MGS under non-convex settings without the bounded gradients
assumption, filling a critical theoretical gap in decentralized training.
Finally, promising experiments on CIFAR datasets support our theoretical
findings.

</details>


### [49] [DemandCast: Global hourly electricity demand forecasting](https://arxiv.org/abs/2510.08000)
*Kevin Steijn,Vamsi Priya Goli,Enrico Antonini*

Main category: cs.LG

TL;DR: 使用XGBoost算法预测不同区域的电力需求，结果为能源规划提供支持。


<details>
  <summary>Details</summary>
Motivation: 为了应对全球能源转型的挑战，开发一个可以准确预测电力需求的模型，以帮助能源系统规划者和政策制定者。

Method: 采用XGBoost的梯度提升算法，结合历史电力需求以及天气和社会经济变量，进行预测。

Result: 模型通过整合历史数据与天气及社会经济变量，能够准确预测电力需求，并提供可扩展的需求预测。

Conclusion: 本研究开发的机器学习框架基于XGBoost算法，可准确预测不同地理区域的电力需求，为能源系统规划者与政策制定者提供支持。

Abstract: This paper presents a machine learning framework for electricity demand
forecasting across diverse geographical regions using the gradient boosting
algorithm XGBoost. The model integrates historical electricity demand and
comprehensive weather and socioeconomic variables to predict normalized
electricity demand profiles. To enable robust training and evaluation, we
developed a large-scale dataset spanning multiple years and countries, applying
a temporal data-splitting strategy that ensures benchmarking of out-of-sample
performance. Our approach delivers accurate and scalable demand forecasts,
providing valuable insights for energy system planners and policymakers as they
navigate the challenges of the global energy transition.

</details>


### [50] [Recycling Pretrained Checkpoints: Orthogonal Growth of Mixture-of-Experts for Efficient Large Language Model Pre-Training](https://arxiv.org/abs/2510.08008)
*Ruizhe Wang,Yucheng Ding,Xiao Liu,Yaoxiang Wang,Peng Cheng,Baining Guo,Zhengjun Zha,Yeyun Gong*

Main category: cs.LG

TL;DR: 该研究提出了一种检查点回收方法，能够通过增加预训练模型的参数并继续训练，来高效利用已有的计算成本，从而提高大型语言模型的准确性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的预训练计算成本迅速增加，存在未被充分利用的计算成本，我们希望通过回收预训练检查点来更有效率地使用这些成本。

Method: 我们提出了适合于收敛的专家混合模型的正交增长方法：用于深度增长的跨位置层复制和用于宽度增长的带噪声注入的专家复制。为了确定这种增长的最佳时机，我们进行了全面的缩放实验。

Result: 我们的方法在相同的额外计算预算下，比从头开始训练提高了10.66%的准确性。通过扩展参数并继续训练，可以实现更高效的模型性能。

Conclusion: 我们的检查点回收方法为经济高效的大型语言模型预训练奠定了基础。通过扩展预训练检查点的参数量并继续训练，可以有效利用现有检查点中未被充分利用的计算成本。同时，进行了全面的缩放实验，揭示了最终准确度与沉没成本的强正相关性，表明较大的先前投资会带来更好的性能。我们的方法在相同的计算预算下，比从头开始训练提高了10.66%的准确性。

Abstract: The rapidly increasing computational cost of pretraining Large Language
Models necessitates more efficient approaches. Numerous computational costs
have been invested in existing well-trained checkpoints, but many of them
remain underutilized due to engineering constraints or limited model capacity.
To efficiently reuse this "sunk" cost, we propose to recycle pretrained
checkpoints by expanding their parameter counts and continuing training. We
propose orthogonal growth method well-suited for converged Mixture-of-Experts
model: interpositional layer copying for depth growth and expert duplication
with injected noise for width growth. To determine the optimal timing for such
growth across checkpoints sequences, we perform comprehensive scaling
experiments revealing that the final accuracy has a strong positive correlation
with the amount of sunk cost, indicating that greater prior investment leads to
better performance. We scale our approach to models with 70B parameters and
over 1T training tokens, achieving 10.66% accuracy gain over training from
scratch under the same additional compute budget. Our checkpoint recycling
approach establishes a foundation for economically efficient large language
model pretraining.

</details>


### [51] [DUA-D2C: Dynamic Uncertainty Aware Method for Overfitting Remediation in Deep Learning](https://arxiv.org/abs/2411.15876)
*Md. Saiful Bari Siddiqui,Md Mohaiminul Islam,Md. Golam Rabiul Alam*

Main category: cs.LG

TL;DR: DUA-D2C 是一种改进的训练数据集成技术，通过动态加权子模型贡献，有效解决深度学习中的过拟合问题，并提高了泛化能力。


<details>
  <summary>Details</summary>
Motivation: 深度学习中，由于数据异常、噪声以及训练数据有限，过拟合问题仍然非常突出。为了更好地解决过拟合，我们需要一种能够有效利用数据集信息，同时减少单个数据异常和噪声影响的方法。

Method: 本文提出了一种优化训练数据集成的方法，名为动态不确定性感知 Divide2Conquer (DUA-D2C)。DUA-D2C 基于子模型在共享验证集上的表现，动态加权子模型的贡献。通过考虑准确性和预测不确定性来优化集成过程。

Result: DUA-D2C 在多个领域的基准数据集上进行了实证评估，显示其显著提高了泛化能力。通过评估决策边界、损失曲线和其他性能指标，进一步证明了 DUA-D2C 的有效性。

Conclusion: DUA-D2C 改进了模型集成过程，通过将子模型的贡献动态加权，使中央模型能够更有效地从次级模型中学习，从而有效地解决了深度学习中的过拟合问题。研究结果表明，即使在其他正则化方法的基础上应用，DUA-D2C 也能提高泛化性能，是一种理论上可靠且有效的现代深度学习过拟合对策。

Abstract: Overfitting remains a significant challenge in deep learning, often arising
from data outliers, noise, and limited training data. To address this, the
Divide2Conquer (D2C) method was previously proposed, which partitions training
data into multiple subsets and trains identical models independently on each.
This strategy enables learning more consistent patterns while minimizing the
influence of individual outliers and noise. However, D2C's standard aggregation
typically treats all subset models equally or based on fixed heuristics (like
data size), potentially underutilizing information about their varying
generalization capabilities. Building upon this foundation, we introduce
Dynamic Uncertainty-Aware Divide2Conquer (DUA-D2C), an advanced technique that
refines the aggregation process. DUA-D2C dynamically weights the contributions
of subset models based on their performance on a shared validation set,
considering both accuracy and prediction uncertainty. This intelligent
aggregation allows the central model to preferentially learn from subsets
yielding more generalizable and confident edge models, thereby more effectively
combating overfitting. Empirical evaluations on benchmark datasets spanning
multiple domains demonstrate that DUA-D2C significantly improves
generalization. Our analysis includes evaluations of decision boundaries, loss
curves, and other performance metrics, highlighting the effectiveness of
DUA-D2C. This study demonstrates that DUA-D2C improves generalization
performance even when applied on top of other regularization methods,
establishing it as a theoretically grounded and effective approach to combating
overfitting in modern deep learning. Our codes are publicly available at:
https://github.com/Saiful185/DUA-D2C.

</details>


### [52] [Accelerated Evolving Set Processes for Local PageRank Computation](https://arxiv.org/abs/2510.08010)
*Binbin Huang,Luo Luo,Yanghua Xiao,Deqing Yang,Baojian Zhou*

Main category: cs.LG

TL;DR: 提出了一种新的框架通过嵌套演化集过程加速个性化PageRank的计算，实验验证了其高效性。


<details>
  <summary>Details</summary>
Motivation: 加速个性化PageRank (PPR) 的计算，以解决现有方法的时间复杂度问题。

Method: 在每个过程阶段，采用局部不精确近端点迭代来解决简化线性系统。该框架引导的算法仅需解决\tilde{\mathcal{O}}(1/\sqrt{\alpha})个线性系统，其中\alpha为减震系数。

Result: 在真实世界图上进行的实验证明了方法的有效性，并在早期阶段显示出显著的收敛效果。

Conclusion: 本文提出了一种新的框架，通过嵌套演化集过程加速个性化PageRank (PPR) 的计算，有效解决了现有文献中的开放性猜想，并通过实验验证了其高效性。

Abstract: This work proposes a novel framework based on nested evolving set processes
to accelerate Personalized PageRank (PPR) computation. At each stage of the
process, we employ a localized inexact proximal point iteration to solve a
simplified linear system. We show that the time complexity of such localized
methods is upper bounded by $\min\{\tilde{\mathcal{O}}(R^2/\epsilon^2),
\tilde{\mathcal{O}}(m)\}$ to obtain an $\epsilon$-approximation of the PPR
vector, where $m$ denotes the number of edges in the graph and $R$ is a
constant defined via nested evolving set processes. Furthermore, the algorithms
induced by our framework require solving only
$\tilde{\mathcal{O}}(1/\sqrt{\alpha})$ such linear systems, where $\alpha$ is
the damping factor. When $1/\epsilon^2\ll m$, this implies the existence of an
algorithm that computes an $\ epsilon $-approximation of the PPR vector with an
overall time complexity of $\tilde{\mathcal{O}}\left(R^2 /
(\sqrt{\alpha}\epsilon^2)\right)$, independent of the underlying graph size.
Our result resolves an open conjecture from existing literature. Experimental
results on real-world graphs validate the efficiency of our methods,
demonstrating significant convergence in the early stages.

</details>


### [53] [Backdoor Vectors: a Task Arithmetic View on Backdoor Attacks and Defenses](https://arxiv.org/abs/2510.08016)
*Stanisław Pawlak,Jan Dubiński,Daniel Marczak,Bartłomiej Twardowski*

Main category: cs.LG

TL;DR: 该研究提出了一种通过后门攻击向量理解和增强模型合并中后门攻击抵抗力的方法，以及一种轻量级防御方法。


<details>
  <summary>Details</summary>
Motivation: 由于模型合并技术容易受到后门攻击威胁，因此需要更好地理解和防御这种攻击。

Method: 通过计算后门攻击向量（Backdoor Vector, BV）以及稀疏后门攻击向量（Sparse Backdoor Vector, SBV）的方法来理解和增强后门抵抗力。同时，提出了注入后门向量相减方法（Injection BV Subtraction, IBVS）作为一种假设自由的防御。

Result: 提出的SBV方法在多个攻击合并中表现出更高的后门有效性，而IBVS提供了一种轻量级且通用的防御方式，即使在后门完全未知的情况下依然有效。

Conclusion: 提出了一个用于理解和防御模型合并中的后门攻击的新框架。该框架通过处理攻击向量，揭示后门攻击的核心漏洞并提出有效的防御方法。

Abstract: Model merging (MM) recently emerged as an effective method for combining
large deep learning models. However, it poses significant security risks.
Recent research shows that it is highly susceptible to backdoor attacks, which
introduce a hidden trigger into a single fine-tuned model instance that allows
the adversary to control the output of the final merged model at inference
time. In this work, we propose a simple framework for understanding backdoor
attacks by treating the attack itself as a task vector. $Backdoor\ Vector\
(BV)$ is calculated as the difference between the weights of a fine-tuned
backdoored model and fine-tuned clean model. BVs reveal new insights into
attacks understanding and a more effective framework to measure their
similarity and transferability. Furthermore, we propose a novel method that
enhances backdoor resilience through merging dubbed $Sparse\ Backdoor\ Vector\
(SBV)$ that combines multiple attacks into a single one. We identify the core
vulnerability behind backdoor threats in MM: $inherent\ triggers$ that exploit
adversarial weaknesses in the base model. To counter this, we propose
$Injection\ BV\ Subtraction\ (IBVS)$ - an assumption-free defense against
backdoors in MM. Our results show that SBVs surpass prior attacks and is the
first method to leverage merging to improve backdoor effectiveness. At the same
time, IBVS provides a lightweight, general defense that remains effective even
when the backdoor threat is entirely unknown.

</details>


### [54] [Do We Really Need Permutations? Impact of Width Expansion on Linear Mode Connectivity](https://arxiv.org/abs/2510.08023)
*Akira Ito,Masanori Yamada,Daiki Chijiwa,Atsutoshi Kumagai*

Main category: cs.LG

TL;DR: 通过简化模型（无需参数置换）仅扩宽模型即可实现线性模式连接，提出了一种新的理论解释。


<details>
  <summary>Details</summary>
Motivation: 虽然之前的研究表明参数置换对于实现线性模式连接是必要的，但本研究旨在通过实验验证即使在没有任何置换的情况下，仅通过扩宽模型也能实现线性模式连接。

Method: 本研究通过经验分析展示无需参数置换，仅通过扩宽模型及适当的softmax温度校准即可实现线性模式连接，并引入了层次指数加权连接（LEWC）以分析中间层输出。

Result: 实验结果显示，在实施适当的软缩放温度校准后，仅通过扩宽模型即可实现线性模式连接，并证明了层次指数加权连接能有效地解释这一现象。

Conclusion: 本研究首次证明，通过简化模型结构（即无需参数置换）单纯扩宽模型即可实现线性模式连接（LMC），这挑战了之前关于需要广泛参数置换的观点。

Abstract: Recently, Ainsworth et al. empirically demonstrated that, given two
independently trained models, applying a parameter permutation that preserves
the input-output behavior allows the two models to be connected by a low-loss
linear path. When such a path exists, the models are said to achieve linear
mode connectivity (LMC). Prior studies, including Ainsworth et al., have
reported that achieving LMC requires not only an appropriate permutation search
but also sufficiently wide models (e.g., a 32 $\times$ width multiplier for
ResNet-20). This is broadly believed to be because increasing the model width
ensures a large enough space of candidate permutations, increasing the chance
of finding one that yields LMC. In this work, we empirically demonstrate that,
even without any permutations, simply widening the models is sufficient for
achieving LMC when using a suitable softmax temperature calibration. We further
explain why this phenomenon arises by analyzing intermediate layer outputs.
Specifically, we introduce layerwise exponentially weighted connectivity
(LEWC), which states that the output of each layer of the merged model can be
represented as an exponentially weighted sum of the outputs of the
corresponding layers of the original models. Consequently the merged model's
output matches that of an ensemble of the original models, which facilitates
LMC. To the best of our knowledge, this work is the first to show that widening
the model not only facilitates nonlinear mode connectivity, as suggested in
prior research, but also significantly increases the possibility of achieving
linear mode connectivity.

</details>


### [55] [Approximate Domain Unlearning for Vision-Language Models](https://arxiv.org/abs/2510.08132)
*Kodai Kawamura,Yuta Goto,Rintaro Yanagi,Hirokatsu Kataoka,Go Irie*

Main category: cs.LG

TL;DR: 本文提出了一种新的近似域遗忘方法，通过解开域分布来提高预训练视觉语言模型的效率和安全性。


<details>
  <summary>Details</summary>
Motivation: 预训练的视觉语言模型在下游任务中保留了不相关的信息，影响计算效率并存在潜在的信息泄漏风险。因此，研究者们对选择性移除这些不必要知识而保持模型整体性能的近似遗忘技术产生了浓厚的兴趣。

Method: 本文提出了一种显式解开域分布并自适应捕获实例特定域信息的新方法。

Result: 实验结果表明，本文方法优于基于VLM调整技术的基线方法。

Conclusion: 本文提出了一种新的问题设置--近似域遗忘（ADU），并引入了一种新颖的方法来解决预训练视觉语言模型（VLMs）中的域分布高度纠缠的问题。在实验中，该方法优于基于VLM调整技术的基线方法，并为VLM中的实际和细粒度遗忘铺平了道路。

Abstract: Pre-trained Vision-Language Models (VLMs) exhibit strong generalization
capabilities, enabling them to recognize a wide range of objects across diverse
domains without additional training. However, they often retain irrelevant
information beyond the requirements of specific downstream tasks, raising
concerns about computational efficiency and potential information leakage. This
has motivated growing interest in approximate unlearning, which aims to
selectively remove unnecessary knowledge while preserving overall model
performance. Existing approaches to approximate unlearning have primarily
focused on class unlearning, where a VLM is retrained to fail to recognize
specified object classes while maintaining accuracy for others. However, merely
forgetting object classes is often insufficient in practical applications. For
instance, an autonomous driving system should accurately recognize real cars
while avoiding misrecognition of illustrated cars depicted in roadside
advertisements as real cars, which could be hazardous. In this paper, we
introduce Approximate Domain Unlearning (ADU), a novel problem setting that
requires reducing recognition accuracy for images from specified domains (e.g.,
illustration) while preserving accuracy for other domains (e.g., real). ADU
presents new technical challenges: due to the strong domain generalization
capability of pre-trained VLMs, domain distributions are highly entangled in
the feature space, making naive approaches based on penalizing target domains
ineffective. To tackle this limitation, we propose a novel approach that
explicitly disentangles domain distributions and adaptively captures
instance-specific domain information. Extensive experiments show that our
approach outperforms baselines built upon VLM tuning techniques, paving the way
for practical and fine-grained unlearning in VLMs. Code:
https://kodaikawamura.github.io/Domain_Unlearning/.

</details>


### [56] [Arbitrary Entropy Policy Optimization: Entropy Is Controllable in Reinforcement Finetuning](https://arxiv.org/abs/2510.08141)
*Chen Wang,Zhaochun Li,Jionghao Bai,Yuzhi Zhang,Shisheng Cui,Zhou Zhao,Yue Wang*

Main category: cs.LG

TL;DR: 本文提出AEPO方法解决了GRPO中的熵塌缩问题，成功控制了熵并改善了LLM性能。


<details>
  <summary>Details</summary>
Motivation: 针对现有方法无法有效解决熵塌缩问题而且会引入偏差和不稳定的问题，提出了一种新的优化策略。

Method: 提出了一种称为Arbitrary Entropy Policy Optimization (AEPO)的新方法，通过调整温度的分布进行REINFORCE策略梯度处理并稳定熵来优化政策。

Result: 实验表明AEPO可以在任意目标水平上稳定熵，消除GRPO中的熵塌缩，并揭示了性能与熵之间的非单调关系，同时提供了一个更广泛的强化微调范式。

Conclusion: AEPO解决了GRPO中的熵塌缩问题，并通过精确的熵控制提升了模型的推理能力。实验表明，增加熵可以改善性能，但过多则有害。

Abstract: Reinforcement finetuning (RFT) is essential for enhancing the reasoning
capabilities of large language models (LLM), yet the widely adopted Group
Relative Policy Optimization (GRPO) suffers from entropy collapse, where
entropy monotonically decreases, exploration vanishes, and policies converge
prematurely. Existing entropy-regularized methods only partially alleviate this
issue while introducing bias and instability, leaving entropy control
unresolved and the connection between entropy, exploration, and performance
unclear. We propose Arbitrary Entropy Policy Optimization (AEPO), which
eliminates entropy collapse by replacing entropy bonuses with REINFORCE policy
gradient on temperature-adjusted distributions and stabilizing entropy through
temperature regulation. AEPO integrates three key designs: policy gradient as
regularization, distribution as regularization, and REINFORCE as
regularization, enabling precise entropy control without distorting
optimization. Experiments demonstrate three major contributions: AEPO (1)
stabilizes entropy at arbitrary target levels, effectively removing collapse in
GRPO; (2) reveals a non-monotonic relation where performance first improves
then declines with increasing entropy, clarifying the link between entropy,
exploration, and reasoning; and (3) generalizes beyond entropy, providing a
broader RFT paradigm where superior target distributions can serve as REINFORCE
regularizers.

</details>


### [57] [Think Just Enough: Sequence-Level Entropy as a Confidence Signal for LLM Reasoning](https://arxiv.org/abs/2510.08146)
*Aman Sharma,Paras Chopra*

Main category: cs.LG

TL;DR: 通过熵框架实现语言模型推理任务的计算节省，同时维持精度。


<details>
  <summary>Details</summary>
Motivation: 提升大语言模型在推理任务中的token使用效率和减少计算成本。

Method: 使用香农熵和token级别的logprobs作为信号来调整推理任务的token效率。

Result: 框架在推理优化模型家族中表现稳定，计算成本降低25-50%，准确性与原来持平。

Conclusion: 先进的推理模型能够在早期准确识别出它们的正确答案，利用这种能力可以节省计算资源和降低延迟。

Abstract: We introduce a simple, yet novel entropy-based framework to drive token
efficiency in large language models during reasoning tasks. Our approach uses
Shannon entropy from token-level logprobs as a confidence signal to enable
early stopping, achieving 25-50% computational savings while maintaining task
accuracy. Crucially, we demonstrate that entropy-based confidence calibration
represents an emergent property of advanced post-training optimization present
in modern reasoning models but notably absent in standard instruction-tuned and
pre-trained models (Llama 3.3 70B). We show that the entropy threshold to stop
reasoning varies from model to model but can be calculated easily in one shot
using only a few examples from existing reasoning datasets. Our results
indicate that advanced reasoning models often know that they've gotten a
correct answer early on, and that this emergent confidence awareness can be
exploited to save tokens and reduce latency. The framework demonstrates
consistent performance across reasoning-optimized model families with 25-50%
computational cost reduction while preserving accuracy, revealing that
confidence mechanisms represent a distinguishing characteristic of modern
post-trained reasoning systems versus their predecessors.

</details>


### [58] [Unsupervised Multi-Source Federated Domain Adaptation under Domain Diversity through Group-Wise Discrepancy Minimization](https://arxiv.org/abs/2510.08150)
*Larissa Reichart,Cem Ata Baykara,Ali Burak Ünal,Mete Akgün,Harlin Lee*

Main category: cs.LG

TL;DR: GALA是一个可扩展的UMDA框架，通过独特的目标和策略，总结在高多样性场景中优于现有方法，尤其是在多个异质域中。


<details>
  <summary>Details</summary>
Motivation: 为了解决分布式UMDA中的隐私约束和扩展性问题，同时有效处理众多异质域，作者提出了一个可扩展且鲁棒的联合UMDA框架——GALA。

Method: 提出两种关键组件：一种新的组间差异最小化目标，可以有效地接近完整的成对域对齐，而不需要二次计算；一种温度控制的基于质心的加权策略，根据与目标的对齐动态优先考虑源域。

Result: 在引入的新基准Digit-18进行评估，实验表明GALA在高多样性场景下表现出色。

Conclusion: GALA在标准基准上连续获得竞争性或最先进的结果，在多来源设置中显著优于先前的方法。

Abstract: Unsupervised multi-source domain adaptation (UMDA) aims to learn models that
generalize to an unlabeled target domain by leveraging labeled data from
multiple, diverse source domains. While distributed UMDA methods address
privacy constraints by avoiding raw data sharing, existing approaches typically
assume a small number of sources and fail to scale effectively. Increasing the
number of heterogeneous domains often makes existing methods impractical,
leading to high computational overhead or unstable performance. We propose
GALA, a scalable and robust federated UMDA framework that introduces two key
components: (1) a novel inter-group discrepancy minimization objective that
efficiently approximates full pairwise domain alignment without quadratic
computation; and (2) a temperature-controlled, centroid-based weighting
strategy that dynamically prioritizes source domains based on alignment with
the target. Together, these components enable stable and parallelizable
training across large numbers of heterogeneous sources. To evaluate performance
in high-diversity scenarios, we introduce Digit-18, a new benchmark comprising
18 digit datasets with varied synthetic and real-world domain shifts. Extensive
experiments show that GALA consistently achieves competitive or
state-of-the-art results on standard benchmarks and significantly outperforms
prior methods in diverse multi-source settings where others fail to converge.

</details>


### [59] [Bidirectional Representations Augmented Autoregressive Biological Sequence Generation:Application in De Novo Peptide Sequencing](https://arxiv.org/abs/2510.08169)
*Xiang Zhang,Jiaqi Wei,Zijie Qiu,Sheng Xu,Zhi Jin,ZhiQiang Gao,Nanqing Dong,Siqi Sun*

Main category: cs.LG

TL;DR: 提出了一种通过整合非自回归机制丰富上下文信息来增强自回归生成能力的新框架，显著提升了生物序列建模性能。


<details>
  <summary>Details</summary>
Motivation: 自回归模型由于其单向性质无法捕获全局双向token依赖性，因此在许多生物任务中受限。非自回归模型提供了整体的双向表示，但在生成一致性和可扩展性方面面临挑战。

Method: 该方法结合了共享输入编码器和两个解码器：非自回归解码器学习生物特征的潜在双向性，自回归解码器通过利用这些双向特征来生成生物序列。通过新的跨解码器注意力模块，自回归解码器可以迭代查询并整合双向特征。该模型还采用了重要性退火和跨解码器梯度阻塞的训练策略。

Result: 我们的模型在九种物种的de novo肽测序基准测试中表现出色，显著超过了自回归和非自回归基线，独特地结合了自回归稳定性和非自回归上下文意识，实现了鲁棒性和优越性。

Conclusion: 该研究提出了一种新的混合框架，通过动态整合非自回归机制的丰富上下文信息来增强自回归生成能力。实验结果表明，模型在九种生物序列基准测试中表现优异，超越了现有的自回归和非自回归模型。

Abstract: Autoregressive (AR) models, common in sequence generation, are limited in
many biological tasks such as de novo peptide sequencing and protein modeling
by their unidirectional nature, failing to capture crucial global bidirectional
token dependencies. Non-Autoregressive (NAR) models offer holistic,
bidirectional representations but face challenges with generative coherence and
scalability. To transcend this, we propose a hybrid framework enhancing AR
generation by dynamically integrating rich contextual information from
non-autoregressive mechanisms. Our approach couples a shared input encoder with
two decoders: a non-autoregressive one learning latent bidirectional biological
features, and an AR decoder synthesizing the biological sequence by leveraging
these bidirectional features. A novel cross-decoder attention module enables
the AR decoder to iteratively query and integrate these bidirectional features,
enriching its predictions. This synergy is cultivated via a tailored training
strategy with importance annealing for balanced objectives and cross-decoder
gradient blocking for stable, focused learning. Evaluations on a demanding
nine-species benchmark of de novo peptide sequencing show that our model
substantially surpasses AR and NAR baselines. It uniquely harmonizes AR
stability with NAR contextual awareness, delivering robust, superior
performance on diverse downstream data. This research advances biological
sequence modeling techniques and contributes a novel architectural paradigm for
augmenting AR models with enhanced bidirectional understanding for complex
sequence generation. Code is available at https://github.com/BEAM-Labs/denovo.

</details>


### [60] [Long-tailed Recognition with Model Rebalancing](https://arxiv.org/abs/2510.08177)
*Jiaan Luo,Feng Hong,Qiang Hu,Xiaofeng Cao,Feng Liu,Jiangchao Yao*

Main category: cs.LG

TL;DR: 提出了一种新的模型平衡框架MORE，通过重新平衡模型参数空间来改善长尾识别，尤其是尾部类别的泛化效果。


<details>
  <summary>Details</summary>
Motivation: 长尾识别在深度学习中是普遍且具有挑战性的，因为倾斜的类别分布通常会阻碍模型对尾部类别的泛化。虽然先前的方法在数据增强、损失重平衡和解耦训练等角度上表现出希望，但在诸如多标签长尾识别等广泛情况下取得一致性改进仍然困难。

Method: 研究提出了一种新的模型平衡框架，称为Model Rebalancing (MORE)，该框架通过直接重新平衡模型的参数空间来缓解不平衡。具体来说，MORE引入了一个低秩参数组件，通过量身定制的损失和正弦重加权计划来调节参数空间分配，但不增加整体模型复杂性或推理成本。

Result: MORE显著改善了特别是尾部类别的泛化，并有效补充了现有的不平衡缓解方法。

Conclusion: 通过广泛的实验表明，MORE能够显著改善尤其是尾部类别的泛化效果，并且有效补充现有的不平衡缓解方法。这些结果突显了MORE在长尾设置中作为一个强大的即插即用模块的潜力。

Abstract: Long-tailed recognition is ubiquitous and challenging in deep learning and
even in the downstream finetuning of foundation models, since the skew class
distribution generally prevents the model generalization to the tail classes.
Despite the promise of previous methods from the perspectives of data
augmentation, loss rebalancing and decoupled training etc., consistent
improvement in the broad scenarios like multi-label long-tailed recognition is
difficult. In this study, we dive into the essential model capacity impact
under long-tailed context, and propose a novel framework, Model Rebalancing
(MORE), which mitigates imbalance by directly rebalancing the model's parameter
space. Specifically, MORE introduces a low-rank parameter component to mediate
the parameter space allocation guided by a tailored loss and sinusoidal
reweighting schedule, but without increasing the overall model complexity or
inference costs. Extensive experiments on diverse long-tailed benchmarks,
spanning multi-class and multi-label tasks, demonstrate that MORE significantly
improves generalization, particularly for tail classes, and effectively
complements existing imbalance mitigation methods. These results highlight
MORE's potential as a robust plug-and-play module in long-tailed settings.

</details>


### [61] [FuelCast: Benchmarking Tabular and Temporal Models for Ship Fuel Consumption](https://arxiv.org/abs/2510.08217)
*Justus Viga,Penelope Mueck,Alexander Löser,Torben Weis*

Main category: cs.LG

TL;DR: 研究介绍了新的数据集和标准化基准，并应用TabPFN模型进行燃料消耗预测，结果显示数据驱动的模型具有很强的性能。


<details>
  <summary>Details</summary>
Motivation: 船舶行业中，燃料消耗和排放对经济效率和环境可持续性有重要影响，因此准确预测船舶燃料消耗对于优化海洋运输操作至关重要。现有异构方法和有限的高质量数据集阻碍了不同建模方法的直接比较。

Method: 本研究设计了一个标准化的基准测试，涵盖了表格回归和时间序列回归，并使用了TabPFN基础模型进行船舶燃料消耗建模。此外，研究还涉及了情境学习的应用。

Result: 研究结果表明，所有评估的模型均表现强劲，支持了数据驱动的船上燃料预测的可行性。包含环境条件的模型表现优于仅依赖船速的简单多项式基线，而TabPFN略优于其他技术。纳入时间上下文还可提高准确性。

Conclusion: 本研究表明，利用环境条件的模型在船舶燃料消耗预测中表现优于基础多项式基线模型，同时TabPFN略胜于其他技术。引入的新的数据集和定义的基准为研究提供了有力支持，展示了数据驱动的方法在船上燃料预测中的可行性。

Abstract: In the shipping industry, fuel consumption and emissions are critical factors
due to their significant impact on economic efficiency and environmental
sustainability. Accurate prediction of ship fuel consumption is essential for
further optimization of maritime operations. However, heterogeneous
methodologies and limited high-quality datasets hinder direct comparison of
modeling approaches. This paper makes three key contributions: (1) we introduce
and release a new dataset
(https://huggingface.co/datasets/krohnedigital/FuelCast) comprising operational
and environmental data from three ships; (2) we define a standardized benchmark
covering tabular regression and time-series regression (3) we investigate the
application of in-context learning for ship consumption modeling using the
TabPFN foundation model - a first in this domain to our knowledge. Our results
demonstrate strong performance across all evaluated models, supporting the
feasibility of onboard, data-driven fuel prediction. Models incorporating
environmental conditions consistently outperform simple polynomial baselines
relying solely on vessel speed. TabPFN slightly outperforms other techniques,
highlighting the potential of foundation models with in-context learning
capabilities for tabular prediction. Furthermore, including temporal context
improves accuracy.

</details>


### [62] [Expressive Value Learning for Scalable Offline Reinforcement Learning](https://arxiv.org/abs/2510.08218)
*Nicolas Espinosa-Dice,Kiante Brantley,Wen Sun*

Main category: cs.LG

TL;DR: EVOR是一种可扩展的离线强化学习方法，通过流匹配训练价值函数，推理时通过拒绝采样进行策略提取，表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法依赖时间回溯（BPTT）或策略蒸馏的问题，这些方法限制了离线强化学习的可扩展性。

Method: EVOR通过流匹配训练一个最优、正则化的Q函数。在推理时，EVOR通过拒绝采样与表达性价值函数进行推理时策略提取。

Result: EVOR通过结合表达性策略与表达性价值函数，实现了无需重训练的高效优化、正则化和计算可扩展搜索。

Conclusion: EVOR在离线强化学习任务中表现优于基线方法，证明了将表达性价值学习集成到离线强化学习中的优势。

Abstract: Reinforcement learning (RL) is a powerful paradigm for learning to make
sequences of decisions. However, RL has yet to be fully leveraged in robotics,
principally due to its lack of scalability. Offline RL offers a promising
avenue by training agents on large, diverse datasets, avoiding the costly
real-world interactions of online RL. Scaling offline RL to increasingly
complex datasets requires expressive generative models such as diffusion and
flow matching. However, existing methods typically depend on either
backpropagation through time (BPTT), which is computationally prohibitive, or
policy distillation, which introduces compounding errors and limits scalability
to larger base policies. In this paper, we consider the question of how to
develop a scalable offline RL approach without relying on distillation or
backpropagation through time. We introduce Expressive Value Learning for
Offline Reinforcement Learning (EVOR): a scalable offline RL approach that
integrates both expressive policies and expressive value functions. EVOR learns
an optimal, regularized Q-function via flow matching during training. At
inference-time, EVOR performs inference-time policy extraction via rejection
sampling against the expressive value function, enabling efficient
optimization, regularization, and compute-scalable search without retraining.
Empirically, we show that EVOR outperforms baselines on a diverse set of
offline RL tasks, demonstrating the benefit of integrating expressive value
learning into offline RL.

</details>


### [63] [Post-hoc Stochastic Concept Bottleneck Models](https://arxiv.org/abs/2510.08219)
*Wiktor Jan Hoffmann,Sonia Laguna,Moritz Vandenhirtz,Emanuele Palumbo,Julia E. Vogt*

Main category: cs.LG

TL;DR: PSCBMs提升了CBMs的性能，无需重新训练，适合数据或计算资源受限的情况。


<details>
  <summary>Details</summary>
Motivation: 现有方法在概念之间建模依赖需要重新训练整个模型，在无法访问原始数据或计算能力有限时可能不可行。

Method: 将预训练的CBM与多变量正态分布结合，添加协方差预测模块，无需重新训练主干模型。

Result: 在真实数据中，PSCBMs通过建模概念依赖，在测试时提高概念和目标的准确性，并且在干预下表现更佳。

Conclusion: PSCBMs能够在不重新训练模型的情况下，通过添加较小的协方差预测模块来改进CBMs，其在测试时的性能与标准CBMs相当甚至更优。

Abstract: Concept Bottleneck Models (CBMs) are interpretable models that predict the
target variable through high-level human-understandable concepts, allowing
users to intervene on mispredicted concepts to adjust the final output. While
recent work has shown that modeling dependencies between concepts can improve
CBM performance, especially under interventions, such approaches typically
require retraining the entire model, which may be infeasible when access to the
original data or compute is limited. In this paper, we introduce Post-hoc
Stochastic Concept Bottleneck Models (PSCBMs), a lightweight method that
augments any pre-trained CBM with a multivariate normal distribution over
concepts by adding only a small covariance-prediction module, without
retraining the backbone model. We propose two training strategies and show on
real-world data that PSCBMs consistently match or improve both concept and
target accuracy over standard CBMs at test time. Furthermore, we show that due
to the modeling of concept dependencies, PSCBMs perform much better than CBMs
under interventions, while remaining far more efficient than retraining a
similar stochastic model from scratch.

</details>


### [64] [Reinforcement Learning from Probabilistic Forecasts for Safe Decision-Making via Conditional Value-at-Risk Planning](https://arxiv.org/abs/2510.08226)
*Michal Koren,Or Peretz,Tai Dinh,Philip S. Yu*

Main category: cs.LG

TL;DR: 本文提出了一种不确定性感知马尔科夫决策过程（UAMDP），在高频股票交易和库存控制中展示了相对于深度学习基线的显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 在不稳定、高风险环境中进行序列决策不仅需要最大化期望收益，还需要对不确定性进行原则性管理。这促使研究人员发展一个统一框架来更好地应对这些挑战。

Method: 该论文提出了不确定性感知马尔科夫决策过程（UAMDP），结合了贝叶斯预测、后验采样强化学习和在条件风险价值（CVaR）约束下进行的规划。该方法评估了 UAMDP 在高频率股票交易和零售库存控制两个领域的表现。

Result: 与强大的深度学习基线相比，UAMDP 方法提高了长期预测准确性（RMSE 减少高达 25%，sMAPE 减少 32%），并将这些提升转化为经济表现：交易夏普率从 1.54 上升至 1.74，最大回撤减少约一半。

Conclusion: 整合校准概率建模、与后验不确定性对齐的探索和风险感知控制，能够提供一种更安全、更具盈利性的稳健和通用化的序列决策方法。

Abstract: Sequential decisions in volatile, high-stakes settings require more than
maximizing expected return; they require principled uncertainty management.
This paper presents the Uncertainty-Aware Markov Decision Process (UAMDP), a
unified framework that couples Bayesian forecasting, posterior-sampling
reinforcement learning, and planning under a conditional value-at-risk (CVaR)
constraint. In a closed loop, the agent updates its beliefs over latent
dynamics, samples plausible futures via Thompson sampling, and optimizes
policies subject to preset risk tolerances. We establish regret bounds that
converge to the Bayes-optimal benchmark under standard regularity conditions.
We evaluate UAMDP in two domains-high-frequency equity trading and retail
inventory control-both marked by structural uncertainty and economic
volatility. Relative to strong deep learning baselines, UAMDP improves
long-horizon forecasting accuracy (RMSE decreases by up to 25\% and sMAPE by
32\%), and these gains translate into economic performance: the trading Sharpe
ratio rises from 1.54 to 1.74 while maximum drawdown is roughly halved. These
results show that integrating calibrated probabilistic modeling, exploration
aligned with posterior uncertainty, and risk-aware control yields a robust,
generalizable approach to safer and more profitable sequential decision-making.

</details>


### [65] [Enhancing Reasoning for Diffusion LLMs via Distribution Matching Policy Optimization](https://arxiv.org/abs/2510.08233)
*Yuchen Zhu,Wei Guo,Jaemoo Choi,Petr Molodyk,Bo Yuan,Molei Tao,Yongxin Chen*

Main category: cs.LG

TL;DR: 本文提出了一种名为分布匹配策略优化（DMPO）的强化学习方法，用于提升扩散大型语言模型（dLLMs）的推理能力，在多个基准测试中表现出色，并达到高达42.9%的准确性提高。


<details>
  <summary>Details</summary>
Motivation: 扩散大型语言模型（dLLMs）在推理能力和性能上需要与自回归大型语言模型（AR-LLMs）相媲美，而一个合适的强化学习算法是实现这一目标的关键。

Method: 本文提出的分布匹配策略优化（DMPO）通过交叉熵优化，将dLLM的政策分布与奖励倾斜的最佳分布进行匹配，从而提高dLLMs的推理能力。

Result: DMPO在多个推理基准测试中表现出色，无需监督微调，使准确性提高高达42.9%相对于之前的SOTA基线，超过基础模型55.8%。

Conclusion: 分布匹配强化学习框架能够有效提升dLLMs的推理性能，提出的方法在多个基准测试中表现出优异的准确性提升，无需监督微调。

Abstract: Diffusion large language models (dLLMs) are promising alternatives to
autoregressive large language models (AR-LLMs), as they potentially allow
higher inference throughput. Reinforcement learning (RL) is a crucial component
for dLLMs to achieve comparable performance with AR-LLMs on important tasks,
such as reasoning. However, RL algorithms that are well-suited for dLLMs'
unique characteristics have yet to be developed. This paper proposes
Distribution Matching Policy Optimization (DMPO), a principled and
theoretically grounded RL fine-tuning method specifically designed to enhance
the reasoning capabilities of dLLMs by matching the dLLM policy distribution to
the optimal, reward-tilted one through cross-entropy optimization. We identify
a key challenge in the implementation with a small training batch size and
propose several effective solutions through a novel weight baseline subtraction
technique. DMPO exhibits superior performance on multiple reasoning benchmarks
without supervised fine-tuning, with an accuracy improvement of up to $42.9\%$
over previously SOTA baselines and $55.8\%$ over the base model, underscoring
the effectiveness of the distribution matching framework. Our code is available
at https://github.com/yuchen-zhu-zyc/DMPO.

</details>


### [66] [The Hidden Bias: A Study on Explicit and Implicit Political Stereotypes in Large Language Models](https://arxiv.org/abs/2510.08236)
*Konrad Löhr,Shuzhou Yuan,Michael Färber*

Main category: cs.LG

TL;DR: 研究发现LLM存在左倾政治偏见和多种显性与隐性刻板印象。


<details>
  <summary>Details</summary>
Motivation: 鉴于LLM在信息传播和决策过程中的重要性，理解其潜在偏见尤其是政治领域的偏见，对防止对公众舆论和民主进程的不当影响至关重要。

Method: 使用政治指南针测试（PCT）评估模型的政治倾向和刻板印象传播，包括显性和隐性两部分测试。

Result: 所有调查的模型均表现出一致的左倾政治倾向，刻板印象在语言变化中更加突显。隐性和显性刻板印象呈现出显著的一致性。

Conclusion: 研究揭示了大型语言模型中固有的左倾政治倾向以及模型在隐性和显性刻板印象上的一致性。

Abstract: Large Language Models (LLMs) are increasingly integral to information
dissemination and decision-making processes. Given their growing societal
influence, understanding potential biases, particularly within the political
domain, is crucial to prevent undue influence on public opinion and democratic
processes. This work investigates political bias and stereotype propagation
across eight prominent LLMs using the two-dimensional Political Compass Test
(PCT). Initially, the PCT is employed to assess the inherent political leanings
of these models. Subsequently, persona prompting with the PCT is used to
explore explicit stereotypes across various social dimensions. In a final step,
implicit stereotypes are uncovered by evaluating models with multilingual
versions of the PCT. Key findings reveal a consistent left-leaning political
alignment across all investigated models. Furthermore, while the nature and
extent of stereotypes vary considerably between models, implicit stereotypes
elicited through language variation are more pronounced than those identified
via explicit persona prompting. Interestingly, for most models, implicit and
explicit stereotypes show a notable alignment, suggesting a degree of
transparency or "awareness" regarding their inherent biases. This study
underscores the complex interplay of political bias and stereotypes in LLMs.

</details>


### [67] [Mix- and MoE-DPO: A Variational Inference Approach to Direct Preference Optimization](https://arxiv.org/abs/2510.08256)
*Jason Bohne,Pawel Polak,David Rosenberg,Brian Bloniarz,Gary Kazantsev*

Main category: cs.LG

TL;DR: Mix- and MoE-DPO enhances DPO with mixture models and expert architectures, providing scalable alignment of language models with user preferences.


<details>
  <summary>Details</summary>
Motivation: Existing DPO formulations are limited by using a single model, restricting expressivity in multi-task settings and adaptability to diverse preference distributions. The motivation is to enhance DPO by developing a more flexible and specialized model using mixtures and expert architectures.

Method: The paper proposes a framework that extends Direct Preference Optimization (DPO) using soft mixture models and mixture-of-experts architectures, employing a stochastic variational inference approach to optimize a variational evidence lower bound (ELBO).

Result: The proposed approach, Mix- and MoE-DPO, demonstrates powerful and scalable alignment of large language models with user preferences through validation on various model sizes and multi-preference datasets.

Conclusion: Mix- and MoE-DPO is a powerful and scalable method for preference-based alignment of large language models, offering advantages in generalization, specialization, and contextual alignment.

Abstract: Direct Preference Optimization (DPO) has recently emerged as a simple and
effective alternative to reinforcement learning from human feedback (RLHF) for
aligning large language models (LLMs) with user preferences. However, existing
DPO formulations rely on a single monolithic model, which limits their
expressivity in multi-task settings and their adaptability to heterogeneous or
diverse preference distributions. In this work, we propose Mix- and MoE-DPO, a
framework that extends DPO with both soft mixture models and mixture-of-experts
(MoE) architectures, using a stochastic variational inference approach. Our
method introduces a latent-variable model over expert assignments and optimizes
a variational evidence lower bound (ELBO), enabling stable and efficient
learning of specialized expert policies from preference data. Mix- and MoE-DPO
provides three key advantages over standard DPO: (i) generalization via
universal function approximation through mixtures; (ii) reward and policy
specialization through expert components tailored to distinct preference modes;
and (iii) contextual alignment through input-dependent soft gating that enables
user-specific mixture policies. Our framework supports both shared base
architectures with expert-specific policy heads and fully independent expert
models, allowing flexible trade-offs between parameter efficiency and
specialization. We validate our approach on a variety of model sizes and
multi-preference datasets, demonstrating that Mix- and MoE-DPO offers a
powerful and scalable method for preference-based LLM alignment.

</details>


### [68] [Counterfactual Identifiability via Dynamic Optimal Transport](https://arxiv.org/abs/2510.08294)
*Fabio De Sousa Ribeiro,Ainkaran Santhirasekaram,Ben Glocker*

Main category: cs.LG

TL;DR: 提出了一种能够识别高维多变量结果反事实的方法，并验证了其在各类场景中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有反事实推理方法缺乏识别性，削弱了因果估计的有效性。因此需要建立一个多变量反事实识别的基础。

Method: 使用动态最优传输工具，进行流匹配以获得唯一、单调和保序的反事实传输映射。

Result: 在受控场景中通过反事实真实值验证理论，并展示在真实图像上的公理反事实健全性方面的改进。

Conclusion: 这一研究提供了高维多变量结果的反事实识别方法，为在连续时间流中进行反事实识别奠定了基础，包括非马尔科夫设置下的情况。

Abstract: We address the open question of counterfactual identification for
high-dimensional multivariate outcomes from observational data. Pearl (2000)
argues that counterfactuals must be identifiable (i.e., recoverable from the
observed data distribution) to justify causal claims. A recent line of work on
counterfactual inference shows promising results but lacks identification,
undermining the causal validity of its estimates. To address this, we establish
a foundation for multivariate counterfactual identification using
continuous-time flows, including non-Markovian settings under standard
criteria. We characterise the conditions under which flow matching yields a
unique, monotone and rank-preserving counterfactual transport map with tools
from dynamic optimal transport, ensuring consistent inference. Building on
this, we validate the theory in controlled scenarios with counterfactual
ground-truth and demonstrate improvements in axiomatic counterfactual soundness
on real images.

</details>


### [69] [Bridging the Physics-Data Gap with FNO-Guided Conditional Flow Matching: Designing Inductive Bias through Hierarchical Physical Constraints](https://arxiv.org/abs/2510.08295)
*Tsuyoshi Okita*

Main category: cs.LG

TL;DR: 提出了一种物理感知的生成框架，提高了时间序列生成的质量和准确性，并减少了物理违约。


<details>
  <summary>Details</summary>
Motivation: 传统的时间序列生成忽略领域特定的物理约束，限制了统计和物理一致性。

Method: 结合傅里叶神经算子（FNO）和条件流匹配（CFM），通过时间依赖的层次约束和FNO引导的修正来集成。

Result: 实验结果显示在谐波振荡器、人类活动识别和锂离子电池退化方面达到生成质量提高16.3%，物理违约减少46%，预测准确性提高18.5%。

Conclusion: 我们提出了一种嵌入物理定律层次结构的生成框架，在时间序列生成中引入物理感知的归纳偏差，提高了生成质量和预测准确性，减少了物理违约。

Abstract: Conventional time-series generation often ignores domain-specific physical
constraints, limiting statistical and physical consistency. We propose a
hierarchical framework that embeds the inherent hierarchy of physical
laws-conservation, dynamics, boundary, and empirical relations-directly into
deep generative models, introducing a new paradigm of physics-informed
inductive bias. Our method combines Fourier Neural Operators (FNOs) for
learning physical operators with Conditional Flow Matching (CFM) for
probabilistic generation, integrated via time-dependent hierarchical
constraints and FNO-guided corrections. Experiments on harmonic oscillators,
human activity recognition, and lithium-ion battery degradation show 16.3%
higher generation quality, 46% fewer physics violations, and 18.5% improved
predictive accuracy over baselines.

</details>


### [70] [Robust and Efficient Collaborative Learning](https://arxiv.org/abs/2510.08311)
*Abdellah El Mrini,Sadegh Farhadkhan,Rachid Guerraoui*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Collaborative machine learning is challenged by training-time adversarial
behaviors. Existing approaches to tolerate such behaviors either rely on a
central server or induce high communication costs. We propose Robust Pull-based
Epidemic Learning (RPEL), a novel, scalable collaborative approach to ensure
robust learning despite adversaries. RPEL does not rely on any central server
and, unlike traditional methods, where communication costs grow in
$\mathcal{O}(n^2)$ with the number of nodes $n$, RPEL employs a pull-based
epidemic-based communication strategy that scales in $\mathcal{O}(n \log n)$.
By pulling model parameters from small random subsets of nodes, RPEL
significantly lowers the number of required messages without compromising
convergence guarantees, which hold with high probability. Empirical results
demonstrate that RPEL maintains robustness in adversarial settings, competes
with all-to-all communication accuracy, and scales efficiently across large
networks.

</details>


### [71] [Learning What's Missing: Attention Dispersion and EMA Stabilization in Length Generalization](https://arxiv.org/abs/2510.08341)
*Pál Zsámboki,Benjamin Levi,David Ansel Josef Smith,Mitansh Kagalwala,Arlington Kell,Samuel Liechty,Cong Wang*

Main category: cs.LG

TL;DR: 研究变压器在不同长度序列上的泛化能力，证明了弃权和EMA可以提高变压器的长度泛化性能。


<details>
  <summary>Details</summary>
Motivation: 探究变压器在处理不同长度序列时的泛化能力，特别是对于棋盘游戏风格的推理任务。

Method: 通过理论证明和随机超参数搜索验证，在单层注意力变压器中使用弃权和EMA提高序列长度泛化能力。

Result: 验证了弃权和EMA对提高序列长度泛化能力的有效性，并在OthelloGPT模型中进一步确认了EMA的效果。

Conclusion: 我们通过理论分析和实验验证，发现在变压器模型中应用弃权和指数移动平均（EMA）可以对抗序列长度增长带来的精度下降问题。

Abstract: We study length generalization in transformers through the set complement
task, where a model must predict a uniform distribution over tokens absent from
an input sequence -- an ability central to board-game style reasoning. Our main
theoretical result establishes two statements. First, we prove tight bounds on
embedding and value dimensions for single-layer attention-only transformers.
Second, we show that if such a model achieves balanced logit displacement at
lengths 1 and 2, then it must generalize to longer sequences, though with
reduced precision. A mechanistic reading of the proof explains this limitation:
as more tokens are attended to, softmax compresses logit displacements, eroding
separation between valid and invalid outputs. Training dynamics also suggest a
second obstacle: when many next tokens are possible, updates become noisy. We
hypothesize that dropout can counteract the first effect and Exponential Moving
Average (EMA) the second. We validate these hypotheses through random
hyperparameter search on the set complement task, which confirms both
mechanisms. We then test OthelloGPT, a GPT-1 style model trained on random
Othello moves, and find that EMA again improves length generalization in this
more complex setting.

</details>


### [72] [Guided Star-Shaped Masked Diffusion](https://arxiv.org/abs/2510.08369)
*Viacheslav Meshchaninov,Egor Shibaev,Artem Makoian,Ivan Klimov,Danil Sheshenya,Andrei Malinin,Nikita Balagansky,Daniil Gavrilov,Aibek Alanov,Dmitry Vetrov*

Main category: cs.LG

TL;DR: 提出了一种增强预训练掩码扩散模型采样质量和效率的新算法，通过星状范式改革生成过程，并结合再掩码调度器进行错误修正。


<details>
  <summary>Details</summary>
Motivation: 预训练的掩码扩散模型的性能受到其采样过程的限制，导致决策不可逆且在低步生成状态下困难。

Method: 引入了一种新的采样算法，通过轻量级微调单层模型，用星状范式改革生成过程，并结合可学习的再掩码调度器进行错误修正。

Result: 提出的方法在采样步骤较少时显著提高了样本质量和效率，并在不同场景中展示了可用性。

Conclusion: 提出的采样算法在文本和代码生成实验中表现优异，优于或与现有方法匹敌。

Abstract: The performance of pre-trained masked diffusion models is often constrained
by their sampling procedure, which makes decisions irreversible and struggles
in low-step generation regimes. We introduce a novel sampling algorithm that
works with pre-trained models and, after a lightweight fine-tuning of a single
layer, significantly improves sample quality and efficiency. Our method
reformulates the generation process using a star-shaped paradigm, which
inherently allows for error correction. To make this process effective, we
augment it with a learnable re-masking scheduler that intelligently identifies
and revises likely errors. This approach yields a substantial quality boost,
particularly when using a small number of sampling steps. We extensively ablate
key components of our approach and show its usability in different scenarios.
In comprehensive experiments on text, and code generation, our sampling
algorithm outperforms or matches existing methods.

</details>


### [73] [Contrastive Self-Supervised Learning at the Edge: An Energy Perspective](https://arxiv.org/abs/2510.08374)
*Fernanda Famá,Roberto Pereira,Charalampos Kalalas,Paolo Dini,Lorena Qendro,Fahim Kawsar,Mohammad Malekzadeh*

Main category: cs.LG

TL;DR: 研究对四个对比学习框架进行了评估，发现SimCLR能耗最低，并探讨了轻量级架构和对比学习在资源有限设备上的部署。


<details>
  <summary>Details</summary>
Motivation: 尽管对比学习在自监督表示学习中显示出相当大的前景，但其在资源受限设备上的部署仍未得到广泛探讨。传统对比学习框架训练所需的巨大计算需求带来了许多挑战，尤其是在能耗、数据可用性和内存使用方面。

Method: 研究评估了四个广泛使用的对比学习框架：SimCLR、MoCo、SimSiam和Barlow Twins，并引入了包含能量配置文件和减少训练数据条件的系统基准测试策略。

Result: 研究发现SimCLR在多个数据环境中具有最低的能耗，并对轻量级神经架构与对比学习框架结合时的表现进行了评估。

Conclusion: 研究结果表明，尽管SimCLR通常被认为具有较高的计算成本，但在各种数据情况下其能耗最低。研究同时延伸至评估与CL框架结合时的轻量级神经架构，旨在为在具有限处理能力的边缘/雾环境中部署CL提供资源影响方面的见解，并为其未来优化开辟多个研究方向。

Abstract: While contrastive learning (CL) shows considerable promise in self-supervised
representation learning, its deployment on resource-constrained devices remains
largely underexplored. The substantial computational demands required for
training conventional CL frameworks pose a set of challenges, particularly in
terms of energy consumption, data availability, and memory usage. We conduct an
evaluation of four widely used CL frameworks: SimCLR, MoCo, SimSiam, and Barlow
Twins. We focus on the practical feasibility of these CL frameworks for edge
and fog deployment, and introduce a systematic benchmarking strategy that
includes energy profiling and reduced training data conditions. Our findings
reveal that SimCLR, contrary to its perceived computational cost, demonstrates
the lowest energy consumption across various data regimes. Finally, we also
extend our analysis by evaluating lightweight neural architectures when paired
with CL frameworks. Our study aims to provide insights into the resource
implications of deploying CL in edge/fog environments with limited processing
capabilities and opens several research directions for its future optimization.

</details>


### [74] [Characterizing the Multiclass Learnability of Forgiving 0-1 Loss Functions](https://arxiv.org/abs/2510.08382)
*Jacob Trauger,Tyson Trauger,Ambuj Tewari*

Main category: cs.LG

TL;DR: 本文构建了一种新的组合维度，用于表征宽容0-1损失函数的可学习性，并证明了假设类在此维度有限时是可学习的。


<details>
  <summary>Details</summary>
Motivation: 为了表征在有限标签多类设置中的宽容0-1损失函数的可学习性。

Method: 通过构建基于Natarajan维度的新组合维度来表征可学习性，实现了我们的方法。

Result: 证明了假设类在广义Natarajan维度有限时是可学习的，并展示了与集合值反馈学习的联系。

Conclusion: 我们证明了在有限标签多类设置中，一个假设类是可学习的，当且仅当广义Natarajan维度是有限的。

Abstract: In this paper we will give a characterization of the learnability of
forgiving 0-1 loss functions in the finite label multiclass setting. To do
this, we create a new combinatorial dimension that is based off of the
Natarajan Dimension \citep{natarajan1989learning} and we show that a hypothesis
class is learnable in our setting if and only if this Generalized Natarajan
Dimension is finite. We also show a connection to learning with set-valued
feedback. Through our results we show that the learnability of a set learning
problem is characterized by the Natarajan Dimension.

</details>


### [75] [FlyLoRA: Boosting Task Decoupling and Parameter Efficiency via Implicit Rank-Wise Mixture-of-Experts](https://arxiv.org/abs/2510.08396)
*Heming Zou,Yunliang Zang,Wutong Xu,Yao Zhu,Xiangyang Ji*

Main category: cs.LG

TL;DR: FlyLoRA是一种受蝇类嗅觉回路启发的LoRA变体，通过隐式路由器和稀疏随机投影实现性能提升，解决了任务间和任务内干扰的问题。


<details>
  <summary>Details</summary>
Motivation: LoRA在基础模型微调中由于参数干扰导致性能不佳，现有的以混合专家（MoE）为基础的LoRA变体虽在单任务指令微调中有效，但在多任务模型合并中依旧面临干扰。受到蝇类嗅觉回路的启发，提出FlyLoRA以解决这些问题。

Method: 提出FlyLoRA，使用隐式MoE，包含（1）上投影矩阵中按秩专家激活，（2）一个统一专家路由和下投影的隐式路由器，使用冻结的稀疏随机投影矩阵代替传统的密集可训练版本。

Result: FlyLoRA通过消除显式路由器，同时由于随机矩阵的正交性质内在地减轻了任务间干扰，从而解决了任务间的去相关和计算效率之间的权衡。

Conclusion: FlyLoRA在多个领域的实验中显示出稳定的性能提升，证明其设计的有效性，不仅在经验上有收益，还展示了生物结构如何激发AI技术的创新。

Abstract: Low-Rank Adaptation (LoRA) is a widely used parameter-efficient fine-tuning
method for foundation models, but it suffers from parameter interference,
resulting in suboptimal performance. Although Mixture-of-Experts (MoE)-based
LoRA variants show promise in mitigating intra-task correlations in single-task
instruction tuning, they introduce additional router parameters and remain
ineffective in multi-task model merging where inter-task interference arises.
Inspired by the fly olfactory circuit, we propose FlyLoRA, an implicit
MoE-based LoRA variant that introduces: (1) rank-wise expert activation in the
up-projection matrix, and (2) an implicit router that unifies expert routing
and down-projection, where a frozen sparse random projection matrix replaces
the traditional dense trainable version. This design resolves the trade-off
between intra-task decorrelation and computational efficiency by eliminating
the need for an explicit router, while inherently mitigating inter-task
interference due to the orthogonality property of random matrices. Extensive
experiments across four domains -- general knowledge understanding, scientific
question answering, mathematical reasoning, and code generation -- demonstrate
consistent performance improvements over existing methods. Beyond empirical
gains, FlyLoRA highlights how biological structures can inspire innovations in
AI technologies. Code is available at https://github.com/gfyddha/FlyLoRA.

</details>


### [76] [Prompts Generalize with Low Data: Non-vacuous Generalization Bounds for Optimizing Prompts with More Informative Priors](https://arxiv.org/abs/2510.08413)
*David Madras,Joshua Safyan,Qiuyi,Zhang*

Main category: cs.LG

TL;DR: 本文探讨了困惑度正则化在提示优化中的作用，推导了非空洞性生成化界限，解释了提示工程的成功。


<details>
  <summary>Details</summary>
Motivation: 现有研究无法完全解释在数据稀缺情况下提示工程的成功，本文提出困惑度作为有效先验，可以帮助更有效地进行提示优化和生成化。

Method: 应用PAC-Bayes理论推导离散提示空间，并通过困惑度正则化进行更深层次的分析以获得更紧的生成化界限。

Result: 通过设置更有用的先验，数据稀缺情况下困惑度正则化有效地限制了探索空间，提升了提示优化的生成化效果。

Conclusion: 本文通过推导数据稀缺情况下的生成化界限，证明了困惑度作为有效先验在提示优化中的作用，从而更好地解释了提示工程技术的成功。

Abstract: Many prompt engineering techniques have been successful in practice, even
when optimizing over a large prompt space with with a small amount of
task-specific data. Recent work has partially explained this success by showing
generalization bounds which apply PAC-Bayes theory to the discrete prompt
space, but they are non-vacuous only in data-rich scenarios. We argue that such
widespread success can be more fully explained through more carefully
considering data- or distribution-dependent perplexity, which acts as an
effective prior and steers the optimization towards prompts that are more
``natural'' for the task at hand. We derive novel generalization bounds that
are non-vacuous for data-scarce prompt optimization via more useful priors,
formally analyzing how perplexity regularization tightens these bounds by
limiting exploration. Empirically, we explore both the bounds' effectiveness
and the practical benefits of perplexity regularization in improving prompt
generalization.

</details>


### [77] [Reinforcing Diffusion Models by Direct Group Preference Optimization](https://arxiv.org/abs/2510.08425)
*Yihong Luo,Tianyang Hu,Jing Tang*

Main category: cs.LG

TL;DR: 提出了一种新的算法DGPO，解决了GRPO在扩散模型中的低效问题，训练速度更快，性能更佳。


<details>
  <summary>Details</summary>
Motivation: 解决现有的GRPO方法在适应扩散模型时，由于需要随机策略导致的训练效率低下问题。

Method: 提出了一种新的在线RL算法——DGPO，通过组级偏好直接学习，相比传统的策略梯度框架，实现了更高效的训练。

Result: DGPO训练速度比现有最先进的方法快约20倍，并在领域内和领域外的奖励指标上表现优异。

Conclusion: DGPO算法能够在不需要随机策略的情况下使用确定性ODE采样器进行训练，从而显著加快了训练速度，并在多种度量上实现了优于现有方法的性能。

Abstract: While reinforcement learning methods such as Group Relative Preference
Optimization (GRPO) have significantly enhanced Large Language Models, adapting
them to diffusion models remains challenging. In particular, GRPO demands a
stochastic policy, yet the most cost-effective diffusion samplers are based on
deterministic ODEs. Recent work addresses this issue by using inefficient
SDE-based samplers to induce stochasticity, but this reliance on model-agnostic
Gaussian noise leads to slow convergence. To resolve this conflict, we propose
Direct Group Preference Optimization (DGPO), a new online RL algorithm that
dispenses with the policy-gradient framework entirely. DGPO learns directly
from group-level preferences, which utilize relative information of samples
within groups. This design eliminates the need for inefficient stochastic
policies, unlocking the use of efficient deterministic ODE samplers and faster
training. Extensive results show that DGPO trains around 20 times faster than
existing state-of-the-art methods and achieves superior performance on both
in-domain and out-of-domain reward metrics. Code is available at
https://github.com/Luo-Yihong/DGPO.

</details>


### [78] [xRouter: Training Cost-Aware LLMs Orchestration System via Reinforcement Learning](https://arxiv.org/abs/2510.08439)
*Cheng Qian,Zuxin Liu,Shirley Kokane,Akshara Prabhakar,Jielin Qiu,Haolin Chen,Zhiwei Liu,Heng Ji,Weiran Yao,Shelby Heinecke,Silvio Savarese,Caiming Xiong,Huan Wang*

Main category: cs.LG

TL;DR: xRouter通过学习型路由器优化现代LLM的成本与性能，通过强化学习实现无须手工工程的路由系统，在各种基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 希望通过学术研究创建一种工具调用的路由系统，解决当前高成本与性能间的矛盾，针对不同任务类型优化模型使用。

Method: xRouter通过一套端到端的强化学习训练，其中包含一个明确的成本感知奖励系统，旨在消除人为工程路由规则的需要。

Result: xRouter实现了显著的成本性能权衡，例如在维持相同任务完成率的前提下大幅降低成本。研究提供了关于学习型路由的可靠帮助和不利因素的实证见解。

Conclusion: xRouter在各种基准测试中实现了强大的成本性能权衡，并提供了有关学习路由可靠性和局限性的实证见解，希望这些发现和开放的实现能够促进学习型、成本感知的LLM编排技术的发展。

Abstract: Modern LLM deployments confront a widening cost-performance spectrum: premium
models deliver strong reasoning but are expensive, while lightweight models are
economical yet brittle on complex tasks. Static escalation rules and keyword
heuristics under-utilize this spectrum and fail to adapt across task types. We
present xRouter, a tool-calling-based routing system in which a learned router
can either answer directly or invoke one or more external models. The router is
trained end-to-end with reinforcement learning using an explicit, cost-aware
reward that encodes cost-performance trade-offs, eliminating the need for
hand-engineered routing rules. Our implementation encompasses the full
reinforcement learning framework, including reward and cost accounting, as well
as the deployment and evaluation pipelines. Across diverse benchmarks, xRouter
achieves strong cost-performance trade-offs (e.g., substantial cost reductions
at comparable task completion rates), and provides empirical insights into what
reliably helps learned routing and what does not, ranging from model
trainability to the difficulty of eliciting sophisticated orchestration
behaviors in small open models. We hope these findings and our open
implementation will serve as a practical substrate for advancing learned,
cost-aware LLM orchestration.

</details>


### [79] [Synthetic Series-Symbol Data Generation for Time Series Foundation Models](https://arxiv.org/abs/2510.08445)
*Wenxuan Wang,Kai Wu,Yujian Betterest Li,Dan Wang,Xiaoyu Zhang*

Main category: cs.LG

TL;DR: SymTime模型结合符号信息增强时间序列分析，解决了数据稀缺问题，表现优异。


<details>
  <summary>Details</summary>
Motivation: 由于时间序列分析基础模型面对训练数据稀缺和不平衡等问题，受复杂动态系统理论启发，提出一种数据生成机制来解决这些挑战。

Method: 设计了一种系列-符号数据生成机制，可以不受限制地生成高质量的时间序列数据及相应的符号表达，同时开发了SymTime模型，通过符号信息增强时间序列表示。

Result: SymTime模型在微调后可以在五个主要时间序列分析任务中表现出竞争力，与基于真实数据集预训练的基础模型相媲美。

Conclusion: 系列-符号数据生成和预训练机制在克服数据稀缺性方面具有潜力，并能提升任务表现。

Abstract: Foundation models for time series analysis (TSA) have attracted significant
attention. However, challenges such as training data scarcity and imbalance
continue to hinder their development. Inspired by complex dynamic system
theories, we design a series-symbol data generation mechanism, enabling the
unrestricted creation of high-quality time series data paired with
corresponding symbolic expressions. To leverage series-symbol data pairs with
strong correlations, we develop \texttt{SymTime}, a pre-trained foundation
model for enhancing time series representation using symbolic information.
\texttt{SymTime} demonstrates competitive performance across five major TSA
tasks when fine-tunes with downstream tasks, rivaling foundation models
pre-trained on real-world datasets. This approach underscores the potential of
series-symbol data generation and pretraining mechanisms in overcoming data
scarcity and enhancing task performance. The code is available at
https://github.com/wwhenxuan/SymTime.

</details>


### [80] [gLSTM: Mitigating Over-Squashing by Increasing Storage Capacity](https://arxiv.org/abs/2510.08450)
*Hugh Blayney,Álvaro Arroyo,Xiaowen Dong,Michael M. Bronstein*

Main category: cs.LG

TL;DR: 本文提出了一种新型GNN架构，解决了过度压缩问题，提升了模型的信息存储和检索能力，并在多个测试中取得了优异成绩。


<details>
  <summary>Details</summary>
Motivation: GNN模型在处理信息时面临过度压缩的问题，导致信息瓶颈。为了提高节点表示的信息容量，重新审视现有任务的局限性并引入容量的视角。

Method: 将模型存储和检索容量的概念引入GNN设计，借鉴序列建模领域的关联记忆、快速权重编程和xLSTM模型思想，开发出一种新的GNN架构。

Result: 新架构在容量合成任务和许多真实世界图基准上表现出色，说明通过改进节点信息存储和检索容量能够有效缓解过度压缩问题。

Conclusion: 提出了一种新的GNN架构，通过改进节点表示的信息存储和检索能力，缓解了过度压缩问题，并在新的容量合成任务以及多个真实世界图基准上展示了其优异性能。

Abstract: Graph Neural Networks (GNNs) leverage the graph structure to transmit
information between nodes, typically through the message-passing mechanism.
While these models have found a wide variety of applications, they are known to
suffer from over-squashing, where information from a large receptive field of
node representations is collapsed into a single fixed sized vector, resulting
in an information bottleneck. In this paper, we re-examine the over-squashing
phenomenon through the lens of model storage and retrieval capacity, which we
define as the amount of information that can be stored in a node's
representation for later use. We study some of the limitations of existing
tasks used to measure over-squashing and introduce a new synthetic task to
demonstrate that an information bottleneck can saturate this capacity.
Furthermore, we adapt ideas from the sequence modeling literature on
associative memories, fast weight programmers, and the xLSTM model to develop a
novel GNN architecture with improved capacity. We demonstrate strong
performance of this architecture both on our capacity synthetic task, as well
as a range of real-world graph benchmarks.

</details>


### [81] [SummDiff: Generative Modeling of Video Summarization with Diffusion](https://arxiv.org/abs/2510.08458)
*Kwanseok Kim,Jaehoon Hahm,Sumin Kim,Jinhwan Sul,Byunghak Kim,Joonseok Lee*

Main category: cs.LG

TL;DR: 提出了一种新的视频摘要生成方法SummDiff，利用扩散模型生成符合不同人类视角的摘要，获得了先进的性能并引入了新的评估指标。


<details>
  <summary>Details</summary>
Motivation: 之前的方法忽略了视频摘要生成中的主观性，通过平均帧分值来进行回归。提出了一种新的问题框架，将视频摘要生成视为条件生成任务，让模型学习良好的摘要分布，更好地反映人类观点的多样性。

Method: 采用扩散模型，设计SummDiff方法，根据输入视频动态调整生成多候选摘要，并通过对背包问题的分析引入了新的评估指标。

Result: SummDiff在各种基准测试上实现了最先进的性能，并提供了关于背包问题的深入见解和新的评估指标。

Conclusion: SummDiff取得了最先进的性能，生成的摘要与个人标注者的偏好非常接近。

Abstract: Video summarization is a task of shortening a video by choosing a subset of
frames while preserving its essential moments. Despite the innate subjectivity
of the task, previous works have deterministically regressed to an averaged
frame score over multiple raters, ignoring the inherent subjectivity of what
constitutes a good summary. We propose a novel problem formulation by framing
video summarization as a conditional generation task, allowing a model to learn
the distribution of good summaries and to generate multiple plausible summaries
that better reflect varying human perspectives. Adopting diffusion models for
the first time in video summarization, our proposed method, SummDiff,
dynamically adapts to visual contexts and generates multiple candidate
summaries conditioned on the input video. Extensive experiments demonstrate
that SummDiff not only achieves the state-of-the-art performance on various
benchmarks but also produces summaries that closely align with individual
annotator preferences. Moreover, we provide a deeper insight with novel metrics
from an analysis of the knapsack, which is an important last step of generating
summaries but has been overlooked in evaluation.

</details>


### [82] [In-Context Clustering with Large Language Models](https://arxiv.org/abs/2510.08466)
*Ying Wang,Mengye Ren,Andrew Gordon Wilson*

Main category: cs.LG

TL;DR: 提出了一种新的基于大型语言模型的灵活情境聚类方法（ICC），能够捕捉输入数据之间的复杂关系，实现无监督聚类，表现出优秀的零样本聚类能力以及对数值和图像数据的聚类能力的提升。


<details>
  <summary>Details</summary>
Motivation: 传统聚类算法受限于预定义的相似度度量，不够灵活，因此需要一种能够灵活捕捉复杂关系的聚类方法。情境聚类（ICC）利用大型语言模型（LLM）的注意力机制克服了这些限制，从而可以扩展情境学习至无监督的环境中。

Method: 本文的方法涉及情境聚类（ICC），依赖于大型语言模型（LLM），通过注意力机制灵活地捕捉输入数据之间的复杂关系。在实验中使用了谱聚类和细化训练，并引入了下一令牌预测（NTP）损失以增强LLM对数值和图像数据的聚类能力。

Result: 实验结果显示，预训练的大型语言模型在文本编码的数值数据上表现出零样本聚类能力，注意力矩阵显示出明显的聚类模式，并且使用注意力矩阵进行谱聚类具有竞争力的表现。此外，通过下一令牌预测（NTP）损失的细化训练，使LLMs能够提升对数值和图像数据的聚类能力，同时实现了文本条件下的图像聚类，这在传统方法中无法实现。

Conclusion: 本研究提出了一种新的基于大型语言模型（LLM）的聚类方法——情境聚类（ICC）。通过这种方法，LLM能够在不依赖预定义相似度度量的情况下灵活地捕捉输入之间复杂的关系，并显示出在文本编码的数值数据上零样本聚类的能力。实验结果表明，利用注意力矩阵进行谱聚类具有竞争力的表现，并且通过细化训练提高了LLM对数值和图像数据的聚类能力，从而扩展了情境学习至无监督的环境中。

Abstract: We propose In-Context Clustering (ICC), a flexible LLM-based procedure for
clustering data from diverse distributions. Unlike traditional clustering
algorithms constrained by predefined similarity measures, ICC flexibly captures
complex relationships among inputs through an attention mechanism. We show that
pretrained LLMs exhibit impressive zero-shot clustering capabilities on
text-encoded numeric data, with attention matrices showing salient cluster
patterns. Spectral clustering using attention matrices offers surprisingly
competitive performance. We further enhance the clustering capabilities of LLMs
on numeric and image data through fine-tuning using the Next Token Prediction
(NTP) loss. Moreover, the flexibility of LLM prompting enables text-conditioned
image clustering, a capability that classical clustering methods lack. Our work
extends in-context learning to an unsupervised setting, showcasing the
effectiveness and flexibility of LLMs for clustering. Our code is available at
https://agenticlearning.ai/icc.

</details>


### [83] [Better Together: Leveraging Unpaired Multimodal Data for Stronger Unimodal Models](https://arxiv.org/abs/2510.08492)
*Sharut Gupta,Shobhita Sundaram,Chenyu Wang,Stefanie Jegelka,Phillip Isola*

Main category: cs.LG

TL;DR: 无需依赖成对数据，通过无成对的辅助多模态数据增强表征学习，并在下游任务中验证了这一方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 探讨是否可以直接利用辅助不成对的多模态数据来增强目标模态的表征学习。

Method: 提出了一种无成对多模态学习者（UML）的训练范式，该模型可以交替处理来自不同模态的输入，同时在这些模态之间共享参数。

Result: 使用不成对的辅助模态数据（如文本、音频或图像）能够显著改善各种不同的单模态目标任务的性能。

Conclusion: 实验结果表明，无需显式配对即可利用不成对的辅助多模态数据来提高下游任务的性能。

Abstract: Traditional multimodal learners find unified representations for tasks like
visual question answering, but rely heavily on paired datasets. However, an
overlooked yet potentially powerful question is: can one leverage auxiliary
unpaired multimodal data to directly enhance representation learning in a
target modality? We introduce UML: Unpaired Multimodal Learner, a
modality-agnostic training paradigm in which a single model alternately
processes inputs from different modalities while sharing parameters across
them. This design exploits the assumption that different modalities are
projections of a shared underlying reality, allowing the model to benefit from
cross-modal structure without requiring explicit pairs. Theoretically, under
linear data-generating assumptions, we show that unpaired auxiliary data can
yield representations strictly more informative about the data-generating
process than unimodal training. Empirically, we show that using unpaired data
from auxiliary modalities -- such as text, audio, or images -- consistently
improves downstream performance across diverse unimodal targets such as image
and audio. Our project page: https://unpaired-multimodal.github.io/

</details>


### [84] [Convergence Theorems for Entropy-Regularized and Distributional Reinforcement Learning](https://arxiv.org/abs/2510.08526)
*Yash Jhaveri,Harley Wiltzer,Patrick Shafto,Marc G. Bellemare,David Meger*

Main category: cs.LG

TL;DR: 提出了一种策略优化的理论框架，通过温度解耦和消失熵正则化来保证策略的可解释性和多样性，并提出了一种算法用于估计收益分布。


<details>
  <summary>Details</summary>
Motivation: 强化学习方法在寻找最优策略时，通常会忽略已学习策略的性质，仅关注其预期收益。因此，即使成功了，也难以描述将会学习到哪些策略以及这些策略将会执行什么操作。

Method: 通过消失熵正则化和温度解耦策略，提供一个策略优化的理论框架，确保收敛到特定的最优策略。

Result: 该方法实现了一个可解释的、保持多样性的最优策略，并确保策略导出的对象（价值函数和收益分布）的收敛。在我们的方法的特定实例中，实现的策略均匀地对所有最优动作进行采样。通过温度解耦策略，我们提出了一个算法，该算法可以以任意精度估计与其可解释的、保持多样性的最优策略相关的收益分布。

Conclusion: 提出的理论框架可以用于受益分布的估计，保证可解释性和策略多样性。

Abstract: In the pursuit of finding an optimal policy, reinforcement learning (RL)
methods generally ignore the properties of learned policies apart from their
expected return. Thus, even when successful, it is difficult to characterize
which policies will be learned and what they will do. In this work, we present
a theoretical framework for policy optimization that guarantees convergence to
a particular optimal policy, via vanishing entropy regularization and a
temperature decoupling gambit. Our approach realizes an interpretable,
diversity-preserving optimal policy as the regularization temperature vanishes
and ensures the convergence of policy derived objects--value functions and
return distributions. In a particular instance of our method, for example, the
realized policy samples all optimal actions uniformly. Leveraging our
temperature decoupling gambit, we present an algorithm that estimates, to
arbitrary accuracy, the return distribution associated to its interpretable,
diversity-preserving optimal policy.

</details>


### [85] [On the optimization dynamics of RLVR: Gradient gap and step size thresholds](https://arxiv.org/abs/2510.08539)
*Joe Suk,Yaqi Duan*

Main category: cs.LG

TL;DR: 本文通过提出梯度间隙量，分析了RLVR的训练过程，解释其为何有效并提供了实验证实。


<details>
  <summary>Details</summary>
Motivation: 缺乏对RLVR为何有效的原理性理解。

Method: 理论分析、控制带实验、与LLM实验结合使用。

Result: 经过理论推导和实验证明，证明了梯度间隙对更新方向的影响，以及步长的临界值对学习收敛的影响。

Conclusion: 本文通过分析训练过程中的梯度间隙，建立了RLVR的理论基础，证明了收敛性依赖于更新方向与梯度间隙的对齐。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR), which uses simple
binary feedback to post-train large language models, has shown significant
empirical success. However, a principled understanding of why it works has been
lacking. This paper builds a theoretical foundation for RLVR by analyzing its
training process at both the full-response (trajectory) and token levels.
Central to our analysis is a quantity called the Gradient Gap, which formalizes
the direction of improvement from low-reward to high-reward regions of the
response space. We prove that convergence critically depends on aligning the
update direction with this Gradient Gap. Moreover, we derive a sharp step-size
threshold based on the magnitude of the Gradient Gap: below it, learning
converges, whereas above it, performance collapses. Our theory further predicts
how the critical step size must scale with response length and the success
rate, thereby explaining why practical heuristics such as length normalization
improve stability and showing that, with a fixed learning rate, the success
rate can stagnate strictly below $100\%$. We validate these predictions through
controlled bandit simulations and LLM experiments, including training
Qwen2.5-7B with GRPO.

</details>


### [86] [Improving Reasoning for Diffusion Language Models via Group Diffusion Policy Optimization](https://arxiv.org/abs/2510.08554)
*Kevin Rojas,Jiahe Lin,Kashif Rasul,Anderson Schneider,Yuriy Nevmyvaka,Molei Tao,Wei Deng*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Diffusion language models (DLMs) enable parallel, order-agnostic generation
with iterative refinement, offering a flexible alternative to autoregressive
large language models (LLMs). However, adapting reinforcement learning (RL)
fine-tuning to DLMs remains an open challenge because of the intractable
likelihood. Pioneering work such as diffu-GRPO estimated token-level
likelihoods via one-step unmasking. While computationally efficient, this
approach is severely biased. A more principled foundation lies in
sequence-level likelihoods, where the evidence lower bound (ELBO) serves as a
surrogate. Yet, despite this clean mathematical connection, ELBO-based methods
have seen limited adoption due to the prohibitive cost of likelihood
evaluation. In this work, we revisit ELBO estimation and disentangle its
sources of variance. This decomposition motivates reducing variance through
fast, deterministic integral approximations along a few pivotal dimensions.
Building on this insight, we introduce \textbf{Group Diffusion Policy
Optimization (GDPO)}, a new RL algorithm tailored for DLMs. GDPO leverages
simple yet effective Semi-deterministic Monte Carlo schemes to mitigate the
variance explosion of ELBO estimators under vanilla double Monte Carlo
sampling, yielding a provably lower-variance estimator under tight evaluation
budgets. Empirically, GDPO achieves consistent gains over pretrained
checkpoints and outperforms diffu-GRPO, one of the state-of-the-art baselines,
on the majority of math, reasoning, and coding benchmarks.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [87] [Network Topology and Information Efficiency of Multi-Agent Systems: Study based on MARL](https://arxiv.org/abs/2510.07888)
*Xinren Zhang,Sixi Cheng,Zixin Zhong,Jiadong Yu*

Main category: cs.MA

TL;DR: 研究通讯拓扑及信息效率对多智能体系统的影响，提出新指标提高性能。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体系统中的非稳定性和部分可观测性问题，通过优化通讯结构和评估解决方案。

Method: 通过探索通讯拓扑和信息效率，使用信息熵效率指数（IEI）和专业化效率指数（SEI）作为评估指标，提升成功率和收敛速度。

Result: 定向和顺序拓扑能提升性能，同时降低通讯开销，引入IEI和SEI指标在训练中增强成功率和收敛速度。

Conclusion: 设计自适应通讯拓扑结构结合信息效率高的通讯消息对于复杂多智能体系统中的有效协调至关重要。

Abstract: Multi-agent systems (MAS) solve complex problems through coordinated
autonomous entities with individual decision-making capabilities. While
Multi-Agent Reinforcement Learning (MARL) enables these agents to learn
intelligent strategies, it faces challenges of non-stationarity and partial
observability. Communications among agents offer a solution, but questions
remain about its optimal structure and evaluation. This paper explores two
underexamined aspects: communication topology and information efficiency. We
demonstrate that directed and sequential topologies improve performance while
reducing communication overhead across both homogeneous and heterogeneous
tasks. Additionally, we introduce two metrics -- Information Entropy Efficiency
Index (IEI) and Specialization Efficiency Index (SEI) -- to evaluate message
compactness and role differentiation. Incorporating these metrics into training
objectives improves success rates and convergence speed. Our findings highlight
that designing adaptive communication topologies with information-efficient
messaging is essential for effective coordination in complex MAS.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [88] [Audio-Visual Separation with Hierarchical Fusion and Representation Alignment](https://arxiv.org/abs/2510.07326)
*Han Hu,Dongheng Lin,Qiming Huang,Yuqi Hou,Hyung Jin Chang,Jianbo Jiao*

Main category: cs.MM

TL;DR: 本文通过分层融合策略和表示对齐，实现了音频-视觉源分离任务的性能提升，并在多个数据集上达到最先进的水平。


<details>
  <summary>Details</summary>
Motivation: 研究现有多模态融合方法在音频-视觉分离任务中的性能特征，以提升分离效果。

Method: 在音频编码器的潜在特征与预训练音频模型提取的嵌入进行对齐，并通过分层融合策略整合不同的融合阶段。

Result: 我们的方法在MUSIC、MUSIC-21和VGGSound数据集上取得了最先进的结果，分析表明表示对齐减少了音频和视觉模态之间的差距。

Conclusion: 本文提出了一种分层融合策略，将早期和中期融合阶段有效整合。实验结果表明，该策略在自监督音频-视觉源分离任务上超越了现有方法，达到了最先进的水平。

Abstract: Self-supervised audio-visual source separation leverages natural correlations
between audio and vision modalities to separate mixed audio signals. In this
work, we first systematically analyse the performance of existing multimodal
fusion methods for audio-visual separation task, demonstrating that the
performance of different fusion strategies is closely linked to the
characteristics of the sound: middle fusion is better suited for handling
short, transient sounds, while late fusion is more effective for capturing
sustained and harmonically rich sounds. We thus propose a hierarchical fusion
strategy that effectively integrates both fusion stages. In addition, training
can be made easier by incorporating high-quality external audio
representations, rather than relying solely on the audio branch to learn them
independently. To explore this, we propose a representation alignment approach
that aligns the latent features of the audio encoder with embeddings extracted
from pre-trained audio models. Extensive experiments on MUSIC, MUSIC-21 and
VGGSound datasets demonstrate that our approach achieves state-of-the-art
results, surpassing existing methods under the self-supervised setting. We
further analyse the impact of representation alignment on audio features,
showing that it reduces modality gap between the audio and visual modalities.

</details>


### [89] [AV-EMO-Reasoning: Benchmarking Emotional Reasoning Capabilities in Omni-modal LLMS with Audio-visual Cues](https://arxiv.org/abs/2510.07355)
*Krish Patel,Dingkun Zhou,Ajay Kankipati,Akshaj Gupta,Zeyi Austin Li,Mohul Shukla,Vibhor Narang,Sara Kofman,Zongli Ye,Grace Wang,Xiaoyu Shi,Tingle Li,Guan-Ting Lin,Kan Jen Cheng,Huang-Cheng Chou,Jiachen Lian,Gopala Anumanchipalli*

Main category: cs.MM

TL;DR: 介绍了一种新基准AV-EMO-Reasoning，以评估音视频线索中情感连贯性，实验显示视觉线索提升了情感连贯性，推动人机交互发展。


<details>
  <summary>Details</summary>
Motivation: 尽管全模态大型语言模型（LLM）快速进步，但对情感推理的整体评估仍然有限，并缺乏对视听线索的评估，对此进行了补充。

Method: 引入了AV-EMO-Reasoning基准，通过单回合和多回合的合成视听语料库，以及真实世界的数据集进行评估。

Result: 实验表明，视觉线索能够可靠地提高情感连贯性，相较于仅音频的基线更具优势，同时能够利用视听线索生成更具情感意识的语音。模型在指标方面表现出现互补优势。

Conclusion: AV-EMO-Reasoning提供了一个系统的评估基准，推动了更自然和适应性更强的人机交互的发展。

Abstract: Emotions conveyed through voice and face shape engagement and context in
human-AI interaction. Despite rapid progress in omni-modal large language
models (LLMs), the holistic evaluation of emotional reasoning with audiovisual
cues remains limited. To address this gap, we introduce AV-EMO-Reasoning, a
benchmark designed to systematically assess emotional coherence in LLMs. The
framework leverages a curated, single- and multi-turn synthetic audiovisual
corpus with a real-world set and is assessed under continuous, categorical, and
perceptual metrics. Experiments with leading LLMs show that visual cues
reliably improve emotional coherence over audio-only baselines. Moreover, LLMs
can leverage audio-visual cues to generate more emotion-aware speech. Models
exhibit complementary strengths across metric families, indicating that
automatic scores capture facets distinct from perceptual judgments. By
releasing a systematic evaluation benchmark, AV-EMO-Reasoning offers a
reproducible standard for evaluating emotion-aware dialogue and advances toward
more natural, adaptive human-AI interaction.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [90] [How human is the machine? Evidence from 66,000 Conversations with Large Language Models](https://arxiv.org/abs/2510.07321)
*Antonios Stamatogiannakis,Arsham Ghodsinia,Sepehr Etminanrad,Dilney Gonçalves,David Santos*

Main category: cs.HC

TL;DR: 研究显示大型语言模型在模拟人类消费者行为时，有时会减少、放大或表现出相反的偏差，并在回应一致性上有所不足。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探讨人工智能（特别是大型语言模型）在模拟消费者行为时是否能够准确再现人类的偏差和启发式行为。

Method: 本文采用了十个实验来检验大型语言模型在处理与人类行为相关的偏差和启发式问题时的表现。

Result: 研究发现大型语言模型与人类行为存在四种显著不同：减少或纠正偏差、放大偏差、表现出与人类相反的偏差、同一或类似提示下的回应不一致，这些不一致可能与人类行为特征不符，从而表明大型语言模型模拟或预测消费者行为时可能存在缺陷。

Conclusion: 研究表明，虽然人工智能能够在某些方面模拟人类行为，但在处理偏差及启发式行为时，有时会偏离人类的表现，这可能在使用大型语言模型模拟或预测消费者行为时造成问题。

Abstract: When Artificial Intelligence (AI) is used to replace consumers (e.g.,
synthetic data), it is often assumed that AI emulates established consumers,
and more generally human behaviors. Ten experiments with Large Language Models
(LLMs) investigate if this is true in the domain of well-documented biases and
heuristics. Across studies we observe four distinct types of deviations from
human-like behavior. First, in some cases, LLMs reduce or correct biases
observed in humans. Second, in other cases, LLMs amplify these same biases.
Third, and perhaps most intriguingly, LLMs sometimes exhibit biases opposite to
those found in humans. Fourth, LLMs' responses to the same (or similar) prompts
tend to be inconsistent (a) within the same model after a time delay, (b)
across models, and (c) among independent research studies. Such inconsistencies
can be uncharacteristic of humans and suggest that, at least at one point,
LLMs' responses differed from humans. Overall, unhuman-like responses are
problematic when LLMs are used to mimic or predict consumer behavior. These
findings complement research on synthetic consumer data by showing that sources
of bias are not necessarily human-centric. They also contribute to the debate
about the tasks for which consumers, and more generally humans, can be replaced
by AI.

</details>


### [91] [A LoRa IoT Framework with Machine Learning for Remote Livestock Monitoring in Smart Agriculture](https://arxiv.org/abs/2510.07322)
*Hitesh Mohapatra*

Main category: cs.HC

TL;DR: AgroTrack是一个基于LoRa的物联网系统，用于智能农业中的牲畜远程监控，通过传感器和数据分析提升管理效率和可持续性。


<details>
  <summary>Details</summary>
Motivation: 在智能农业中对自由放养牲畜进行远程监控，需要一种低功耗、长距离通信的解决方案，以提高牲畜管理效率。

Method: 使用LoRa技术进行数据传输，并结合GPS、运动和温度传感器的数据收集，实现实时追踪和基础健康评估。数据通过LoRa传送至网关后再转至云平台以进行可视化和分析。

Result: AgroTrack系统通过提高数据分析能力，将监控工具转变为智能决策支持系统，使农民能够提高牲畜管理、运营效率及农村环境的可持续性。

Conclusion: AgroTrack系统显著提升了智能农业中的牲畜远程监控能力，通过集成先进的分析工具帮助农民优化管理。

Abstract: This work presents AgroTrack, a LoRa-based IoT framework for remote livestock
monitoring in smart agriculture. The system is designed for low-power,
long-range communication and supports real-time tracking and basic health
assessment of free-range livestock through GPS, motion, and temperature sensors
integrated into wearable collars. Data is collected and transmitted via LoRa to
gateways and forwarded to a cloud platform for visualization, alerts, and
analytics. To enhance its practical deployment, AgroTrack incorporates advanced
analytics, including machine learning models for predictive health alerts and
behavioral anomaly detection. This integration transforms the framework from a
basic monitoring tool into an intelligent decision-support system, enabling
farmers to improve livestock management, operational efficiency, and
sustainability in rural environments.

</details>


### [92] [IGUANA: Immersive Guidance, Navigation, and Control for Consumer UAV](https://arxiv.org/abs/2510.07609)
*Victor Victor,Tania Krisanty,Matthew McGinity,Stefan Gumhold,Uwe Aßmann*

Main category: cs.HC

TL;DR: IGUANA是一个结合混合现实技术的无人机控制系统，具有3D地图、虚拟球和空间覆盖三大特色，用户研究表明其在提高用户控制的直观性和态势感知方面效果突出。


<details>
  <summary>Details</summary>
Motivation: 随着无人机和混合现实技术市场的增长，将二者结合可以实现更直观、沉浸和态势感知更强的控制系统。

Method: 进行了用户研究，包括定量和定性评估，以验证设计的有效性。

Result: (1) 3D地图界面直观易用，减轻了用户手动控制的负担，且与传统的双杆控制器相比，体现出更高的准确性、一致性和较低的感知工作负荷；(2) 虚拟球界面直观，但是受到缺乏物理反馈的限制；(3) 空间覆盖非常有助于提升用户的态势感知。

Conclusion: 研究表明，IGUANA系统在提高用户控制无人机的直观性、沉浸感以及态势感知方面效果显著。

Abstract: As the markets for unmanned aerial vehicles (UAVs) and mixed reality (MR)
headsets continue to grow, recent research has increasingly explored their
integration, which enables more intuitive, immersive, and situationally aware
control systems. We present IGUANA, an MR-based immersive guidance, navigation,
and control system for consumer UAVs. IGUANA introduces three key elements
beyond conventional control interfaces: (1) a 3D terrain map interface with
draggable waypoint markers and live camera preview for high-level control, (2)
a novel spatial control metaphor that uses a virtual ball as a physical analogy
for low-level control, and (3) a spatial overlay that helps track the UAV when
it is not visible with the naked eye or visual line of sight is interrupted. We
conducted a user study to evaluate our design, both quantitatively and
qualitatively, and found that (1) the 3D map interface is intuitive and easy to
use, relieving users from manual control and suggesting improved accuracy and
consistency with lower perceived workload relative to conventional dual-stick
controller, (2) the virtual ball interface is intuitive but limited by the lack
of physical feedback, and (3) the spatial overlay is very useful in enhancing
the users' situational awareness.

</details>


### [93] [The Slow Space Editor : Broadening Access to Restorative XR](https://arxiv.org/abs/2510.07610)
*Nate Laffan,Ashley Hom,Andrea Nadine Castillo,Elizabeth Gitelman,Rebecca Zhao,Nikita Shenoy,Kaia Rae Schweig,Katherine Isbister*

Main category: cs.HC

TL;DR: 本文研究如何简化虚拟环境创建过程，提出慢空间概念，并通过设计工具与访谈探讨其在物理和虚拟空间中的应用。


<details>
  <summary>Details</summary>
Motivation: 在这一阶段，我们力求通过简化虚拟环境的创建过程来扩大可能从中受益的用户群体。

Method: 该研究通过研究设计项目，同时进行访谈和设计工具进行探索。

Result: 研究定义了慢空间的概念，通过访谈专业设计师并设计了一个2D工具来研究如何在物理世界中创建慢空间，并最终进行用户工具使用研究。

Conclusion: 本文通过19人的定性研究表明，提供简单的方法让用户控制其环境具有显著的益处。

Abstract: The Slow Space Editor is a 2D tool for creating 3D spaces. It was built as
part of a research-through-design project that investigates how Virtual and
Mixed Reality (XR) environments might be used for reflection and attention
restoration. In this phase, we seek to radically simplify the creation of
virtual environments, thereby broadening the potential group of users who could
benefit from them. The research described in this paper has three aspects.
First, we define the concept of "slow space," situating it alongside existing
research in HCI and environmental psychology. Second, we report on a series of
interviews with professional designers about how slow spaces are created in the
physical world. Third, we share the design of the tool itself, focussing on the
benefits of providing a simple method for users to control their environments.
We conclude with our findings from a 19-person qualitative study of the tool.

</details>


### [94] [Human-in-the-Loop Optimization with Model-Informed Priors](https://arxiv.org/abs/2510.07754)
*Yi-Chi Liao,João Belo,Hee-Seung Moon,Jürgen Steimle,Anna Maria Feit*

Main category: cs.HC

TL;DR: 提出HOMI框架，通过合成数据训练优化器，提升界面设计效率，使用NAF+实现了实时优化，进行了VR输入任务评估。


<details>
  <summary>Details</summary>
Motivation: 人机协同优化需要众多迭代才能识别最优界面设计，当前方法虽有加速但依然面临用户数据收集成本高且经常不切实际的问题。

Method: 本文引入了神经采集函数+（NAF+），这是一种使用强化学习训练的贝叶斯优化方法，通过从大规模合成数据中学习优化策略，提高用户实时优化的效率。

Result: 本文在中空键盘优化实验中评估了HOMI和NAF+，展示了该方法在更高效的界面适应方面的新途径，连接了现场和计算机模拟优化过程。

Conclusion: 本文提出了一种新的概念框架：人机协同优化与模型信息先验（HOMI），通过在部署前利用预测模型生成的多样化合成用户数据来加强人机协同优化。

Abstract: Human-in-the-loop optimization identifies optimal interface designs by
iteratively observing user performance. However, it often requires numerous
iterations due to the lack of prior information. While recent approaches have
accelerated this process by leveraging previous optimization data, collecting
user data remains costly and often impractical. We present a conceptual
framework, Human-in-the-Loop Optimization with Model-Informed Priors (HOMI),
which augments human-in-the-loop optimization with a training phase where the
optimizer learns adaptation strategies from diverse, synthetic user data
generated with predictive models before deployment. To realize HOMI, we
introduce Neural Acquisition Function+ (NAF+), a Bayesian optimization method
featuring a neural acquisition function trained with reinforcement learning.
NAF+ learns optimization strategies from large-scale synthetic data, improving
efficiency in real-time optimization with users. We evaluate HOMI and NAF+ with
mid-air keyboard optimization, a representative VR input task. Our work
presents a new approach for more efficient interface adaptation by bridging in
situ and in silico optimization processes.

</details>


### [95] [The Rise of the Knowledge Sculptor: A New Archetype for Knowledge Work in the Age of Generative AI](https://arxiv.org/abs/2510.07829)
*Cathal Doyle*

Main category: cs.HC

TL;DR: 论文引入知识雕塑师角色，旨在通过人类与生成式AI合作，将AI输出转化为可信赖的知识，并展示其在实践中的应用。


<details>
  <summary>Details</summary>
Motivation: 面对能够自主创作内容的生成式AI系统，传统知识工作强调组织和检索已存在信息的模型显得越来越不够用。因此引入知识雕塑师这一概念以变革生成式时代的知识工作。

Method: 基于社会技术视角，通过一个包含构建愿景、迭代对话、信息雕刻和好奇心驱动综合等能力的框架来概念化知识雕塑师。

Result: 实践表明知识雕塑师角色的有效性，并且该论文本身也作为它所描述雕刻过程的一个实例。

Conclusion: 本文提出了一种新的专业原型：知识雕塑师（KS），用于人类与生成式人工智能（GenAI）的协作，旨在将原始AI输出转化为可信赖且可行动的知识。

Abstract: In the Generative Age, the nature of knowledge work is transforming.
Traditional models that emphasise the organisation and retrieval of
pre-existing information are increasingly inadequate in the face of generative
AI (GenAI) systems capable of autonomous content creation. This paper
introduces the Knowledge Sculptor (KS), a new professional archetype for
Human-GenAI collaboration that transforms raw AI output into trustworthy,
actionable knowledge. Grounded in a socio-technical perspective, the KS is
conceptualised through a framework of competencies, including architecting a
vision, iterative dialogue, information sculpting, and curiosity-driven
synthesis. A practice-based vignette illustrates the KS role in action, and in
a self-referential approach, the paper itself serves as an artefact of the
sculpting process it describes.

</details>


### [96] [A Systematic Evaluation of Self-Supervised Learning for Label-Efficient Sleep Staging with Wearable EEG](https://arxiv.org/abs/2510.07960)
*Emilio Estevan,María Sierra-Torralba,Eduardo López-Larraz,Luis Montesano*

Main category: cs.HC

TL;DR: 研究表明，自监督学习（SSL）在可穿戴EEG睡眠分期中能够提高分类性能，在标签稀缺时取得显著效果，并减少对手动标注的依赖。


<details>
  <summary>Details</summary>
Motivation: 可穿戴EEG设备作为PSG的一种可负担和可扩展的替代方案，广泛采用后会产生大量未标记的数据，无法由临床医生大规模分析。而深度学习的睡眠评分成功依赖于大型标注数据集，因此需要解决标签稀缺的问题。

Method: 研究采用了一系列已经建立的SSL方法，并在两个通过Ikon Sleep可穿戴EEG头带采集的数据库中进行评估。这两个数据库分别是包含PSG和可穿戴EEG记录的高质量基准数据集BOAS，以及大型家庭自录、未标记的记录集HOGAR。

Result: 研究结果表明，SSL的分类性能比监督基线提高了多达10%，尤其在标注数据稀缺的情况下具有显著优势。SSL在标签仅占5%到10%的情况下实现了临床级精度超过80%，而监督方法需要两倍的标注数据。此外，SSL表现对人口特征、记录环境和信号质量的变化具有鲁棒性。

Conclusion: 本文的研究结果表明，SSL在标签稀缺的情况下能够有效提高分类性能，减少对手动标注的依赖，并促进可穿戴EEG睡眠监测系统的发展。

Abstract: Wearable EEG devices have emerged as a promising alternative to
polysomnography (PSG). As affordable and scalable solutions, their widespread
adoption results in the collection of massive volumes of unlabeled data that
cannot be analyzed by clinicians at scale. Meanwhile, the recent success of
deep learning for sleep scoring has relied on large annotated datasets.
Self-supervised learning (SSL) offers an opportunity to bridge this gap,
leveraging unlabeled signals to address label scarcity and reduce annotation
effort. In this paper, we present the first systematic evaluation of SSL for
sleep staging using wearable EEG. We investigate a range of well-established
SSL methods and evaluate them on two sleep databases acquired with the Ikon
Sleep wearable EEG headband: BOAS, a high-quality benchmark containing PSG and
wearable EEG recordings with consensus labels, and HOGAR, a large collection of
home-based, self-recorded, and unlabeled recordings. Three evaluation scenarios
are defined to study label efficiency, representation quality, and
cross-dataset generalization. Results show that SSL consistently improves
classification performance by up to 10% over supervised baselines, with gains
particularly evident when labeled data is scarce. SSL achieves clinical-grade
accuracy above 80% leveraging only 5% to 10% of labeled data, while the
supervised approach requires twice the labels. Additionally, SSL
representations prove robust to variations in population characteristics,
recording environments, and signal quality. Our findings demonstrate the
potential of SSL to enable label-efficient sleep staging with wearable EEG,
reducing reliance on manual annotations and advancing the development of
affordable sleep monitoring systems.

</details>


### [97] [Pre/Absence: Prompting Cultural Awareness and Understanding for Lost Architectural Heritage in Virtual Reality](https://arxiv.org/abs/2510.07967)
*Yaning Li,Ke Zhao,Shucheng Zheng,Xingyu Chen,Chenyi Chen,Wenxi Dai,Weile Jiang,Qi Dong,Yiqing Zhao,Meng Li,Lin-Ping Yuan*

Main category: cs.HC

TL;DR: 研究探讨了如何通过VR来增强对消失遗产的文化意识和情感连接，提出用户可以成为文化意义的共同构建者，有助于设计批判性遗产叙述的框架。


<details>
  <summary>Details</summary>
Motivation: 解决当前解读实践中压缩复杂文化层为事实摘要的问题，并避免线性叙事忽略遗址消失后的持续重新解读。

Method: 使用虚拟现实体验Pre/Absence，通过存在-缺失的辩证法，在时空叙事中交织有形和消失的遗产方面，并进行了一项混合方法研究，与28名参与者比较了Pre/Absence和纸质体验。

Result: 虚拟现实体验更强烈地增强了文化意识，激发了对消失的情感参与，并鼓励对遗产不断变化的社会和政治意义进行批判性反思。

Conclusion: 研究结果表明，虚拟现实可以超越静态重建，让用户成为文化意义的共同构建者，为人机交互中的批判性遗产叙述设计提供了一个细致入微的框架。

Abstract: Lost architectural heritage presents interpretive challenges due to vanished
structures and fragmented historical records. Using Hanyuan Hall of the Tang
dynasty's Daming Palace as a case study, we conducted a formative investigation
with archaeologists, heritage administrators, and visitors to identify key
issues in current interpretation practices. We found that these practices often
compress complex cultural layers into factual summaries and rely on linear
narratives that overlook the continuing reinterpretations following a site's
disappearance. In response, we designed Pre/Absence, a virtual reality
experience grounded in the presence-absence dialectic to interweave tangible
and vanished aspects of heritage within a spatiotemporal narrative. A
mixed-method study with 28 participants compared Pre/Absence to a paper-based
experience. Both improved users' factual understanding, but the VR experience
more strongly enhanced cultural awareness, evoked emotional engagement with
loss, and encouraged critical reflection on the evolving social and political
meanings of heritage. The findings suggest that VR can move beyond static
reconstruction to engage users as co-constructors of cultural meaning,
providing a nuanced framework for critical heritage narrative design in
human-computer interaction.

</details>


### [98] [Quantifying Locomotion Differences Between Virtual Reality Users With and Without Motor Impairments](https://arxiv.org/abs/2510.07987)
*Rachel L. Franz,Jacob O. Wobbrock*

Main category: cs.HC

TL;DR: 研究VR运动技术对上肢损伤人士的可达性，滑动查看技术表现出组间相似性，其它技术通过动作指标揭示差异，适合用于识别损伤。


<details>
  <summary>Details</summary>
Motivation: 当前的VR系统假设用户具有典型能力，但缺乏关于这些系统如何对身体有障碍的人士不太友好的理解。为了解不可访问的运动技术和交互，本研究对上肢损伤人士与普通人群间的VR运动技术性能差异进行了量化研究。

Method: 进行了比较研究，参与者包括有上肢损伤的和没有损伤的人，使用六种运动技术导航虚拟环境，并收集控制器和头戴设备的低级交互数据进行分析。

Result: 滑动查看技术在各组间的表现相似，而其他技术通过头戴设备数据揭示了组间差异。头戴设备的动作相关指标适合识别上肢损伤，并解释特定技术上的组间性能差异。

Conclusion: 滑动查看（Sliding Looking）技术在所有性能指标上都表现出组间相似性，可能是VR应用程序的良好默认运动技术。同时，动作相关指标可以识别用户是否有上肢障碍，并解释不同组在特定运动技术中的性能差异。

Abstract: Today's virtual reality (VR) systems and environments assume that users have
typical abilities, which can make VR inaccessible to people with physical
impairments. However, there is not yet an understanding of how inaccessible
locomotion techniques are, and which interactions make them inaccessible. To
this end, we conducted a study in which people with and without upper-body
impairments navigated a virtual environment with six locomotion techniques to
quantify performance differences among groups. We found that groups performed
similarly with Sliding Looking on all performance measures, suggesting that
this might be a good default locomotion technique for VR apps. To understand
the nature of performance differences with the other techniques, we collected
low-level interaction data from the controllers and headset and analyzed
interaction differences with a set of movement-, button-, and target-related
metrics. We found that movement-related metrics from headset data reveal
differences among groups with all techniques, suggesting these are good metrics
for identifying whether a user has an upper-body impairment. We also identify
movement-, button, and target-related metrics that can explain performance
differences between groups for particular locomotion techniques.

</details>


### [99] [Development of Mental Models in Human-AI Collaboration: A Conceptual Framework](https://arxiv.org/abs/2510.08104)
*Joshua Holstein,Gerhard Satzger*

Main category: cs.HC

TL;DR: 本文探讨了人类与AI协作中文化模型的演变，提出了一个综合框架并识别三种心理模型：领域、信息处理、互补意识。


<details>
  <summary>Details</summary>
Motivation: 填补决策者心理模型在其与AI系统持续互动中进化的研究空白。

Method: 构建一个整合的社会技术框架，用以研究人类与人工智能协作中文化模型的演变机制。

Result: 引入了三种独特的心理模型，并识别了心理模型的动态性质，确立了指导有效设计人类与AI协作的机制。

Conclusion: 研究提出了一个综合的社会技术框架，以识别驱动心理模型演变的机制，如数据情境化、推理透明性和性能反馈。

Abstract: Artificial intelligence has become integral to organizational decision-making
and while research has explored many facets of this human-AI collaboration, the
focus has mainly been on designing the AI agent(s) and the way the
collaboration is set up - generally assuming a human decision-maker to be
"fixed". However, it has largely been neglected that decision-makers' mental
models evolve through their continuous interaction with AI systems. This paper
addresses this gap by conceptualizing how the design of human-AI collaboration
influences the development of three complementary and interdependent mental
models necessary for this collaboration. We develop an integrated
socio-technical framework that identifies the mechanisms driving the mental
model evolution: data contextualization, reasoning transparency, and
performance feedback. Our work advances human-AI collaboration literature
through three key contributions: introducing three distinct mental models
(domain, information processing, complementarity-awareness); recognizing the
dynamic nature of mental models; and establishing mechanisms that guide the
purposeful design of effective human-AI collaboration.

</details>


### [100] [Sentiment Matters: An Analysis of 200 Human-SAV Interactions](https://arxiv.org/abs/2510.08202)
*Lirui Guo,Michael G. Burke,Wynita M. Griggs*

Main category: cs.HC

TL;DR: 研究介绍了一个开源的SAVs人机交互数据集，并通过基准案例研究证明了其效用，为设计对话型SAV接口提供新见解，并对情感分析工具进行性能基准测试。


<details>
  <summary>Details</summary>
Motivation: 随着共享自动驾驶车辆（SAVs）逐渐成为交通系统的重要组成部分，研究人类与SAVs之间的有效交互显得尤为重要。

Method: 本文提供了一个由200个人与SAVs交互的数据集，包括文本数据（如2136次对话）和实证数据（如交互后调查结果中的多种心理因素）。

Result: 研究通过随机森林建模和弦图识别了SAVs接受度和服务质量关键预测因子，强调了回应情绪极性的关键影响。同时，研究比较了使用LLM的情感分析工具与传统的TextBlob方法的表现，发现零样本LLM提示更符合用户报告的情感。

Conclusion: 本文为设计对话型SAV接口提供了新颖的见解，为进一步探索高级情感建模、自适应用户交互和多模态对话系统建立了基础。

Abstract: Shared Autonomous Vehicles (SAVs) are likely to become an important part of
the transportation system, making effective human-SAV interactions an important
area of research. This paper introduces a dataset of 200 human-SAV interactions
to further this area of study. We present an open-source human-SAV
conversational dataset, comprising both textual data (e.g., 2,136 human-SAV
exchanges) and empirical data (e.g., post-interaction survey results on a range
of psychological factors). The dataset's utility is demonstrated through two
benchmark case studies: First, using random forest modeling and chord diagrams,
we identify key predictors of SAV acceptance and perceived service quality,
highlighting the critical influence of response sentiment polarity (i.e.,
perceived positivity). Second, we benchmark the performance of an LLM-based
sentiment analysis tool against the traditional lexicon-based TextBlob method.
Results indicate that even simple zero-shot LLM prompts more closely align with
user-reported sentiment, though limitations remain. This study provides novel
insights for designing conversational SAV interfaces and establishes a
foundation for further exploration into advanced sentiment modeling, adaptive
user interactions, and multimodal conversational systems.

</details>


### [101] [Motion Exploration of Articulated Product Concepts in Interactive Sketching Environment](https://arxiv.org/abs/2510.08328)
*Kalyan Ramana Gattoz,Prasad S. Onkar*

Main category: cs.HC

TL;DR: 提出了一种数字化处理草图的新方法，显著降低设计过程中的认知负荷，用户满意度高。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在资源和时间消耗大以及草图静态影响产品运动展示的问题，因此需要一种更有效的设计工具。

Method: 开发了一种应用程序用于评估这种数字化草图的方法，同时关注用户在设计任务中的满意度和心理负荷。

Result: 结果显示与传统方法相比认知努力减少了77%，用户满意度较高。

Conclusion: 本研究提出了一种新的方法，使设计师能够以更加互动的方式创建、模拟和测试机械概念的运动，从而减少设计过程中繁琐和重复的工作。

Abstract: In the early stages of engineering design, it is essential to know how a
product behaves, especially how it moves. As designers must keep adjusting the
motion until it meets the intended requirements, this process is often
repetitive and time-consuming. Although the physics behind these motions is
usually based on simple equations, manually working through them can be tedious
and inefficient. To ease this burden, some tasks are now handled by computers.
One common method involves converting hand-drawn sketches into models using CAD
or CAE software. However, this approach can be time- and resource-intensive.
Additionally, product sketches are usually best understood only by the
designers who created them. Others may struggle to interpret them correctly,
relying heavily on intuition and prior experience. Since sketches are static,
they fail to show how a product moves, limiting their usefulness. This paper
presents a new approach that addresses these issues by digitising the natural
act of sketching. It allows designers to create, simulate, and test the motion
of mechanical concepts in a more interactive way. An application was developed
to evaluate this method, focusing on user satisfaction and mental workload
during a design task. The results showed a 77% reduction in cognitive effort
compared to traditional methods, with users reporting high satisfaction. Future
work will focus on expanding this approach from 2D (planar) to full 3D
(spatial) design environments, enabling more complex product concept
development.

</details>


### [102] [What Makes a Visualization Complex?](https://arxiv.org/abs/2510.08332)
*Mengdi Chu,Zefeng Qiu,Meng Ling,Shuning Jiang,Robert S. Laramee,Michael Sedlmair,Jian Chen*

Main category: cs.HC

TL;DR: 研究通过大规模实验发现，低级图像属性和高级元素都影响数据可视化中的感知视觉复杂性。特征拥挤度是复杂性最强预测因子，边缘密度解释结点连线图复杂性。文本批注模块的增加存在钟形曲线效应。


<details>
  <summary>Details</summary>
Motivation: 研究数据可视化中的感知视觉复杂性，并使用基于图像的客观度量来实现。

Method: 我们通过大规模众包实验收集了349名参与者和1,800个可视化图像的视觉复杂性(VC)分数。我们然后检查了这些分数如何与12个基于图像的度量对齐，这些度量涵盖信息论、杂乱程度、颜色以及我们两个基于对象的度量。

Result: 我们展示了低级图像属性和高级元素都影响感知VC；角点数量和不同颜色在可视化中是稳健的度量。特征拥挤度是丰富相同刺激的可视化中感知复杂性的最强预测因子；边缘密度有效解释了结点连线图中的VC。我们观察到文本批注存在钟形曲线效应: TiR的增加最初减少复杂性，达到最佳点后进一步增加文本提高了感知复杂性。量化管道是可解释的，能够实现基于度量的解释。

Conclusion: 我们发现，低级图像属性和高级元素都影响可视化图像的感知视觉复杂性；角点数量和不同颜色是跨可视化的稳健指标。特征拥挤度是捕捉颜色和纹理中统计模式的信息理论度量，是丰富相同刺激的可视化中感知复杂性的最强预测因子；边缘密度有效解释了结点连线图中的视觉复杂性。此外，我们观察到文本批注存在一个钟形曲线效应：增加文本与墨水比例(TiR)最初减少复杂性，达到最佳点后进一步的文本增加了感知复杂性。我们的量化管道是可解释的，使基于度量的解释具有依据。

Abstract: We investigate the perceived visual complexity (VC) in data visualizations
using objective image-based metrics. We collected VC scores through a
large-scale crowdsourcing experiment involving 349 participants and 1,800
visualization images. We then examined how these scores align with 12
image-based metrics spanning information-theoretic, clutter, color, and our two
object-based metrics. Our results show that both low-level image properties and
the high-level elements affect perceived VC in visualization images; The number
of corners and distinct colors are robust metrics across visualizations.
Second, feature congestion, an information-theoretic metric capturing
statistical patterns in color and texture, is the strongest predictor of
perceived complexity in visualizations rich in the same stimuli; edge density
effectively explains VC in node-link diagrams. Additionally, we observe a
bell-curve effect for text annotations: increasing text-to-ink ratio (TiR)
initially reduces complexity, reaching an optimal point, beyond which further
text increases perceived complexity. Our quantification pipeline is also
interpretable, enabling metric-based explanations, grounded in the
VisComplexity2K dataset, bridging computational metrics with human perceptual
responses. osf.io/5xe8a has the preregistration and osf.io/bdet6 has the
VisComplexity2K dataset, source code, and all Apdx. and figures.

</details>
