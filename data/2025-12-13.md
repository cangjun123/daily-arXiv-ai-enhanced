<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 8]
- [cs.CL](#cs.CL) [Total: 2]
- [cs.LG](#cs.LG) [Total: 10]
- [cs.AI](#cs.AI) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation](https://arxiv.org/abs/2512.10949)
*Yiwen Tang,Zoey Guo,Kaixin Zhu,Ray Zhang,Qizhi Chen,Dongzhi Jiang,Junli Liu,Bohan Zeng,Haoming Song,Delin Qu,Tianyi Bai,Dan Xu,Wentao Zhang,Bin Zhao*

Main category: cs.CV

TL;DR: 该研究首次系统探索了RL在文本到3D自回归生成中的应用，通过奖励设计、RL算法、基准测试和分层RL框架的研究，开发了首个RL增强的文本到3D模型AR3D-R1


<details>
  <summary>Details</summary>
Motivation: 传统RL已成功应用于2D图像生成，但3D生成由于空间复杂度高、需要全局几何一致性和细粒度局部纹理，对奖励设计和RL算法更加敏感，相关研究尚未深入探索。

Method: 从四个维度系统研究：(1)奖励设计：评估奖励维度和模型选择；(2)RL算法：研究GRPO变体，特别是词元级优化；(3)引入MME-3DR基准测试；(4)提出分层GRPO（Hi-GRPO）框架优化全局到局部的3D生成。

Result: 基于研究洞见开发了首个RL增强的文本到3D模型AR3D-R1，证明人与奖励的对齐、多模态模型的鲁棒信号、词元级优化的有效性、以及分层RL框架的优势。

Conclusion: 这项研究为RL驱动的3D生成推理提供了重要洞见，展示了RL在复杂3D生成任务中的应用潜力，推动3D生成技术发展。

Abstract: Reinforcement learning (RL), earlier proven to be effective in large language and multi-modal models, has been successfully extended to enhance 2D image generation recently. However, applying RL to 3D generation remains largely unexplored due to the higher spatial complexity of 3D objects, which require globally consistent geometry and fine-grained local textures. This makes 3D generation significantly sensitive to reward designs and RL algorithms. To address these challenges, we conduct the first systematic study of RL for text-to-3D autoregressive generation across several dimensions. (1) Reward designs: We evaluate reward dimensions and model choices, showing that alignment with human preference is crucial, and that general multi-modal models provide robust signal for 3D attributes. (2) RL algorithms: We study GRPO variants, highlighting the effectiveness of token-level optimization, and further investigate the scaling of training data and iterations. (3) Text-to-3D Benchmarks: Since existing benchmarks fail to measure implicit reasoning abilities in 3D generation models, we introduce MME-3DR. (4) Advanced RL paradigms: Motivated by the natural hierarchy of 3D generation, we propose Hi-GRPO, which optimizes the global-to-local hierarchical 3D generation through dedicated reward ensembles. Based on these insights, we develop AR3D-R1, the first RL-enhanced text-to-3D model, expert from coarse shape to texture refinement. We hope this study provides insights into RL-driven reasoning for 3D generation. Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1.

</details>


### [2] [MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence](https://arxiv.org/abs/2512.10863)
*Jingli Lin,Runsen Xu,Shaohao Zhu,Sihan Yang,Peizhou Cao,Yunlong Ran,Miao Hu,Chenming Zhu,Yiman Xie,Yilin Long,Wenbo Hu,Dahua Lin,Tai Wang,Jiangmiao Pang*

Main category: cs.CV

TL;DR: MMSI-Video-Bench是一个完全人工标注的视频空间智能评估基准，包含1,106个问题，涵盖感知、规划、预测和跨视频推理四个层级，旨在全面评估MLLMs在物理环境中的空间理解能力。


<details>
  <summary>Details</summary>
Motivation: 当前的MLLMs需要具备对连续视觉输入的空间理解能力，以发展成为物理环境中的通用助手，但现有基准无法全面评估这方面的进展。

Method: 构建了一个包含1,106个问题的视频基准，这些问题基于来自25个数据集和内部视频的1,278个片段，操作化了感知、规划、预测和跨视频推理的四级框架，并由3DV专家精心设计和审查。

Result: 评估了25个开源和专有MLLMs，发现明显的人机差距：许多模型接近随机水平，最佳推理模型落后人类近60%。空间微调的模型在基准上泛化效果仍不佳，错误分析揭示了几何推理、运动基础、长时预测和跨视频对应方面的系统性失败。

Conclusion: MMSI-Video-Bench为推进视频空间智能研究提供了坚实的测试平台，揭示了当前MLLMs在空间理解方面的重大局限性，并指出了未来改进的方向。

Abstract: Spatial understanding over continuous visual input is crucial for MLLMs to evolve into general-purpose assistants in physical environments. Yet there is still no comprehensive benchmark that holistically assesses the progress toward this goal. In this work, we introduce MMSI-Video-Bench, a fully human-annotated benchmark for video-based spatial intelligence in MLLMs. It operationalizes a four-level framework, Perception, Planning, Prediction, and Cross-Video Reasoning, through 1,106 questions grounded in 1,278 clips from 25 datasets and in-house videos. Each item is carefully designed and reviewed by 3DV experts with explanatory rationales to ensure precise, unambiguous grounding. Leveraging its diverse data sources and holistic task coverage, MMSI-Video-Bench also supports three domain-oriented sub-benchmarks (Indoor Scene Perception Bench, Robot Bench and Grounding Bench) for targeted capability assessment. We evaluate 25 strong open-source and proprietary MLLMs, revealing a striking human--AI gap: many models perform near chance, and the best reasoning model lags humans by nearly 60%. We further find that spatially fine-tuned models still fail to generalize effectively on our benchmark. Fine-grained error analysis exposes systematic failures in geometric reasoning, motion grounding, long-horizon prediction, and cross-video correspondence. We also show that typical frame-sampling strategies transfer poorly to our reasoning-intensive benchmark, and that neither 3D spatial cues nor chain-of-thought prompting yields meaningful gains. We expect our benchmark to establish a solid testbed for advancing video-based spatial intelligence.

</details>


### [3] [BabyVLM-V2: Toward Developmentally Grounded Pretraining and Benchmarking of Vision Foundation Models](https://arxiv.org/abs/2512.10932)
*Shengao Wang,Wenqi Wang,Zecheng Wang,Max Whitton,Michael Wakeham,Arjun Chandra,Joey Huang,Pengyue Zhu,Helen Chen,David Li,Jeffrey Li,Shawn Li,Andrew Zagula,Amy Zhao,Andrew Zhu,Sayaka Nakamura,Yuki Yamamoto,Jerry Jun Yokono,Aaron Mueller,Bryan A. Plummer,Kate Saenko,Venkatesh Saligrama,Boqing Gong*

Main category: cs.CV

TL;DR: BabyVLM-V2是一个基于儿童发展轨迹的视觉语言建模框架，通过婴幼儿启发的多模态预训练和认知评估工具箱，实现了样本高效的视觉基础模型预训练。


<details>
  <summary>Details</summary>
Motivation: 受早期儿童发展轨迹的启发，研究人员希望开发一个能够模拟婴幼儿学习过程的视觉语言模型预训练框架。现有的视觉基础模型通常需要大量数据进行训练，而婴幼儿的学习过程具有高度的样本效率，这为开发更高效的预训练方法提供了自然目标。

Method: BabyVLM-V2框架包含三个核心组件：1）基于纵向、多方面婴幼儿中心视听语料的预训练数据集，涵盖视频-语音、图像-语音和多轮对话数据；2）BabyVLM-V2模型本身；3）DevCV Toolbox认知评估套件，将NIH Baby Toolbox中的所有视觉相关测量指标转化为十个多模态任务基准，覆盖空间推理、记忆和词汇理解等早期儿童能力。

Result: 实验结果显示，一个从头开始预训练的紧凑模型在DevCV Toolbox上能够达到有竞争力的性能，甚至在某些任务上超越了GPT-4o。这表明基于发展心理学启发的预训练方法能够实现样本高效的学习。

Conclusion: BabyVLM-V2提供了一个原则化、统一的框架，有望加速在视觉基础模型发展可信预训练方面的研究。该框架展示了基于早期儿童发展轨迹的预训练方法在实现样本高效学习方面的潜力，为开发更接近人类学习方式的人工智能系统提供了新思路。

Abstract: Early children's developmental trajectories set up a natural goal for sample-efficient pretraining of vision foundation models. We introduce BabyVLM-V2, a developmentally grounded framework for infant-inspired vision-language modeling that extensively improves upon BabyVLM-V1 through a longitudinal, multifaceted pretraining set, a versatile model, and, most importantly, DevCV Toolbox for cognitive evaluation. The pretraining set maximizes coverage while minimizing curation of a longitudinal, infant-centric audiovisual corpus, yielding video-utterance, image-utterance, and multi-turn conversational data that mirror infant experiences. DevCV Toolbox adapts all vision-related measures of the recently released NIH Baby Toolbox into a benchmark suite of ten multimodal tasks, covering spatial reasoning, memory, and vocabulary understanding aligned with early children's capabilities. Experimental results show that a compact model pretrained from scratch can achieve competitive performance on DevCV Toolbox, outperforming GPT-4o on some tasks. We hope the principled, unified BabyVLM-V2 framework will accelerate research in developmentally plausible pretraining of vision foundation models.

</details>


### [4] [Any4D: Unified Feed-Forward Metric 4D Reconstruction](https://arxiv.org/abs/2512.10935)
*Jay Karhade,Nikhil Keetha,Yuchen Zhang,Tanisha Gupta,Akash Sharma,Sebastian Scherer,Deva Ramanan*

Main category: cs.CV

TL;DR: Any4D是一种可扩展的多视角Transformer模型，用于实现度量尺度、密集前馈的4D重建，能够灵活处理多种输入模态并达到较高的精度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有的4D重建方法通常局限于双目密集场景流或稀疏3D点跟踪，或者仅限于处理单目RGB视频，缺乏能够灵活整合多种传感器数据（如RGB-D、IMU、雷达）的统一框架。

Method: 提出模块化的4D场景表示方法：将以自我为中心的深度图和相机内参（局部相机坐标系）与以环境为中心的相机外参和场景流（全局世界坐标系）分离编码，构建多视角Transformer直接生成逐像素运动和几何预测。

Result: 在多个数据集上取得更低的误差（2-3倍降低）和更高的计算效率（15倍加速），为下游应用提供了高效可靠的4D重建能力。

Conclusion: Any4D通过创新的模块化表示和灵活的传感器融合机制，实现了高性能、高效率的4D重建，为多种应用场景提供了强大支持。

Abstract: We present Any4D, a scalable multi-view transformer for metric-scale, dense feed-forward 4D reconstruction. Any4D directly generates per-pixel motion and geometry predictions for N frames, in contrast to prior work that typically focuses on either 2-view dense scene flow or sparse 3D point tracking. Moreover, unlike other recent methods for 4D reconstruction from monocular RGB videos, Any4D can process additional modalities and sensors such as RGB-D frames, IMU-based egomotion, and Radar Doppler measurements, when available. One of the key innovations that allows for such a flexible framework is a modular representation of a 4D scene; specifically, per-view 4D predictions are encoded using a variety of egocentric factors (depthmaps and camera intrinsics) represented in local camera coordinates, and allocentric factors (camera extrinsics and scene flow) represented in global world coordinates. We achieve superior performance across diverse setups - both in terms of accuracy (2-3X lower error) and compute efficiency (15X faster), opening avenues for multiple downstream applications.

</details>


### [5] [OmniView: An All-Seeing Diffusion Model for 3D and 4D View Synthesis](https://arxiv.org/abs/2512.10940)
*Xiang Fan,Sharath Girish,Vivek Ramanujan,Chaoyang Wang,Ashkan Mirzaei,Petr Sushko,Aliaksandr Siarohin,Sergey Tulyakov,Ranjay Krishna*

Main category: cs.CV

TL;DR: OmniView是一个统一的4D一致性框架，通过分离表示空间、时间和视角条件，可灵活组合输入，能处理多视角合成、动态场景、文本/图像到视频等多种任务，在多个基准测试中性能优于特定任务模型。


<details>
  <summary>Details</summary>
Motivation: 现有的相机控制扩散模型方法各自专注于特定的4D一致性任务子集（如新视角合成、文本到视频等），导致在不同数据片段上分散训练。需要一个统一框架来泛化处理广泛的4D任务。

Method: OmniView框架将空间、时间和视角条件分开表示，允许灵活组合这些输入条件。通过统一的模型架构处理静态、动态和多视角输入，支持时间轨迹的前后推断，以及在文本或图像提示下的相机控制视频生成。

Result: 在多项基准测试中，OmniView与特定任务模型竞争表现优异：在多视角合成LLFF数据集上图像质量提升33%，动态新视角合成Neural 3D Video基准上提升60%，静态相机控制RE-10K上提升20%，文本条件下视频生成的相机轨迹误差减少4倍。

Conclusion: OmniView展示了通用4D视频模型的可行性，其强大的泛化能力表明单一模型可以统一处理多种4D一致性任务，为通用视频合成提供了新方向。

Abstract: Prior approaches injecting camera control into diffusion models have focused on specific subsets of 4D consistency tasks: novel view synthesis, text-to-video with camera control, image-to-video, amongst others. Therefore, these fragmented approaches are trained on disjoint slices of available 3D/4D data. We introduce OmniView, a unified framework that generalizes across a wide range of 4D consistency tasks. Our method separately represents space, time, and view conditions, enabling flexible combinations of these inputs. For example, OmniView can synthesize novel views from static, dynamic, and multiview inputs, extrapolate trajectories forward and backward in time, and create videos from text or image prompts with full camera control. OmniView is competitive with task-specific models across diverse benchmarks and metrics, improving image quality scores among camera-conditioned diffusion models by up to 33\% in multiview NVS LLFF dataset, 60\% in dynamic NVS Neural 3D Video benchmark, 20\% in static camera control on RE-10K, and reducing camera trajectory errors by 4x in text-conditioned video generation. With strong generalizability in one model, OmniView demonstrates the feasibility of a generalist 4D video model. Project page is available at https://snap-research.github.io/OmniView/

</details>


### [6] [Mull-Tokens: Modality-Agnostic Latent Thinking](https://arxiv.org/abs/2512.10941)
*Arijit Ray,Ahmed Abdelkader,Chengzhi Mao,Bryan A. Plummer,Kate Saenko,Ranjay Krishna,Leonidas Guibas,Wen-Sheng Chu*

Main category: cs.CV

TL;DR: 本文介绍Mull-Tokens方法，通过在文本和图像模态间使用与模态无关的潜在token进行无监督中间推理，提升多模态空间推理性能


<details>
  <summary>Details</summary>
Motivation: 现有基于文本-图像交替思维的多模态推理模型存在脆弱性、扩展性差、依赖专业工具或高昂图像生成成本等问题，需要更简单有效的跨模态推理机制

Method: 提出Mull-Tokens方法：1) 训练与模态无关的潜在token来承载中间推理信息，可在文本和图像模态间自由转换；2) 借鉴潜在推理框架最佳实践；3) 先使用交错文本-图像轨迹监督训练，再仅用最终答案进行无监督微调

Result: 在4个挑战性空间推理基准测试中（包括解谜、视角转换等任务），Mull-Tokens相比仅使用文本推理或交错图像-文本推理的基线方法平均提升+3%，在推理密集型谜题解决任务上最高提升+16%

Conclusion: Mull-Tokens为多模态抽象思维提供简单解决方案，突破了当前文本与视觉推理的局限性，展示了潜在token在跨模态推理中的有效性

Abstract: Reasoning goes beyond language; the real world requires reasoning about space, time, affordances, and much more that words alone cannot convey. Existing multimodal models exploring the potential of reasoning with images are brittle and do not scale. They rely on calling specialist tools, costly generation of images, or handcrafted reasoning data to switch between text and image thoughts. Instead, we offer a simpler alternative -- Mull-Tokens -- modality-agnostic latent tokens pre-trained to hold intermediate information in either image or text modalities to let the model think free-form towards the correct answer. We investigate best practices to train Mull-Tokens inspired by latent reasoning frameworks. We first train Mull-Tokens using supervision from interleaved text-image traces, and then fine-tune without any supervision by only using the final answers. Across four challenging spatial reasoning benchmarks involving tasks such as solving puzzles and taking different perspectives, we demonstrate that Mull-Tokens improve upon several baselines utilizing text-only reasoning or interleaved image-text reasoning, achieving a +3% average improvement and up to +16% on a puzzle solving reasoning-heavy split compared to our strongest baseline. Adding to conversations around challenges in grounding textual and visual reasoning, Mull-Tokens offers a simple solution to abstractly think in multiple modalities.

</details>


### [7] [AlcheMinT: Fine-grained Temporal Control for Multi-Reference Consistent Video Generation](https://arxiv.org/abs/2512.10943)
*Sharath Girish,Viacheslav Ivanov,Tsai-Shien Chen,Hao Chen,Aliaksandr Siarohin,Sergey Tulyakov*

Main category: cs.CV

TL;DR: AlcheMinT是一个统一框架，通过时间戳条件控制实现了细粒度时序控制的主体驱动视频生成，能够在视频中精确控制多主体的出现和消失时间。


<details>
  <summary>Details</summary>
Motivation: 现有的主体驱动视频生成方法缺乏对主体出现和消失的细粒度时序控制，这在合成视频、故事板和可控动画等应用中至关重要。

Method: 1) 引入新颖的位置编码机制来编码与主体身份相关的时间间隔，并与预训练视频生成模型的位置嵌入无缝集成；2) 加入主体描述性文本标记，加强视觉身份与视频描述之间的绑定，减少生成过程中的模糊性；3) 通过标记级连接避免额外的交叉注意力模块，实现可忽略的参数开销。

Result: 实验表明AlcheMinT在视觉质量上达到最先进的视频个性化方法水平，同时首次实现对视频内多主体生成的精确时序控制。

Conclusion: AlcheMinT通过时间戳条件控制解决了现有主体驱动视频生成方法中缺乏细粒度时序控制的问题，为合成视频、故事板和可控动画等应用提供了更精确的控制能力。

Abstract: Recent advances in subject-driven video generation with large diffusion models have enabled personalized content synthesis conditioned on user-provided subjects. However, existing methods lack fine-grained temporal control over subject appearance and disappearance, which are essential for applications such as compositional video synthesis, storyboarding, and controllable animation. We propose AlcheMinT, a unified framework that introduces explicit timestamps conditioning for subject-driven video generation. Our approach introduces a novel positional encoding mechanism that unlocks the encoding of temporal intervals, associated in our case with subject identities, while seamlessly integrating with the pretrained video generation model positional embeddings. Additionally, we incorporate subject-descriptive text tokens to strengthen binding between visual identity and video captions, mitigating ambiguity during generation. Through token-wise concatenation, AlcheMinT avoids any additional cross-attention modules and incurs negligible parameter overhead. We establish a benchmark evaluating multiple subject identity preservation, video fidelity, and temporal adherence. Experimental results demonstrate that AlcheMinT achieves visual quality matching state-of-the-art video personalization methods, while, for the first time, enabling precise temporal control over multi-subject generation within videos. Project page is at https://snap-research.github.io/Video-AlcheMinT

</details>


### [8] [SceneMaker: Open-set 3D Scene Generation with Decoupled De-occlusion and Pose Estimation Model](https://arxiv.org/abs/2512.10957)
*Yukai Shi,Weiyu Li,Zihao Wang,Hongyang Li,Xingyu Chen,Ping Tan,Lei Zhang*

Main category: cs.CV

TL;DR: 本文将3D场景生成解耦为去遮挡和姿态估计两个模块，通过增强的去遮挡模型和统一的姿态估计模型来解决开放式场景中严重遮挡和姿态估计问题


<details>
  <summary>Details</summary>
Motivation: 现有方法由于缺乏足够的开放式去遮挡和姿态估计先验知识，在严重遮挡和开放式场景下难以同时生成高质量几何结构和准确姿态

Method: 首先解耦去遮挡模型与3D物体生成，利用图像数据集和收集的去遮挡数据集增强多样性；其次提出统一姿态估计模型，集成全局和局部注意力机制；构建开放式3D场景数据集

Result: 综合实验表明，该解耦框架在室内和开放式场景中均表现出优越性能

Conclusion: SceneMaker框架通过解耦设计和增强的关键组件，能有效处理复杂遮挡情况并提高姿态估计精度，在开放式3D场景生成中具有显著优势

Abstract: We propose a decoupled 3D scene generation framework called SceneMaker in this work. Due to the lack of sufficient open-set de-occlusion and pose estimation priors, existing methods struggle to simultaneously produce high-quality geometry and accurate poses under severe occlusion and open-set settings. To address these issues, we first decouple the de-occlusion model from 3D object generation, and enhance it by leveraging image datasets and collected de-occlusion datasets for much more diverse open-set occlusion patterns. Then, we propose a unified pose estimation model that integrates global and local mechanisms for both self-attention and cross-attention to improve accuracy. Besides, we construct an open-set 3D scene dataset to further extend the generalization of the pose estimation model. Comprehensive experiments demonstrate the superiority of our decoupled framework on both indoor and open-set scenes. Our codes and datasets is released at https://idea-research.github.io/SceneMaker/.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [9] [Quantifying Emotional Tone in Tolkien's The Hobbit: Dialogue Sentiment Analysis with RegEx, NRC-VAD, and Python](https://arxiv.org/abs/2512.10865)
*Lilin Qiu*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This study analyzes the emotional tone of dialogue in J. R. R. Tolkien's The Hobbit (1937) using computational text analysis. Dialogue was extracted with regular expressions, then preprocessed, and scored using the NRC-VAD lexicon to quantify emotional dimensions. The results show that the dialogue maintains a generally positive (high valence) and calm (low arousal) tone, with a gradually increasing sense of agency (dominance) as the story progresses. These patterns reflect the novel's emotional rhythm: moments of danger and excitement are regularly balanced by humor, camaraderie, and relief. Visualizations -- including emotional trajectory graphs and word clouds -- highlight how Tolkien's language cycles between tension and comfort. By combining computational tools with literary interpretation, this study demonstrates how digital methods can uncover subtle emotional structures in literature, revealing the steady rhythm and emotional modulation that shape the storytelling in The Hobbit.

</details>


### [10] [Computational emotion analysis with multimodal LLMs: Current evidence on an emerging methodological opportunity](https://arxiv.org/abs/2512.10882)
*Hauke Licht*

Main category: cs.CL

TL;DR: 该研究评估了多模态大语言模型在视频情感唤起分析中的表现，发现在理想条件下模型有高度可靠性和低人口统计偏差，但在实际议会辩论场景中表现不佳，强调了AI方法在政治分析中需要持续评估的必要性。


<details>
  <summary>Details</summary>
Motivation: 情绪在政治传播中至关重要，多模态生成式AI为情感分析提供了新可能，但目前缺乏关于多模态AI在情感分析中有效性的证据，需要填补这一研究空白。

Method: 使用两个互补的人工标注视频数据集，评估当前多模态大语言模型在基于视频的情感唤起分析中的表现，其中一个数据集是理想条件下的录制，另一个是真实议会辩论中的讲话者录像。

Result: 在理想条件下，多模态大语言模型的情感唤起评分高度可靠，基本无明显的人口统计偏差；但在实际的议会辩论录音中，模型的唤起评分无法达到理想条件下的表现，可能对后续统计推断产生负面影响。

Conclusion: 研究强调了持续深入评估新兴生成式AI方法在政治分析中应用的必要性，并为这种评估提供了一个可复现的合适框架。

Abstract: Emotions are central to politics and analyzing their role in political communication has a long tradition. As research increasingly leverages audio-visual materials to analyze the display of emotions, the emergence of multimodal generative AI promises great advances. However, we lack evidence about the effectiveness of multimodal AI in emotion analysis. This paper addresses this gap by evaluating current multimodal large language models (mLLMs) in video-based analysis of emotional arousal in two complementary data sets of human-labeled video recordings. I find that under ideal circumstances, mLLMs' emotional arousal ratings are highly reliable and show little to know indication of demographic bias. However, in recordings of speakers in real-world parliamentary debates, mLLMs' arousal ratings fail to deliver on this promise with potential negative consequences for downstream statistical inferences. This study therefore underscores the need for continued, thorough evaluation of emerging generative AI methods in political analysis and contributes a suitable replicable framework.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [11] [Asynchronous Reasoning: Training-Free Interactive Thinking LLMs](https://arxiv.org/abs/2512.10931)
*George Yakushev,Nataliia Babina,Masoud Vahid Dastgerdi,Vyacheslav Zhdanovskiy,Alina Shutova,Denis Kuznedelev*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Many state-of-the-art LLMs are trained to think before giving their answer. Reasoning can greatly improve language model capabilities and safety, but it also makes them less interactive: given a new input, a model must stop thinking before it can respond. Real-world use cases such as voice-based or embedded assistants require an LLM agent to respond and adapt to additional information in real time, which is incompatible with sequential interactions. In contrast, humans can listen, think, and act asynchronously: we begin thinking about the problem while reading it and continue thinking while formulating the answer. In this work, we augment LLMs capable of reasoning to operate in a similar way without additional training. Our method uses the properties of rotary embeddings to enable LLMs built for sequential interactions to simultaneously think, listen, and generate outputs. We evaluate our approach on math, commonsense, and safety reasoning and find that it can generate accurate thinking-augmented answers in real time, reducing time to first non-thinking token from minutes to <= 5s. and the overall real-time delays by 6-11x.

</details>


### [12] [Stronger Normalization-Free Transformers](https://arxiv.org/abs/2512.10938)
*Mingzhi Chen,Taiming Lu,Jiachen Zhu,Mingjie Sun,Zhuang Liu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Although normalization layers have long been viewed as indispensable components of deep learning architectures, the recent introduction of Dynamic Tanh (DyT) has demonstrated that alternatives are possible. The point-wise function DyT constrains extreme values for stable convergence and reaches normalization-level performance; this work seeks further for function designs that can surpass it. We first study how the intrinsic properties of point-wise functions influence training and performance. Building on these findings, we conduct a large-scale search for a more effective function design. Through this exploration, we introduce $\mathrm{Derf}(x) = \mathrm{erf}(αx + s)$, where $\mathrm{erf}(x)$ is the rescaled Gaussian cumulative distribution function, and identify it as the most performant design. Derf outperforms LayerNorm, RMSNorm, and DyT across a wide range of domains, including vision (image recognition and generation), speech representation, and DNA sequence modeling. Our findings suggest that the performance gains of Derf largely stem from its improved generalization rather than stronger fitting capacity. Its simplicity and stronger performance make Derf a practical choice for normalization-free Transformer architectures.

</details>


### [13] [UrbanAI 2025 Challenge: Linear vs Transformer Models for Long-Horizon Exogenous Temperature Forecasting](https://arxiv.org/abs/2512.10866)
*Ruslan Gokhman*

Main category: cs.LG

TL;DR: 在仅有历史室内温度数据的长期外生预测任务中，线性模型（尤其是DLinear）在标准数据集划分下比Transformer系列模型表现更好，表明精心设计的线性模型在该挑战性场景中仍是强基线。


<details>
  <summary>Details</summary>
Motivation: 探索在仅使用历史室内温度数据（无外部特征）的长期预测任务中，简单线性模型与复杂Transformer架构的性能对比，验证线性模型在挑战性外生预测场景中的有效性。

Method: 使用标准化训练集、验证集和测试集划分，评估Linear、NLinear、DLinear线性模型与Transformer、Informer、Autoformer等Transformer系列模型在长期外生温度预测任务上的性能。

Result: 线性基线模型（Linear、NLinear、DLinear）在所有数据划分中一致优于更复杂的Transformer架构，其中DLinear在各划分上取得了最佳整体准确率。

Conclusion: 精心设计的线性模型在仅使用历史值的挑战性外生时间序列预测场景中仍具有强大竞争力，可视为该任务的强基线方法。

Abstract: We study long-horizon exogenous-only temperature forecasting - a challenging univariate setting where only the past values of the indoor temperature are used for prediction - using linear and Transformer-family models. We evaluate Linear, NLinear, DLinear, Transformer, Informer, and Autoformer under standardized train, validation, and test splits. Results show that linear baselines (Linear, NLinear, DLinear) consistently outperform more complex Transformer-family architectures, with DLinear achieving the best overall accuracy across all splits. These findings highlight that carefully designed linear models remain strong baselines for time series forecasting in challenging exogenous-only settings.

</details>


### [14] [Guided Transfer Learning for Discrete Diffusion Models](https://arxiv.org/abs/2512.10877)
*Julian Kleutgens,Claudio Battiloro,Lingkai Kong,Benjamin Grewe,Francesca Dominici,Mauricio Tec*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Discrete diffusion models achieve strong performance across language and other discrete domains, providing a powerful alternative to autoregressive models. However, their strong performance relies on large training datasets, which are costly or risky to obtain, especially when adapting to new domains. Transfer learning is the natural way to adapt pretrained discrete diffusion models, but current methods require fine-tuning large diffusion models, which is computationally expensive and often impractical. Building on ratio-based transfer learning for continuous diffusion, we provide Guided Transfer Learning for discrete diffusion models (GTL). This enables sampling from a target distribution without modifying the pretrained denoiser. The same guidance formulation applies to both discrete-time diffusion and continuous-time score-based discrete diffusion, yielding a unified treatment. Guided discrete diffusion often requires many forward passes of the guidance network, which becomes impractical for large vocabularies and long sequences. To address this, we further present an efficient guided sampler that concentrates evaluations on planner-selected positions and top candidate tokens, thus lowering sampling time and computation. This makes guided language modeling practical at scale for large vocabularies and long sequences. We evaluate GTL on sequential data, including synthetic Markov chains and language modeling, and provide empirical analyses of its behavior.

</details>


### [15] [Physics-Informed Learning of Flow Distribution and Receiver Heat Losses in Parabolic Trough Solar Fields](https://arxiv.org/abs/2512.10886)
*Stefan Matthes,Markus Schramm*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Parabolic trough Concentrating Solar Power (CSP) plants operate large hydraulic networks of collector loops that must deliver a uniform outlet temperature despite spatially heterogeneous optical performance, heat losses, and pressure drops. While loop temperatures are measured, loop-level mass flows and receiver heat-loss parameters are unobserved, making it impossible to diagnose hydraulic imbalances or receiver degradation using standard monitoring tools.
  We present a physics-informed learning framework that infers (i) loop-level mass-flow ratios and (ii) time-varying receiver heat-transfer coefficients directly from routine operational data. The method exploits nocturnal homogenization periods -- when hot oil is circulated through a non-irradiated field -- to isolate hydraulic and thermal-loss effects. A differentiable conjugate heat-transfer model is discretized and embedded into an end-to-end learning pipeline optimized using historical plant data from the 50 MW Andasol 3 solar field.
  The model accurately reconstructs loop temperatures (RMSE $<2^\circ$C) and produces physically meaningful estimates of loop imbalances and receiver heat losses. Comparison against drone-based infrared thermography (QScan) shows strong correspondence, correctly identifying all areas with high-loss receivers. This demonstrates that noisy real-world CSP operational data contain enough information to recover latent physical parameters when combined with appropriate modeling and differentiable optimization.

</details>


### [16] [SparseSwaps: Tractable LLM Pruning Mask Refinement at Scale](https://arxiv.org/abs/2512.10922)
*Max Zimmer,Christophe Roux,Moritz Wagner,Deborah Hendrych,Sebastian Pokutta*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The resource requirements of Neural Networks can be significantly reduced through pruning -- the removal of seemingly less important parameters. However, with the rise of Large Language Models (LLMs), full retraining to recover pruning-induced performance degradation is often prohibitive and classical approaches such as global magnitude pruning are suboptimal on Transformer architectures. State-of-the-art methods hence solve a layer-wise mask selection problem, the problem of finding a pruning mask which minimizes the per-layer pruning error on a small set of calibration data. Exactly solving this problem to optimality using Integer Programming (IP) solvers is computationally infeasible due to its combinatorial nature and the size of the search space, and existing approaches therefore rely on approximations or heuristics. In this work, we demonstrate that the mask selection problem can be made drastically more tractable at LLM scale. To that end, we decouple the rows by enforcing equal sparsity levels per row. This allows us to derive optimal 1-swaps (exchanging one kept and one pruned weight) that can be computed efficiently using the Gram matrix of the calibration data. Using these observations, we propose a tractable and simple 1-swap algorithm that warm starts from any pruning mask, runs efficiently on GPUs at LLM scale, and is essentially hyperparameter-free. We demonstrate that our approach reduces per-layer pruning error by up to 60% over Wanda (Sun et al., 2023) and consistently improves perplexity and zero-shot accuracy across state-of-the-art GPT architectures.

</details>


### [17] [Digital Twin Supervised Reinforcement Learning Framework for Autonomous Underwater Navigation](https://arxiv.org/abs/2512.10925)
*Zamirddine Mari,Mohamad Motasem Nawaf,Pierre Drap*

Main category: cs.LG

TL;DR: 论文提出基于PPO算法的深度强化学习方法解决水下机器人BlueROV2的自主导航挑战，包括GPS缺失、能见度差和障碍物等问题，在仿真和真实环境中验证了其优于传统DWA方法的效果及从仿真到现实的迁移能力。


<details>
  <summary>Details</summary>
Motivation: 水下环境自主导航面临GPS缺失、能见度差和水下障碍物等重大挑战，需要开发能够适应复杂水下环境的智能导航方法。

Method: 采用基于Proximal Policy Optimization (PPO)算法的深度强化学习，观测空间结合目标导航信息、虚拟占用网格和操作区域边界的射线投射，并与常用的Dynamic Window Approach (DWA)基准方法进行对比。

Result: PPO策略在高度复杂环境中一致优于DWA，表现出更好的局部适应性和更少的碰撞，同时验证了从仿真环境到物理BlueROV2的迁移能力。

Conclusion: 深度强化学习在水下机器人自主导航中具有实际应用价值，能够有效处理复杂水下环境中的导航挑战。

Abstract: Autonomous navigation in underwater environments remains a major challenge due to the absence of GPS, degraded visibility, and the presence of submerged obstacles. This article investigates these issues through the case of the BlueROV2, an open platform widely used for scientific experimentation. We propose a deep reinforcement learning approach based on the Proximal Policy Optimization (PPO) algorithm, using an observation space that combines target-oriented navigation information, a virtual occupancy grid, and ray-casting along the boundaries of the operational area. The learned policy is compared against a reference deterministic kinematic planner, the Dynamic Window Approach (DWA), commonly employed as a robust baseline for obstacle avoidance. The evaluation is conducted in a realistic simulation environment and complemented by validation on a physical BlueROV2 supervised by a 3D digital twin of the test site, helping to reduce risks associated with real-world experimentation. The results show that the PPO policy consistently outperforms DWA in highly cluttered environments, notably thanks to better local adaptation and reduced collisions. Finally, the experiments demonstrate the transferability of the learned behavior from simulation to the real world, confirming the relevance of deep RL for autonomous navigation in underwater robotics.

</details>


### [18] [Decoupled Q-Chunking](https://arxiv.org/abs/2512.10926)
*Qiyang Li,Seohong Park,Sergey Levine*

Main category: cs.LG

TL;DR: 该论文提出了一种新算法，通过解耦评论家(critic)和策略(policy)的动作块长度来解决分块评论家方法中策略提取的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统TD方法存在自引导偏差问题，分块评论家通过评估短动作序列来加速价值传播，但策略需要以开环方式输出整个动作块，这在需要策略反应性的环境中会产生次优效果，且当块长度增加时难以建模。

Method: 提出解耦分块评论家与策略的块长度，让策略在较短的动作块上运行。通过从原始分块评论家乐观地回溯来构建部分动作块的蒸馏评论家，近似估计当部分动作块扩展为完整块时可实现的最大价值。

Result: 在具有挑战性的长时域离线目标条件任务上进行评估，该方法始终优于先前的方法。

Conclusion: 该方法保留多步价值传播的优势，同时避免了开环次优性和学习长动作块分块策略的困难，为分块评论家框架提供了更灵活的解决方案。

Abstract: Temporal-difference (TD) methods learn state and action values efficiently by bootstrapping from their own future value predictions, but such a self-bootstrapping mechanism is prone to bootstrapping bias, where the errors in the value targets accumulate across steps and result in biased value estimates. Recent work has proposed to use chunked critics, which estimate the value of short action sequences ("chunks") rather than individual actions, speeding up value backup. However, extracting policies from chunked critics is challenging: policies must output the entire action chunk open-loop, which can be sub-optimal for environments that require policy reactivity and also challenging to model especially when the chunk length grows. Our key insight is to decouple the chunk length of the critic from that of the policy, allowing the policy to operate over shorter action chunks. We propose a novel algorithm that achieves this by optimizing the policy against a distilled critic for partial action chunks, constructed by optimistically backing up from the original chunked critic to approximate the maximum value achievable when a partial action chunk is extended to a complete one. This design retains the benefits of multi-step value propagation while sidestepping both the open-loop sub-optimality and the difficulty of learning action chunking policies for long action chunks. We evaluate our method on challenging, long-horizon offline goal-conditioned tasks and show that it reliably outperforms prior methods. Code: github.com/ColinQiyangLi/dqc.

</details>


### [19] [Hierarchical Dataset Selection for High-Quality Data Sharing](https://arxiv.org/abs/2512.10952)
*Xiaona Zhou,Yingyan Zeng,Ran Jin,Ismini Lourentzou*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The success of modern machine learning hinges on access to high-quality training data. In many real-world scenarios, such as acquiring data from public repositories or sharing across institutions, data is naturally organized into discrete datasets that vary in relevance, quality, and utility. Selecting which repositories or institutions to search for useful datasets, and which datasets to incorporate into model training are therefore critical decisions, yet most existing methods select individual samples and treat all data as equally relevant, ignoring differences between datasets and their sources. In this work, we formalize the task of dataset selection: selecting entire datasets from a large, heterogeneous pool to improve downstream performance under resource constraints. We propose Dataset Selection via Hierarchies (DaSH), a dataset selection method that models utility at both dataset and group (e.g., collections, institutions) levels, enabling efficient generalization from limited observations. Across two public benchmarks (Digit-Five and DomainNet), DaSH outperforms state-of-the-art data selection baselines by up to 26.2% in accuracy, while requiring significantly fewer exploration steps. Ablations show DaSH is robust to low-resource settings and lack of relevant datasets, making it suitable for scalable and adaptive dataset selection in practical multi-source learning workflows.

</details>


### [20] [Bidirectional Normalizing Flow: From Data to Noise and Back](https://arxiv.org/abs/2512.10953)
*Yiyang Lu,Qiao Sun,Xianbang Wang,Zhicheng Jiang,Hanhong Zhao,Kaiming He*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Normalizing Flows (NFs) have been established as a principled framework for generative modeling. Standard NFs consist of a forward process and a reverse process: the forward process maps data to noise, while the reverse process generates samples by inverting it. Typical NF forward transformations are constrained by explicit invertibility, ensuring that the reverse process can serve as their exact analytic inverse. Recent developments in TARFlow and its variants have revitalized NF methods by combining Transformers and autoregressive flows, but have also exposed causal decoding as a major bottleneck. In this work, we introduce Bidirectional Normalizing Flow ($\textbf{BiFlow}$), a framework that removes the need for an exact analytic inverse. BiFlow learns a reverse model that approximates the underlying noise-to-data inverse mapping, enabling more flexible loss functions and architectures. Experiments on ImageNet demonstrate that BiFlow, compared to its causal decoding counterpart, improves generation quality while accelerating sampling by up to two orders of magnitude. BiFlow yields state-of-the-art results among NF-based methods and competitive performance among single-evaluation ("1-NFE") methods. Following recent encouraging progress on NFs, we hope our work will draw further attention to this classical paradigm.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [21] [LLMs Can Assist with Proposal Selection at Large User Facilities](https://arxiv.org/abs/2512.10895)
*Lijie Ding,Janell Thomson,Jon Taylor,Changwoo Do*

Main category: cs.AI

TL;DR: 该论文探讨了使用LLMs（大型语言模型）进行提案评审的可行性，在大型用户设施中提供了一种比传统人工评审更具可扩展性、一致性和成本效益的替代方法。


<details>
  <summary>Details</summary>
Motivation: 提案选择过程依赖对提交提案的相对强度评估，但传统人工评分存在提案间相关性弱、易受评审者偏见和不一致性影响的问题。

Method: 采用基于成对偏好的排名方法，利用LLMs处理工作量巨大的成对比较。研究利用了美国橡树岭国家实验室斯波莱申中子源三个光束线上的精心策划的提案和发表记录。

Result: LLMs排名与人类排名高度相关（斯皮尔曼相关系数ρ约0.2-0.8，移除10%异常值后可达≥0.5）。LLMs在识别高发表潜力的提案方面表现不逊于人类评审者，而成本降低了两个数量级。LLMs还能进行高级分析，如通过嵌入模型定量评估提案相似性。

Conclusion: LLMs为大规模提案评审提供了可行、高效且经济的方法，不仅能取代人工评审，还能提供传统方法难以实现的深度分析能力。

Abstract: We explore how large language models (LLMs) can enhance the proposal selection process at large user facilities, offering a scalable, consistent, and cost-effective alternative to traditional human review. Proposal selection depends on assessing the relative strength among submitted proposals; however, traditional human scoring often suffers from weak inter-proposal correlations and is subject to reviewer bias and inconsistency. A pairwise preference-based approach is logically superior, providing a more rigorous and internally consistent basis for ranking, but its quadratic workload makes it impractical for human reviewers. We address this limitation using LLMs. Leveraging the uniquely well-curated proposals and publication records from three beamlines at the Spallation Neutron Source (SNS), Oak Ridge National Laboratory (ORNL), we show that the LLM rankings correlate strongly with the human rankings (Spearman $ρ\simeq 0.2-0.8$, improving to $\geq 0.5$ after 10\% outlier removal). Moreover, LLM performance is no worse than that of human reviewers in identifying proposals with high publication potential, while costing over two orders of magnitude less. Beyond ranking, LLMs enable advanced analyses that are challenging for humans, such as quantitative assessment of proposal similarity via embedding models, which provides information crucial for review committees.

</details>


### [22] [On Decision-Making Agents and Higher-Order Causal Processes](https://arxiv.org/abs/2512.10937)
*Matt Wilson*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We establish a precise correspondence between decision-making agents in partially observable Markov decision processes (POMDPs) and one-input process functions, the classical limit of higher-order quantum operations. In this identification an agent's policy and memory update combine into a process function w that interacts with a POMDP environment via the link product. This suggests a dual interpretation: in the physics view, the process function acts as the environment into which local operations (agent interventions) are inserted, whereas in the AI view it encodes the agent and the inserted functions represent environments. We extend this perspective to multi-agent systems by identifying observation-independent decentralized POMDPs as natural domains for multi-input process functions.

</details>
