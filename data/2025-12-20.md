<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 21]
- [cs.CL](#cs.CL) [Total: 6]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.LG](#cs.LG) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [LinkedOut: Linking World Knowledge Representation Out of Video LLM for Next-Generation Video Recommendation](https://arxiv.org/abs/2512.16891)
*Haichao Zhang,Yao Lu,Lichen Wang,Yunzhe Li,Daiwei Chen,Yunpeng Xu,Yun Fu*

Main category: cs.CV

TL;DR: LinkedOut是首个基于VLLM的视频推荐方法，直接从视频中提取世界知识表示，解决了多视频输入、轻量级和低延迟部署的挑战。


<details>
  <summary>Details</summary>
Motivation: 视频大语言模型具备世界知识感知的视频理解能力，但在部署到下游任务时面临三个挑战：解码生成导致高延迟、典型接口不支持多视频输入、语言输出限制了视觉细节保留。

Method: LinkedOut通过可提示查询和辅助模态，从原始视频帧中提取语义基础的知识感知token，并引入跨层知识融合MoE，选择合适的抽象层次进行个性化、可解释的低延迟推荐。

Result: 方法在标准基准测试中达到最先进性能，无需手工标注只使用原始帧，可解释性研究和消融实验验证了层多样性和分层融合的优势。

Conclusion: LinkedOut为充分利用VLLM世界知识先验和视觉推理开辟了实用路径，适用于推荐等下游视觉任务。

Abstract: Video Large Language Models (VLLMs) unlock world-knowledge-aware video understanding through pretraining on internet-scale data and have already shown promise on tasks such as movie analysis and video question answering. However, deploying VLLMs for downstream tasks such as video recommendation remains challenging, since real systems require multi-video inputs, lightweight backbones, low-latency sequential inference, and rapid response. In practice, (1) decode-only generation yields high latency for sequential inference, (2) typical interfaces do not support multi-video inputs, and (3) constraining outputs to language discards fine-grained visual details that matter for downstream vision tasks. We argue that these limitations stem from the absence of a representation that preserves pixel-level detail while leveraging world knowledge. We present LinkedOut, a representation that extracts VLLM world knowledge directly from video to enable fast inference, supports multi-video histories, and removes the language bottleneck. LinkedOut extracts semantically grounded, knowledge-aware tokens from raw frames using VLLMs, guided by promptable queries and optional auxiliary modalities. We introduce a cross-layer knowledge fusion MoE that selects the appropriate level of abstraction from the rich VLLM features, enabling personalized, interpretable, and low-latency recommendation. To our knowledge, LinkedOut is the first VLLM-based video recommendation method that operates on raw frames without handcrafted labels, achieving state-of-the-art results on standard benchmarks. Interpretability studies and ablations confirm the benefits of layer diversity and layer-wise fusion, pointing to a practical path that fully leverages VLLM world-knowledge priors and visual reasoning for downstream vision tasks such as recommendation.

</details>


### [2] [GeoPredict: Leveraging Predictive Kinematics and 3D Gaussian Geometry for Precise VLA Manipulation](https://arxiv.org/abs/2512.16811)
*Jingjing Qian,Boyao Han,Chen Shi,Lei Xiao,Long Yang,Shaoshuai Shi,Li Jiang*

Main category: cs.CV

TL;DR: GeoPredict通过添加预测性运动学和几何先验增强连续动作策略，在需要精确3D推理的任务中显著提升VLA模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的Vision-Language-Action模型虽在机器人操作中表现出良好的泛化能力，但主要基于2D且反应式，在需要精确3D几何推理的任务中可靠性不足。

Method: 提出几何感知的VLA框架GeoPredict，包含：1）轨迹级模块编码运动历史并预测机器人手臂的多步3D关键点轨迹；2）预测性3D高斯几何模块沿未来关键点轨迹预测工作空间几何形状并进行跟踪引导精炼。预测模块仅在训练时通过基于深度的渲染作为监督，推理时只需轻量级查询标记而不涉及3D解码。

Result: 在RoboCasa Human-50、LIBERO和真实世界操作任务上的实验表明，GeoPredict持续优于强力VLA基线，特别是在几何密集和空间要求高的场景中。

Conclusion: GeoPredict通过融入预测性运动学和几何先验，成功提升了VLA模型在3D推理密集型任务中的性能，为机器人操作提供了更可靠的空间理解能力。

Abstract: Vision-Language-Action (VLA) models achieve strong generalization in robotic manipulation but remain largely reactive and 2D-centric, making them unreliable in tasks that require precise 3D reasoning. We propose GeoPredict, a geometry-aware VLA framework that augments a continuous-action policy with predictive kinematic and geometric priors. GeoPredict introduces a trajectory-level module that encodes motion history and predicts multi-step 3D keypoint trajectories of robot arms, and a predictive 3D Gaussian geometry module that forecasts workspace geometry with track-guided refinement along future keypoint trajectories. These predictive modules serve exclusively as training-time supervision through depth-based rendering, while inference requires only lightweight additional query tokens without invoking any 3D decoding. Experiments on RoboCasa Human-50, LIBERO, and real-world manipulation tasks show that GeoPredict consistently outperforms strong VLA baselines, especially in geometry-intensive and spatially demanding scenarios.

</details>


### [3] [Next-Generation License Plate Detection and Recognition System using YOLOv8](https://arxiv.org/abs/2512.16826)
*Arslan Amin,Rafia Mumtaz,Muhammad Jawad Bashir,Syed Mohammad Hassan Zaidi*

Main category: cs.CV

TL;DR: 本文研究了YOLOv8变体在车牌识别和字符识别任务中的表现，提出了一个优化的识别流水线，在保持计算效率的同时实现了高精度。


<details>
  <summary>Details</summary>
Motivation: 在交通管理和车辆监控领域，高效的车牌检测和识别至关重要，但现有方法在多样化环境中难以保持实时准确性。本研究旨在通过YOLOv8改进车牌识别系统的性能，推动智能交通系统的发展。

Method: 使用两个独立数据集训练和评估YOLOv8变体；YOLOv8 Nano用于车牌检测，YOLOv8 Small用于字符识别；提出基于x轴位置的字符排序方法；构建优化的检测-识别流水线。

Result: YOLOv8 Nano在车牌识别任务上达到0.964精度和0.918 mAP50；YOLOv8 Small在字符识别任务上达到0.92精度和0.91 mAP50；提出的字符排序方法和流水线在保持效率的同时确保了高准确性。

Conclusion: 研究的优化流水线为边缘设备上的智能交通系统应用提供了坚实基础，标志着向更智能、高效的城市基础设施迈出了重要一步。

Abstract: In the evolving landscape of traffic management and vehicle surveillance, efficient license plate detection and recognition are indispensable. Historically, many methodologies have tackled this challenge, but consistent real-time accuracy, especially in diverse environments, remains elusive. This study examines the performance of YOLOv8 variants on License Plate Recognition (LPR) and Character Recognition tasks, crucial for advancing Intelligent Transportation Systems. Two distinct datasets were employed for training and evaluation, yielding notable findings. The YOLOv8 Nano variant demonstrated a precision of 0.964 and mAP50 of 0.918 on the LPR task, while the YOLOv8 Small variant exhibited a precision of 0.92 and mAP50 of 0.91 on the Character Recognition task. A custom method for character sequencing was introduced, effectively sequencing the detected characters based on their x-axis positions. An optimized pipeline, utilizing YOLOv8 Nano for LPR and YOLOv8 Small for Character Recognition, is proposed. This configuration not only maintains computational efficiency but also ensures high accuracy, establishing a robust foundation for future real-world deployments on edge devices within Intelligent Transportation Systems. This effort marks a significant stride towards the development of smarter and more efficient urban infrastructures.

</details>


### [4] [Radiology Report Generation with Layer-Wise Anatomical Attention](https://arxiv.org/abs/2512.16841)
*Emmanuel D. Muñiz-De-León,Jorge A. Rosales-de-Golferichs,Ana S. Muñoz-Rodríguez,Alejandro I. Trejo-Castro,Eduardo de Avila-Armenta,Antonio Martínez-Torteya*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Automatic radiology report generation is a promising application of multimodal deep learning, aiming to reduce reporting workload and improve consistency. However, current state-of-the-art (SOTA) systems - such as Multimodal AI for Radiology Applications (MAIRA-2) and Medical Pathways Language Model-Multimodal (MedPaLM-M) - depend on large-scale multimodal training, clinical metadata, and multiple imaging views, making them resource-intensive and inaccessible for most settings. We introduce a compact image-to-text architecture that generates the Findings section of chest X-ray reports from a single frontal image. The model combines a frozen Self-Distillation with No Labels v3 (DINOv3) Vision Transformer (ViT) encoder with a Generative Pre-trained Transformer 2 (GPT-2) decoder enhanced by layer-wise anatomical attention. This mechanism integrates lung and heart segmentation masks through hierarchical Gaussian smoothing, biasing attention toward clinically relevant regions without adding trainable parameters. Evaluated on the official Medical Information Mart for Intensive Care-Chest X-ray (MIMIC-CXR) dataset using Chest Radiograph Expert (CheXpert) and Radiology Graph (RadGraph) metrics, our approach achieved substantial gains: CheXpert Macro-F1 for five key pathologies increased by 168% (0.083 -> 0.238) and Micro-F1 by 146% (0.137 -> 0.337), while broader performance across 14 observations improved by 86% (0.170 -> 0.316). Structural coherence also improved, with RadGraph F1 rising by 9.7%. Despite its small size and purely image-conditioned design, the model demonstrates that decoder-level anatomical guidance improves spatial grounding and enhances coherence in clinically relevant regions. The source code is publicly available at: https://github.com/devMuniz02/UDEM-CXR-Reporting-Thesis-2025.

</details>


### [5] [OPENTOUCH: Bringing Full-Hand Touch to Real-World Interaction](https://arxiv.org/abs/2512.16842)
*Yuxin Ray Song,Jinzhou Li,Rao Fu,Devin Murphy,Kaichen Zhou,Rishi Shiv,Yaqi Li,Haoyu Xiong,Crystal Elaine Owens,Yilun Du,Yiyue Luo,Xianyi Cheng,Antonio Torralba,Wojciech Matusik,Paul Pu Liang*

Main category: cs.CV

TL;DR: OpenTouch：首个野外环境下包含视频-触觉-姿态同步数据的自我中心全手触觉数据集，包含5.1小时数据和2900个带文本标注的片段，用于研究触觉如何基于感知和动作，并建立了检索和分类基准。


<details>
  <summary>Details</summary>
Motivation: 人类手部是我们与物理世界的主要接口，但自我中心感知很少知道接触何时、何地、以多大力量发生。现有的可穿戴触觉传感器稀缺，且没有野外数据集能对齐第一人称视频和全手触摸数据。为弥合视觉感知与物理交互之间的差距，需要创建这样的数据集。

Method: 开发了OpenTouch数据集，包含同步的视频-触觉-姿态数据。使用该数据集建立了检索和分类基准，用于探究触觉如何作为感知和动作的基础。方法包括：1. 数据收集：5.1小时同步数据；2. 数据标注：2900个片段配详细文本注释；3. 基准测试：检索任务（从野外视频查询中可靠检索触觉信号）和分类任务（研究触觉作为掌握理解的紧凑而强大的线索）。

Result: 显示触觉信号为抓握理解提供了紧凑而强大的线索，增强了跨模态对齐，并且可以从野外视频查询中可靠地检索。具体结果包括：触觉有助于理解接触事件的时间和位置，以及接触力的强度；触觉信息改善了视频与物理交互之间的对齐；建立的基准提供了量化评估触觉感知能力的方法。

Conclusion: 通过发布带标注的视觉-触觉-姿态数据集和基准，旨在推进多模态自我中心感知、体现学习和接触丰富的机器人操作。OpenTouch数据集填补了现有研究空白，为理解触觉在人类与物理世界交互中的作用提供了重要资源。

Abstract: The human hand is our primary interface to the physical world, yet egocentric perception rarely knows when, where, or how forcefully it makes contact. Robust wearable tactile sensors are scarce, and no existing in-the-wild datasets align first-person video with full-hand touch. To bridge the gap between visual perception and physical interaction, we present OpenTouch, the first in-the-wild egocentric full-hand tactile dataset, containing 5.1 hours of synchronized video-touch-pose data and 2,900 curated clips with detailed text annotations. Using OpenTouch, we introduce retrieval and classification benchmarks that probe how touch grounds perception and action. We show that tactile signals provide a compact yet powerful cue for grasp understanding, strengthen cross-modal alignment, and can be reliably retrieved from in-the-wild video queries. By releasing this annotated vision-touch-pose dataset and benchmark, we aim to advance multimodal egocentric perception, embodied learning, and contact-rich robotic manipulation.

</details>


### [6] [GenEval 2: Addressing Benchmark Drift in Text-to-Image Evaluation](https://arxiv.org/abs/2512.16853)
*Amita Kamath,Kai-Wei Chang,Ranjay Krishna,Luke Zettlemoyer,Yushi Hu,Marjan Ghazvininejad*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Automating Text-to-Image (T2I) model evaluation is challenging; a judge model must be used to score correctness, and test prompts must be selected to be challenging for current T2I models but not the judge. We argue that satisfying these constraints can lead to benchmark drift over time, where the static benchmark judges fail to keep up with newer model capabilities. We show that benchmark drift is a significant problem for GenEval, one of the most popular T2I benchmarks. Although GenEval was well-aligned with human judgment at the time of its release, it has drifted far from human judgment over time -- resulting in an absolute error of as much as 17.7% for current models. This level of drift strongly suggests that GenEval has been saturated for some time, as we verify via a large-scale human study. To help fill this benchmarking gap, we introduce a new benchmark, GenEval 2, with improved coverage of primitive visual concepts and higher degrees of compositionality, which we show is more challenging for current models. We also introduce Soft-TIFA, an evaluation method for GenEval 2 that combines judgments for visual primitives, which we show is more well-aligned with human judgment and argue is less likely to drift from human-alignment over time (as compared to more holistic judges such as VQAScore). Although we hope GenEval 2 will provide a strong benchmark for many years, avoiding benchmark drift is far from guaranteed and our work, more generally, highlights the importance of continual audits and improvement for T2I and related automated model evaluation benchmarks.

</details>


### [7] [RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing](https://arxiv.org/abs/2512.16864)
*Tianyuan Qu,Lei Ke,Xiaohang Zhan,Longxiang Tang,Yuqi Liu,Bohao Peng,Bei Yu,Dong Yu,Jiaya Jia*

Main category: cs.CV

TL;DR: RePlan是一个解决复杂指令-视觉编辑任务的框架：通过区域对齐规划分解指令和增强区域标注，再使用免训练注意力注入机制进行扩散式精确编辑。该框架在复杂场景和知识密集型编辑任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于指令的图像编辑方法在处理复杂指令（涉及多步骤、精细对象操作）时面临困难，特别当视觉场景混乱或模糊时，现有模型难以精准对齐视觉区域与语言指令。因此，需要一种能分解复杂指令并精确映射到目标区域的方法。

Method: RePlan采用“规划-执行”框架，结合视觉语言规划器与扩散编辑器。规划器使用逐步分解推理（ReAct格式）将复杂指令拆分为步骤，并显式标注目标区域。编辑器通过注意机制进行区域对齐，采用无需训练的注意力区域注入方式，支持并行多区域编辑，避免了迭代修复。

Result: 在IV-Edit基准测试中，RePlan的F1-分割准确率（78.8%）和平均指令覆盖率（95.7%）均优于基准模型。用户偏好评估显示用户明显倾向于RePlan的输出，尤其是在区域准确性和全局一致性方面优于SDXL/SDXL-Instruct、InstructPix2Pix等模型。

Conclusion: RePlan为复杂、知识密集型的指令图像编辑提供了解决思路，通过先规划、再执行，实现精确的区域对齐编辑。尽管在特定场景下仍然存在局限，但该框架为未来研究提供了有益的基础。

Abstract: Instruction-based image editing enables natural-language control over visual modifications, yet existing models falter under Instruction-Visual Complexity (IV-Complexity), where intricate instructions meet cluttered or ambiguous scenes. We introduce RePlan (Region-aligned Planning), a plan-then-execute framework that couples a vision-language planner with a diffusion editor. The planner decomposes instructions via step-by-step reasoning and explicitly grounds them to target regions; the editor then applies changes using a training-free attention-region injection mechanism, enabling precise, parallel multi-region edits without iterative inpainting. To strengthen planning, we apply GRPO-based reinforcement learning using 1K instruction-only examples, yielding substantial gains in reasoning fidelity and format reliability. We further present IV-Edit, a benchmark focused on fine-grained grounding and knowledge-intensive edits. Across IV-Complex settings, RePlan consistently outperforms strong baselines trained on far larger datasets, improving regional precision and overall fidelity. Our project page: https://replan-iv-edit.github.io

</details>


### [8] [Memory-Enhanced SAM3 for Occlusion-Robust Surgical Instrument Segmentation](https://arxiv.org/abs/2512.16880)
*Valay Bundele,Mehran Hosseinzadeh,Hendrik P. A. Lensch*

Main category: cs.CV

TL;DR: 针对手术内镜视频中的器械分割任务，该研究提出了ReMeDI-SAM3方法，它是对SAM3框架的零训练增强扩展。通过引入相关性感知的内存过滤、分段插值方案和基于特征的再识别模块，显著提升了在遮挡、运动等挑战下的分割性能。


<details>
  <summary>Details</summary>
Motivation: 虽然SAM3为视频对象分割提供了强大的时空框架，但在手术场景中的性能受到以下限制：内存更新缺乏判别性、内存容量固定、以及遮挡后身份恢复能力弱。这些限制导致在手术内镜视频中准确分割器械仍面临挑战，尤其是存在频繁遮挡、快速运动、镜面伪影和长期器械重新进入等情况。

Method: 提出了ReMeDI-SAM3方法，包含三个核心组件：(1) 相关性感知内存过滤，带有专门的遮挡感知内存来存储遮挡前帧；(2) 分段插值方案，扩大有效内存容量；(3) 基于特征的再识别模块，结合时间投票机制，实现可靠的遮挡后身份消歧。这些组件共同工作，减少错误累积并支持遮挡后恢复。

Result: 在EndoVis17和EndoVis18数据集上的零样本评估显示，相对于原始SAM3，平均交并比(mcIoU)分别提升了约7%和16%的绝对改进。该方法甚至超过了先前基于训练的方法，在手术器械分割任务中表现出显著优势。

Conclusion: ReMeDI-SAM3通过增强的内存管理和特征重识别机制，有效地解决了SAM3在手术场景中的局限性。该方法的零样本特性使其在实际应用中具有实用价值，为计算机辅助手术干预中的精确器械分割提供了有前景的解决方案。

Abstract: Accurate surgical instrument segmentation in endoscopic videos is crucial for computer-assisted interventions, yet remains challenging due to frequent occlusions, rapid motion, specular artefacts, and long-term instrument re-entry. While SAM3 provides a powerful spatio-temporal framework for video object segmentation, its performance in surgical scenes is limited by indiscriminate memory updates, fixed memory capacity, and weak identity recovery after occlusions. We propose ReMeDI-SAM3, a training-free memory-enhanced extension of SAM3, that addresses these limitations through three components: (i) relevance-aware memory filtering with a dedicated occlusion-aware memory for storing pre-occlusion frames, (ii) a piecewise interpolation scheme that expands the effective memory capacity, and (iii) a feature-based re-identification module with temporal voting for reliable post-occlusion identity disambiguation. Together, these components mitigate error accumulation and enable reliable recovery after occlusions. Evaluations on EndoVis17 and EndoVis18 under a zero-shot setting show absolute mcIoU improvements of around 7% and 16%, respectively, over vanilla SAM3, outperforming even prior training-based approaches. Project page: https://valaybundele.github.io/remedi-sam3/.

</details>


### [9] [M-PhyGs: Multi-Material Object Dynamics from Video](https://arxiv.org/abs/2512.16885)
*Norika Wada,Kohei Yamashita,Ryo Kawahara,Ko Nishino*

Main category: cs.CV

TL;DR: 该论文提出了一种从视频中估计多材质复杂自然物体材料成分和物理参数的方法，以花朵为例进行验证。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设物体是单材质的，且需预知动力学模型或拓扑结构简单，这限制了其在真实复杂多材质物体上的应用，因而需要新的方法来处理自然物体的材料组成和物理特性估计。

Method: 提出了Multi-material Physical Gaussians (M-PhyGs)方法，利用级联的3D和2D损失函数以及时间小批量处理，从自然场景下的短视频中同时分割材料并恢复其连续介质力学参数，且考虑了重力影响。

Result: 在提出的Phlowers数据集上进行实验，证明了M-PhyGs方法及其组件的准确性和有效性。

Conclusion: M-PhyGs为从视频中精确估计多材质复杂自然物体的物理材料参数提供了一种有效的解决方案，为这一具有挑战性的任务提供了新的评估平台。

Abstract: Knowledge of the physical material properties governing the dynamics of a real-world object becomes necessary to accurately anticipate its response to unseen interactions. Existing methods for estimating such physical material parameters from visual data assume homogeneous single-material objects, pre-learned dynamics, or simplistic topologies. Real-world objects, however, are often complex in material composition and geometry lying outside the realm of these assumptions. In this paper, we particularly focus on flowers as a representative common object. We introduce Multi-material Physical Gaussians (M-PhyGs) to estimate the material composition and parameters of such multi-material complex natural objects from video. From a short video captured in a natural setting, M-PhyGs jointly segments the object into similar materials and recovers their continuum mechanical parameters while accounting for gravity. M-PhyGs achieves this efficiently with newly introduced cascaded 3D and 2D losses, and by leveraging temporal mini-batching. We introduce a dataset, Phlowers, of people interacting with flowers as a novel platform to evaluate the accuracy of this challenging task of multi-material physical parameter estimation. Experimental results on Phlowers dataset demonstrate the accuracy and effectiveness of M-PhyGs and its components.

</details>


### [10] [FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction](https://arxiv.org/abs/2512.16900)
*Shuyuan Tu,Yueming Pan,Yinming Huang,Xintong Han,Zhen Xing,Qi Dai,Kai Qiu,Chong Luo,Zuxuan Wu*

Main category: cs.CV

TL;DR: FlashPortrait是一种端到端的视频扩散Transformer模型，能够合成保持身份一致性的无限长度人像动画视频，同时实现高达6倍的推理加速。


<details>
  <summary>Details</summary>
Motivation: 当前的基于扩散模型的长人像动画加速方法难以确保身份一致性，因此需要开发能够同时保持身份一致性和实现加速的方法。

Method: 1. 使用现成提取器计算身份无关的面部表情特征；2. 引入标准化面部表情块，通过均值和方差归一化将面部特征与扩散潜变量对齐，改善面部建模中的身份稳定性；3. 采用动态滑动窗口方案，在重叠区域使用加权混合确保长动画中的平滑过渡和身份一致性；4. 基于特定时间步的潜变量变化率和扩散层间导数幅度比，利用当前时间步的高阶潜变量导数直接预测未来时间步的潜变量，跳过多个去噪步骤实现加速。

Result: 在基准测试上的实验表明，FlashPortrait在定性和定量评估中都表现出有效性，能够合成保持身份一致性的长人像视频，同时实现高达6倍的推理加速。

Conclusion: FlashPortrait通过引入标准化面部特征对齐、动态滑动窗口方案和基于高阶导数的加速策略，成功解决了长人像动画中的身份一致性保持问题，并实现了显著的推理速度提升，为视频合成领域提供了一种有效的解决方案。

Abstract: Current diffusion-based acceleration methods for long-portrait animation struggle to ensure identity (ID) consistency. This paper presents FlashPortrait, an end-to-end video diffusion transformer capable of synthesizing ID-preserving, infinite-length videos while achieving up to 6x acceleration in inference speed. In particular, FlashPortrait begins by computing the identity-agnostic facial expression features with an off-the-shelf extractor. It then introduces a Normalized Facial Expression Block to align facial features with diffusion latents by normalizing them with their respective means and variances, thereby improving identity stability in facial modeling. During inference, FlashPortrait adopts a dynamic sliding-window scheme with weighted blending in overlapping areas, ensuring smooth transitions and ID consistency in long animations. In each context window, based on the latent variation rate at particular timesteps and the derivative magnitude ratio among diffusion layers, FlashPortrait utilizes higher-order latent derivatives at the current timestep to directly predict latents at future timesteps, thereby skipping several denoising steps and achieving 6x speed acceleration. Experiments on benchmarks show the effectiveness of FlashPortrait both qualitatively and quantitatively.

</details>


### [11] [Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection](https://arxiv.org/abs/2512.16905)
*Kaixin Ding,Yang Zhou,Xi Chen,Miao Yang,Jiarong Ou,Rui Chen,Xin Tao,Hengshuang Zhao*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent advances in Text-to-Image (T2I) generative models, such as Imagen, Stable Diffusion, and FLUX, have led to remarkable improvements in visual quality. However, their performance is fundamentally limited by the quality of training data. Web-crawled and synthetic image datasets often contain low-quality or redundant samples, which lead to degraded visual fidelity, unstable training, and inefficient computation. Hence, effective data selection is crucial for improving data efficiency. Existing approaches rely on costly manual curation or heuristic scoring based on single-dimensional features in Text-to-Image data filtering. Although meta-learning based method has been explored in LLM, there is no adaptation for image modalities. To this end, we propose **Alchemist**, a meta-gradient-based framework to select a suitable subset from large-scale text-image data pairs. Our approach automatically learns to assess the influence of each sample by iteratively optimizing the model from a data-centric perspective. Alchemist consists of two key stages: data rating and data pruning. We train a lightweight rater to estimate each sample's influence based on gradient information, enhanced with multi-granularity perception. We then use the Shift-Gsampling strategy to select informative subsets for efficient model training. Alchemist is the first automatic, scalable, meta-gradient-based data selection framework for Text-to-Image model training. Experiments on both synthetic and web-crawled datasets demonstrate that Alchemist consistently improves visual quality and downstream performance. Training on an Alchemist-selected 50% of the data can outperform training on the full dataset.

</details>


### [12] [VIVA: VLM-Guided Instruction-Based Video Editing with Reward Optimization](https://arxiv.org/abs/2512.16906)
*Xiaoyan Cong,Haotian Yang,Angtian Wang,Yizhi Wang,Yiding Yang,Canyu Zhang,Chongyang Ma*

Main category: cs.CV

TL;DR: VIVA提出一个基于指令的视频编辑框架，通过VLM引导编码和奖励优化，解决现有方法在泛化复杂真实指令方面的限制。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的视频编辑方法通常基于简单编辑操作的配对数据训练，难以泛化到多样复杂的真实世界指令。

Method: 使用VLM引导编码将文本指令、视频首帧和可选参考图像编码为视觉基础指令表示；采用后训练阶段Edit-GRPO，通过相对奖励优化模型，确保指令忠实、内容保留和美学效果；并设计了合成多样高质量配对视频-指令数据的流程。

Result: 大量实验表明，VIVA在指令遵循、泛化能力和编辑质量上优于最先进方法。

Conclusion: VIVA通过视觉语言模型和强化学习奖励优化，实现了对复杂指令更具泛化能力的高质量视频编辑。

Abstract: Instruction-based video editing aims to modify an input video according to a natural-language instruction while preserving content fidelity and temporal coherence. However, existing diffusion-based approaches are often trained on paired data of simple editing operations, which fundamentally limits their ability to generalize to diverse and complex, real-world instructions. To address this generalization gap, we propose VIVA, a scalable framework for instruction-based video editing that leverages VLM-guided encoding and reward optimization. First, we introduce a VLM-based instructor that encodes the textual instruction, the first frame of the source video, and an optional reference image into visually-grounded instruction representations, providing fine-grained spatial and semantic context for the diffusion transformer backbone. Second, we propose a post-training stage, Edit-GRPO, which adapts Group Relative Policy Optimization to the domain of video editing, directly optimizing the model for instruction-faithful, content-preserving, and aesthetically pleasing edits using relative rewards. Furthermore, we propose a data construction pipeline designed to synthetically generate diverse, high-fidelity paired video-instruction data of basic editing operations. Extensive experiments show that VIVA achieves superior instruction following, generalization, and editing quality over state-of-the-art methods. Website: https://viva-paper.github.io

</details>


### [13] [Flowing from Reasoning to Motion: Learning 3D Hand Trajectory Prediction from Egocentric Human Interaction Videos](https://arxiv.org/abs/2512.16907)
*Mingfei Chen,Yifan Wang,Zhengqin Li,Homanga Bharadhwaj,Yujin Chen,Chuan Qin,Ziyi Kou,Yuan Tian,Eric Whitmire,Rajinder Sodhi,Hrvoje Benko,Eli Shlizerman,Yue Liu*

Main category: cs.CV

TL;DR: 该研究提出了EgoMAN数据集和模型，解决现有3D手部轨迹预测中语义推理与动作生成脱节的问题。


<details>
  <summary>Details</summary>
Motivation: 现有3D手部轨迹预测研究存在两个主要限制：数据集将动作与语义监督分离，模型对推理和动作的联系较弱。为解决这些问题，研究者旨在开发一个能够紧密连接视觉语言推理和动作生成的系统。

Method: 首先创建EgoMAN大规模第一人称数据集，包含219K个6DoF轨迹和300万个结构化问答对。然后提出EgoMAN模型，这是一个推理到动作的框架，通过轨迹标记接口连接视觉语言推理和动作生成。模型采用渐进训练方法，使推理结果与动作动态对齐。

Result: 该方法能够生成准确且阶段感知的轨迹，在真实场景中展现出良好的泛化能力。

Conclusion: 通过结合大规模语义丰富的数据集和紧密连接推理与动作的模型架构，该研究在3D手部轨迹预测领域取得了显著进展，实现了语义推理与动作生成的深度融合。

Abstract: Prior works on 3D hand trajectory prediction are constrained by datasets that decouple motion from semantic supervision and by models that weakly link reasoning and action. To address these, we first present the EgoMAN dataset, a large-scale egocentric dataset for interaction stage-aware 3D hand trajectory prediction with 219K 6DoF trajectories and 3M structured QA pairs for semantic, spatial, and motion reasoning. We then introduce the EgoMAN model, a reasoning-to-motion framework that links vision-language reasoning and motion generation via a trajectory-token interface. Trained progressively to align reasoning with motion dynamics, our approach yields accurate and stage-aware trajectories with generalization across real-world scenes.

</details>


### [14] [SceneDiff: A Benchmark and Method for Multiview Object Change Detection](https://arxiv.org/abs/2512.16908)
*Yuqun Wu,Chih-hao Lin,Henry Che,Aditi Tiwari,Chuhang Zou,Shenlong Wang,Derek Hoiem*

Main category: cs.CV

TL;DR: 该论文提出了用于多视角场景变化检测的新基准（SceneDiff）和方法（SceneDiff方法），在识别对象级变化方面取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 多视角变化检测对机器人整理、施工监控等应用很重要，但现有方法受视角差异影响大，且缺乏对象级标注基准。

Method: 提出训练免费方法：1）3D对齐捕获场景，2）提取对象区域，3）比较空间和语义特征检测变化。利用预训练的3D、分割和图像编码模型。

Result: 在多视角和双视角基准测试中，相对AP分别提升94%和37.4%，大幅超越现有方法。

Conclusion: SceneDiff基准和方法为多视角对象变化检测提供了有效解决方案，基准和代码将公开。

Abstract: We investigate the problem of identifying objects that have been added, removed, or moved between a pair of captures (images or videos) of the same scene at different times. Detecting such changes is important for many applications, such as robotic tidying or construction progress and safety monitoring. A major challenge is that varying viewpoints can cause objects to falsely appear changed. We introduce SceneDiff Benchmark, the first multiview change detection benchmark with object instance annotations, comprising 350 diverse video pairs with thousands of changed objects. We also introduce the SceneDiff method, a new training-free approach for multiview object change detection that leverages pretrained 3D, segmentation, and image encoding models to robustly predict across multiple benchmarks. Our method aligns the captures in 3D, extracts object regions, and compares spatial and semantic region features to detect changes. Experiments on multi-view and two-view benchmarks demonstrate that our method outperforms existing approaches by large margins (94% and 37.4% relative AP improvements). The benchmark and code will be publicly released.

</details>


### [15] [MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning](https://arxiv.org/abs/2512.16909)
*Yuanchen Ju,Yongyuan Liang,Yen-Jen Wang,Nandiraju Gireesh,Yuanliang Ju,Seungjae Lee,Qiao Gu,Elvis Hsieh,Furong Huang,Koushil Sreenath*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Mobile manipulators in households must both navigate and manipulate. This requires a compact, semantically rich scene representation that captures where objects are, how they function, and which parts are actionable. Scene graphs are a natural choice, yet prior work often separates spatial and functional relations, treats scenes as static snapshots without object states or temporal updates, and overlooks information most relevant for accomplishing the current task. To address these limitations, we introduce MomaGraph, a unified scene representation for embodied agents that integrates spatial-functional relationships and part-level interactive elements. However, advancing such a representation requires both suitable data and rigorous evaluation, which have been largely missing. We thus contribute MomaGraph-Scenes, the first large-scale dataset of richly annotated, task-driven scene graphs in household environments, along with MomaGraph-Bench, a systematic evaluation suite spanning six reasoning capabilities from high-level planning to fine-grained scene understanding. Built upon this foundation, we further develop MomaGraph-R1, a 7B vision-language model trained with reinforcement learning on MomaGraph-Scenes. MomaGraph-R1 predicts task-oriented scene graphs and serves as a zero-shot task planner under a Graph-then-Plan framework. Extensive experiments demonstrate that our model achieves state-of-the-art results among open-source models, reaching 71.6% accuracy on the benchmark (+11.4% over the best baseline), while generalizing across public benchmarks and transferring effectively to real-robot experiments.

</details>


### [16] [SFTok: Bridging the Performance Gap in Discrete Tokenizers](https://arxiv.org/abs/2512.16910)
*Qihang Rao,Borui Zhang,Wenzhao Zheng,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent advances in multimodal models highlight the pivotal role of image tokenization in high-resolution image generation. By compressing images into compact latent representations, tokenizers enable generative models to operate in lower-dimensional spaces, thereby improving computational efficiency and reducing complexity. Discrete tokenizers naturally align with the autoregressive paradigm but still lag behind continuous ones, limiting their adoption in multimodal systems. To address this, we propose \textbf{SFTok}, a discrete tokenizer that incorporates a multi-step iterative mechanism for precise reconstruction. By integrating \textbf{self-forcing guided visual reconstruction} and \textbf{debias-and-fitting training strategy}, SFTok resolves the training-inference inconsistency in multi-step process, significantly enhancing image reconstruction quality. At a high compression rate of only 64 tokens per image, SFTok achieves state-of-the-art reconstruction quality on ImageNet (rFID = 1.21) and demonstrates exceptional performance in class-to-image generation tasks (gFID = 2.29).

</details>


### [17] [AdaTooler-V: Adaptive Tool-Use for Images and Videos](https://arxiv.org/abs/2512.16918)
*Chaoyang Wang,Kaituo Feng,Dongyang Chen,Zhongyu Wang,Zhixun Li,Sicheng Gao,Meng Meng,Xu Zhou,Manyuan Zhang,Yuzhang Shang,Xiangyu Yue*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent advances have shown that multimodal large language models (MLLMs) benefit from multimodal interleaved chain-of-thought (CoT) with vision tool interactions. However, existing open-source models often exhibit blind tool-use reasoning patterns, invoking vision tools even when they are unnecessary, which significantly increases inference overhead and degrades model performance. To this end, we propose AdaTooler-V, an MLLM that performs adaptive tool-use by determining whether a visual problem truly requires tools. First, we introduce AT-GRPO, a reinforcement learning algorithm that adaptively adjusts reward scales based on the Tool Benefit Score of each sample, encouraging the model to invoke tools only when they provide genuine improvements. Moreover, we construct two datasets to support training: AdaTooler-V-CoT-100k for SFT cold start and AdaTooler-V-300k for RL with verifiable rewards across single-image, multi-image, and video data. Experiments across twelve benchmarks demonstrate the strong reasoning capability of AdaTooler-V, outperforming existing methods in diverse visual reasoning tasks. Notably, AdaTooler-V-7B achieves an accuracy of 89.8\% on the high-resolution benchmark V*, surpassing the commercial proprietary model GPT-4o and Gemini 1.5 Pro. All code, models, and data are released.

</details>


### [18] [EasyV2V: A High-quality Instruction-based Video Editing Framework](https://arxiv.org/abs/2512.16920)
*Jinjie Mai,Chaoyang Wang,Guocheng Gordon Qian,Willi Menapace,Sergey Tulyakov,Bernard Ghanem,Peter Wonka,Ashkan Mirzaei*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: While image editing has advanced rapidly, video editing remains less explored, facing challenges in consistency, control, and generalization. We study the design space of data, architecture, and control, and introduce \emph{EasyV2V}, a simple and effective framework for instruction-based video editing. On the data side, we compose existing experts with fast inverses to build diverse video pairs, lift image edit pairs into videos via single-frame supervision and pseudo pairs with shared affine motion, mine dense-captioned clips for video pairs, and add transition supervision to teach how edits unfold. On the model side, we observe that pretrained text-to-video models possess editing capability, motivating a simplified design. Simple sequence concatenation for conditioning with light LoRA fine-tuning suffices to train a strong model. For control, we unify spatiotemporal control via a single mask mechanism and support optional reference images. Overall, EasyV2V works with flexible inputs, e.g., video+text, video+mask+text, video+mask+reference+text, and achieves state-of-the-art video editing results, surpassing concurrent and commercial systems. Project page: https://snap-research.github.io/easyv2v/

</details>


### [19] [Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification](https://arxiv.org/abs/2512.16921)
*Qihao Liu,Chengzhi Mao,Yaojie Liu,Alan Yuille,Wen-Sheng Chu*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Conventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing their divergence. AuditDM fine-tunes an MLLM as an auditor via reinforcement learning to generate challenging questions and counterfactual images that maximize disagreement among target models. Once trained, the auditor uncovers diverse, interpretable exemplars that reveal model weaknesses and serve as annotation-free data for rectification. When applied to SoTA models like Gemma-3 and PaliGemma-2, AuditDM discovers more than 20 distinct failure types. Fine-tuning on these discoveries consistently improves all models across 16 benchmarks, and enables a 3B model to surpass its 28B counterpart. Our results suggest that as data scaling hits diminishing returns, targeted model auditing offers an effective path to model diagnosis and improvement.

</details>


### [20] [Generative Refocusing: Flexible Defocus Control from a Single Image](https://arxiv.org/abs/2512.16923)
*Chun-Wei Tuan Mu,Jia-Bin Huang,Yu-Lun Liu*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Depth-of-field control is essential in photography, but getting the perfect focus often takes several tries or special equipment. Single-image refocusing is still difficult. It involves recovering sharp content and creating realistic bokeh. Current methods have significant drawbacks. They need all-in-focus inputs, depend on synthetic data from simulators, and have limited control over aperture. We introduce Generative Refocusing, a two-step process that uses DeblurNet to recover all-in-focus images from various inputs and BokehNet for creating controllable bokeh. Our main innovation is semi-supervised training. This method combines synthetic paired data with unpaired real bokeh images, using EXIF metadata to capture real optical characteristics beyond what simulators can provide. Our experiments show we achieve top performance in defocus deblurring, bokeh synthesis, and refocusing benchmarks. Additionally, our Generative Refocusing allows text-guided adjustments and custom aperture shapes.

</details>


### [21] [The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text](https://arxiv.org/abs/2512.16924)
*Hanlin Wang,Hao Ouyang,Qiuyu Wang,Yue Yu,Yihao Meng,Wen Wang,Ka Leong Cheng,Shuailei Ma,Qingyan Bai,Yixuan Li,Cheng Chen,Yanhong Zeng,Xing Zhu,Yujun Shen,Qifeng Chen*

Main category: cs.CV

TL;DR: WorldCanvas 是一个多模态框架，通过结合文本、轨迹和参考图像来生成用户可提示的世界事件视频，支持复杂交互和对象一致性控制。


<details>
  <summary>Details</summary>
Motivation: 现有的文本生成视频方法或基于轨迹控制的图像到视频方法存在局限性，无法充分表达复杂的多智能体交互、对象进出场景、参考图像引导的外观以及反直觉事件，需要更丰富的用户可控模拟框架。

Method: 采用多模态方法，结合轨迹（编码运动、时间和可见性）、自然语言（语义意图）和参考图像（对象身份视觉基础），生成具有时间一致性和对象身份保持的连贯视频事件。

Result: 生成的视频不仅具有时间一致性，还表现出涌现一致性，能在对象暂时消失后保持身份和场景连续性，支持多智能体交互、对象进出、参考引导外观和反直觉事件。

Conclusion: WorldCanvas 通过支持表达性世界事件生成，将世界模型从被动预测器推进为交互式、用户可塑造的模拟器，为可控视频生成提供了新的框架。

Abstract: We present WorldCanvas, a framework for promptable world events that enables rich, user-directed simulation by combining text, trajectories, and reference images. Unlike text-only approaches and existing trajectory-controlled image-to-video methods, our multimodal approach combines trajectories -- encoding motion, timing, and visibility -- with natural language for semantic intent and reference images for visual grounding of object identity, enabling the generation of coherent, controllable events that include multi-agent interactions, object entry/exit, reference-guided appearance and counterintuitive events. The resulting videos demonstrate not only temporal coherence but also emergent consistency, preserving object identity and scene despite temporary disappearance. By supporting expressive world events generation, WorldCanvas advances world models from passive predictors to interactive, user-shaped simulators. Our project page is available at: https://worldcanvas.github.io/.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [22] [Grammar-Forced Translation of Natural Language to Temporal Logic using LLMs](https://arxiv.org/abs/2512.16814)
*William English,Dominic Simon,Sumit Kumar Jha,Rickard Ewetz*

Main category: cs.CL

TL;DR: 本文提出了一种名为Grammar Forced Translation (GraFT)的新框架，用于将自然语言翻译为时序逻辑公式。该框架通过约束每一步的有效输出词集合来降低任务复杂度，从而显著提升翻译准确率。


<details>
  <summary>Details</summary>
Motivation: 当前的时序逻辑翻译方法存在三个主要问题：原子命题提取不准确、无法处理共指现象、难以从有限数据中学习。现有方法通过语言模型在全词表范围内迭代预测的方式解决这两个步骤，复杂度较高。

Method: GraFT框架利用两个子任务的独特性质来减少解空间：1）原子命题提取时约束为自然语言表达中的概念；2）翻译阶段结合文法规则和先前预测的词来限制有效输出词集合。这种解空间缩减使模型学习更高效。

Result: 在CW、GLTL和Navi三个基准测试中，GraFT相比现有最佳方法，端到端翻译准确率平均提升5.49%，领域外翻译准确率平均提升14.06%，表现出显著的性能提升。

Conclusion: GraFT通过利用任务特定属性约束解空间，有效降低了时序逻辑翻译的复杂度，在准确率和泛化能力方面都明显优于现有方法，为解决自然语言到形式化语言的翻译问题提供了新思路。

Abstract: Translating natural language (NL) into a formal language such as temporal logic (TL) is integral for human communication with robots and autonomous systems. State-of-the-art approaches decompose the task into a lifting of atomic propositions (APs) phase and a translation phase. However, existing methods struggle with accurate lifting, the existence of co-references, and learning from limited data. In this paper, we propose a framework for NL to TL translation called Grammar Forced Translation (GraFT). The framework is based on the observation that previous work solves both the lifting and translation steps by letting a language model iteratively predict tokens from its full vocabulary. In contrast, GraFT reduces the complexity of both tasks by restricting the set of valid output tokens from the full vocabulary to only a handful in each step. The solution space reduction is obtained by exploiting the unique properties of each problem. We also provide a theoretical justification for why the solution space reduction leads to more efficient learning. We evaluate the effectiveness of GraFT using the CW, GLTL, and Navi benchmarks. Compared with state-of-the-art translation approaches, it can be observed that GraFT the end-to-end translation accuracy by 5.49% and out-of-domain translation accuracy by 14.06% on average.

</details>


### [23] [What Do Prosody and Text Convey? Characterizing How Meaningful Information is Distributed Across Multiple Channels](https://arxiv.org/abs/2512.16832)
*Aditya Yadavalli,Tiago Pimentel,Tamar I Regev,Ethan Wilcox,Alex Warstadt*

Main category: cs.CL

TL;DR: 提出一种基于信息论的方法量化仅由韵律传递的信息量及信息内容，发现音频在传达讽刺和情感信息上比文本重要得多。


<details>
  <summary>Details</summary>
Motivation: 韵律通常传达文本未捕获的关键信息，急需量化韵律单独表达的信息量及其具体内容。

Method: 利用大型语音和语言模型估计话语意义维度（如情绪）与其通讯通道（如音频或文本）之间的互信息。

Result: 对于讽刺和情感，音频通道（即韵律）传递的信息量比文本通道高一个数量级以上；对于疑问句，韵律提供的额外信息相对较少。

Conclusion: 展望未来将本方法应用于更多意义维度、通讯通道和语言的研究计划。

Abstract: Prosody -- the melody of speech -- conveys critical information often not captured by the words or text of a message. In this paper, we propose an information-theoretic approach to quantify how much information is expressed by prosody alone and not by text, and crucially, what that information is about. Our approach applies large speech and language models to estimate the mutual information between a particular dimension of an utterance's meaning (e.g., its emotion) and any of its communication channels (e.g., audio or text). We then use this approach to quantify how much information is conveyed by audio and text about sarcasm, emotion, and questionhood, using speech from television and podcasts. We find that for sarcasm and emotion the audio channel -- and by implication the prosodic channel -- transmits over an order of magnitude more information about these features than the text channel alone, at least when long-term context beyond the current sentence is unavailable. For questionhood, prosody provides comparatively less additional information. We conclude by outlining a program applying our approach to more dimensions of meaning, communication channels, and languages.

</details>


### [24] [LLMCache: Layer-Wise Caching Strategies for Accelerated Reuse in Transformer Inference](https://arxiv.org/abs/2512.16843)
*Harsh Vardhan Bansal*

Main category: cs.CL

TL;DR: LLMCache论文摘要：介绍了LLMCache，一种新颖的层级缓存框架，用于加速Transformer模型推理。


<details>
  <summary>Details</summary>
Motivation: Transformer模型推理延迟较高，现有缓存机制（如token级KV缓存）适用范围有限，不能满足实时、大规模部署需求。

Method: 提出了LLMCache——基于语义相似性重用中间激活值的层级缓存框架，包含轻量级指纹机制用于相似输入匹配，采用自适应淘汰策略管理缓存陈旧度，支持模型无关、跨编码器-解码器架构、任意Transformer层缓存。

Result: 在BERT和GPT-2上针对SQuAD、WikiText-103和OpenBookQA进行实验，推理时间加速最高达3.1倍，准确率损失小于0.5%。

Conclusion: LLMCache是实际且通用的Transformer推理优化方案，适用于真实应用场景。

Abstract: Transformer-based language models have achieved remarkable performance across a wide range of tasks, yet their high inference latency poses a significant challenge for real-timeand large-scale deployment. While existing caching mechanisms,such as token-level key-value caches, offer speedups in autore-gressive decoding, they are limited in scope and applicability. In this paper, we present LLMCache, a novel layer-wise caching framework that accelerates transformer inference by reusing intermediate activations based on semantic similarity of input sequences. Unlike prior work, LLMCache is model-agnostic,operates across both encoder and decoder architectures, and supports caching at arbitrary transformer layers. We introduce a lightweight fingerprinting mechanism for matching seman-tically similar inputs and propose adaptive eviction strategies to manage cache staleness. Experiments on BERT and GPT-2 across SQuAD, WikiText-103, and OpenBookQA show up to 3.1 X speedup in inference time with <0.5% accuracy degradation. Our results highlight LLMCache as a practical and general-purpose solution for optimizing transformer inference in real-world applications

</details>


### [25] [Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image](https://arxiv.org/abs/2512.16899)
*Yushi Hu,Reyhane Askari-Hemmat,Melissa Hall,Emily Dinan,Luke Zettlemoyer,Marjan Ghazvininejad*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Reward models (RMs) are essential for training large language models (LLMs), but remain underexplored for omni models that handle interleaved image and text sequences. We introduce Multimodal RewardBench 2 (MMRB2), the first comprehensive benchmark for reward models on multimodal understanding and (interleaved) generation. MMRB2 spans four tasks: text-to-image, image editing, interleaved generation, and multimodal reasoning ("thinking-with-images"), providing 1,000 expert-annotated preference pairs per task from 23 models and agents across 21 source tasks. MMRB2 is designed with: (1) practical but challenging prompts; (2) responses from state-of-the-art models and agents; and (3) preference pairs with strong human-expert consensus, curated via an ensemble filtering strategy. Using MMRB2, we study existing judges for each subtask, including multimodal LLM-as-a-judge and models trained with human preferences. The latest Gemini 3 Pro attains 75-80% accuracy. GPT-5 and Gemini 2.5 Pro reach 66-75% accuracy, compared to >90% for humans, yet surpass the widely used GPT-4o (59%). The best performing open-source model Qwen3-VL-32B achieves similar accuracies as Gemini 2.5 Flash (64%). We also show that MMRB2 performance strongly correlates with downstream task success using Best-of-N sampling and conduct an in-depth analysis that shows key areas to improve the reward models going forward.

</details>


### [26] [In-Context Algebra](https://arxiv.org/abs/2512.16902)
*Eric Todd,Jannik Brinkmann,Rohit Gandikota,David Bau*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We investigate the mechanisms that arise when transformers are trained to solve arithmetic on sequences where tokens are variables whose meaning is determined only through their interactions. While prior work has found that transformers develop geometric embeddings that mirror algebraic structure, those previous findings emerge from settings where arithmetic-valued tokens have fixed meanings. We devise a new task in which the assignment of symbols to specific algebraic group elements varies from one sequence to another. Despite this challenging setup, transformers achieve near-perfect accuracy on the task and even generalize to unseen algebraic groups. We develop targeted data distributions to create causal tests of a set of hypothesized mechanisms, and we isolate three mechanisms models consistently learn: commutative copying where a dedicated head copies answers, identity element recognition that distinguishes identity-containing facts, and closure-based cancellation that tracks group membership to constrain valid answers. Complementary to the geometric representations found in fixed-symbol settings, our findings show that models develop symbolic reasoning mechanisms when trained to reason in-context with variables whose meanings are not fixed.

</details>


### [27] [Constructive Circuit Amplification: Improving Math Reasoning in LLMs via Targeted Sub-Network Updates](https://arxiv.org/abs/2512.16914)
*Nikhil Prakash,Donghao Ren,Dominik Moritz,Yannick Assogba*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Prior studies investigating the internal workings of LLMs have uncovered sparse subnetworks, often referred to as circuits, that are responsible for performing specific tasks. Additionally, it has been shown that model performance improvement through fine-tuning often results from the strengthening of existing circuits in the model. Taken together, these findings suggest the possibility of intervening directly on such circuits to make precise, task-targeted updates. Motivated by these findings, we propose a novel method called Constructive Circuit Amplification which identifies pivotal tokens from model reasoning traces as well as model components responsible for the desired task, and updates only those components. Applied to mathematical reasoning, it improves accuracy by up to +11.4% across multiple models while modifying as little as 1.59% of model components, with minimal impact on other abilities as measured by MMLU, TriviaQA, and TruthfulQA. These results demonstrate that targeted capabilities can be reliably enhanced by selectively updating a sparse set of model components.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [28] [Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning](https://arxiv.org/abs/2512.16917)
*Qihao Liu,Luoxin Ye,Wufei Ma,Yu-Cheng Chou,Alan Yuille*

Main category: cs.AI

TL;DR: 本文提出了一种名为Generative Adversarial Reasoner的联合训练框架，通过对抗性强化学习同时优化LLM推理器和判别器，以改进数学推理过程中的逻辑一致性，减少计算错误。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在数学推理任务中虽然展现出明确的推理能力，但仍存在过程性错误，如不正确计算、脆弱的逻辑和表面合理但无效的推理步骤。当前需要一种方法能密集、高效地提供步骤级奖励信号，以增强推理过程的稳健性和准确性。

Method: 采用生成对抗推理框架，包含一个LLM推理器和一个LLM判别器，通过对抗性强化学习协同进化。计算高效的审阅计划将推理链划分为逻辑完整的切片；判别器评估每个切片的合理性，并提供简洁的结构化理由。学习过程结合互补信号：推理器因逻辑一致的步骤和正确答案获得奖励，判别器则因正确检测错误或区分推理轨迹而获得奖励。该方法产出密集、校准良好的步骤级奖励，补充稀疏的精确匹配信号。

Result: 在多个数学基准测试中，该方法相比标准强化学习后训练基线取得一致提升。具体而言，在AIME24上，将DeepSeek-R1-Distill-Qwen-7B从54.0提升至61.3（+7.3），将DeepSeek-R1-Distill-Llama-8B从43.7提升至53.7（+10.0）。模块化的判别器还支持灵活的奖励塑造，适用于教师蒸馏、偏好对齐和基于数学证明的推理等目标。

Conclusion: Generative Adversarial Reasoner框架通过对抗性强化学习联合训练推理器和判别器，有效提升了LLMs在数学推理中的逻辑一致性和准确性，提供了密集的步骤级奖励，增强了信用分配和样本效率，并在多个基准测试中取得了显著性能提升。模块化判别器设计还扩展了其在多种推理相关任务中的应用潜力。

Abstract: Large language models (LLMs) with explicit reasoning capabilities excel at mathematical reasoning yet still commit process errors, such as incorrect calculations, brittle logic, and superficially plausible but invalid steps. In this paper, we introduce Generative Adversarial Reasoner, an on-policy joint training framework designed to enhance reasoning by co-evolving an LLM reasoner and an LLM-based discriminator through adversarial reinforcement learning. A compute-efficient review schedule partitions each reasoning chain into logically complete slices of comparable length, and the discriminator evaluates each slice's soundness with concise, structured justifications. Learning couples complementary signals: the LLM reasoner is rewarded for logically consistent steps that yield correct answers, while the discriminator earns rewards for correctly detecting errors or distinguishing traces in the reasoning process. This produces dense, well-calibrated, on-policy step-level rewards that supplement sparse exact-match signals, improving credit assignment, increasing sample efficiency, and enhancing overall reasoning quality of LLMs. Across various mathematical benchmarks, the method delivers consistent gains over strong baselines with standard RL post-training. Specifically, on AIME24, we improve DeepSeek-R1-Distill-Qwen-7B from 54.0 to 61.3 (+7.3) and DeepSeek-R1-Distill-Llama-8B from 43.7 to 53.7 (+10.0). The modular discriminator also enables flexible reward shaping for objectives such as teacher distillation, preference alignment, and mathematical proof-based reasoning.

</details>


### [29] [TOGGLE: Temporal Logic-Guided Large Language Model Compression for Edge](https://arxiv.org/abs/2512.16855)
*Khurram Khalil,Khaza Anuarul Hoque*

Main category: cs.AI

TL;DR: TOGGLE是一个基于时序逻辑的LLM压缩框架，首次将形式化方法引入大语言模型压缩，通过形式化规范和执行语言属性约束，显著减少计算成本和模型大小。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽性能优越，但计算需求大，难以部署到资源受限的边缘设备。现有压缩技术往往损害语言特性，且缺乏保证模型行为的形式化方法。

Method: 利用信号时序逻辑（STL）形式化规范语言属性，通过STL鲁棒性指导的贝叶斯优化，系统探索分层量化和剪枝配置，生成满足约束的压缩模型，无需重训练或微调。

Result: 在四个LLM架构（GPT-2、DeepSeek-V2 7B、LLaMA 3 8B、Mistral 7B）上测试，实现高达3.3倍计算成本（FLOPs）降低和68.8%模型大小减少，同时满足所有语言属性约束。

Conclusion: TOGGLE是首个将形式化方法整合到LLM压缩中的框架，为LLM在边缘硬件上的高效、可验证部署提供了新途径。

Abstract: Large Language Models (LLMs) deliver exceptional performance across natural language tasks but demand substantial computational resources, limiting their deployment on resource-constrained edge devices. Existing compression techniques, such as quantization and pruning, often degrade critical linguistic properties and lack formal guarantees for preserving model behavior. We propose Temporal Logic-Guided Large Language Model Compression (TOGGLE), a novel framework that leverages Signal Temporal Logic (STL) to formally specify and enforce linguistic properties during compression. TOGGLE employs an STL robustness-guided Bayesian optimization to systematically explore layer-wise quantization and pruning configurations, generating compressed models that formally satisfy specified linguistic constraints without retraining or fine-tuning. Evaluating TOGGLE on four LLM architectures (GPT-2, DeepSeek-V2 7B, LLaMA 3 8B, and Mistral 7B), we achieve up to 3.3x reduction in computational costs (FLOPs) and up to a 68.8% reduction in model size while satisfying all linguistic properties. TOGGLE represents the first integration of formal methods into LLM compression, enabling efficient, verifiable deployment of LLMs on edge hardware.

</details>


### [30] [The Social Responsibility Stack: A Control-Theoretic Architecture for Governing Socio-Technical AI](https://arxiv.org/abs/2512.16873)
*Otman A. Basir*

Main category: cs.AI

TL;DR: 论文提出了一个名为"社会责任堆栈（SRS）"的六层架构框架，将社会价值作为明确约束、安全措施、行为接口、审计机制和治理过程嵌入AI系统，为可问责、可适应和可审计的社会技术AI系统提供实践基础。


<details>
  <summary>Details</summary>
Motivation: 当前人工智能系统越来越多地部署在影响人类行为、机构决策和社会结果的领域，而现有的负责任AI和治理努力虽然提供了重要的规范性原则，但往往缺乏在整个系统生命周期内可执行的工程机制。

Method: 开发了SRS框架，它将责任建模为社会技术系统的闭环监督控制问题，整合了设计时安全措施、运行时监控和机构监督。提出了统一的基于约束的表述，引入了安全包络和反馈解释，展示了如何持续监控和执行公平性、自主性、认知负担和解释质量。

Result: 通过临床决策支持、协作自动驾驶汽车和公共部门系统的案例研究，展示了SRS如何将规范性目标转化为可操作的工程和运营控制。

Conclusion: SRS框架在伦理学、控制理论和AI治理之间架起了桥梁，为负责、适应性强且可审计的社会技术AI系统提供了实用的基础。

Abstract: Artificial intelligence systems are increasingly deployed in domains that shape human behaviour, institutional decision-making, and societal outcomes. Existing responsible AI and governance efforts provide important normative principles but often lack enforceable engineering mechanisms that operate throughout the system lifecycle. This paper introduces the Social Responsibility Stack (SRS), a six-layer architectural framework that embeds societal values into AI systems as explicit constraints, safeguards, behavioural interfaces, auditing mechanisms, and governance processes. SRS models responsibility as a closed-loop supervisory control problem over socio-technical systems, integrating design-time safeguards with runtime monitoring and institutional oversight. We develop a unified constraint-based formulation, introduce safety-envelope and feedback interpretations, and show how fairness, autonomy, cognitive burden, and explanation quality can be continuously monitored and enforced. Case studies in clinical decision support, cooperative autonomous vehicles, and public-sector systems illustrate how SRS translates normative objectives into actionable engineering and operational controls. The framework bridges ethics, control theory, and AI governance, providing a practical foundation for accountable, adaptive, and auditable socio-technical AI systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [31] [Training Together, Diagnosing Better: Federated Learning for Collagen VI-Related Dystrophies](https://arxiv.org/abs/2512.16876)
*Astrid Brull,Sara Aguti,Véronique Bolduc,Ying Hu,Daniel M. Jimenez-Gutierrez,Enrique Zuazua,Joaquin Del-Rio,Oleksii Sliusarenko,Haiyan Zhou,Francesco Muntoni,Carsten G. Bönnemann,Xabi Uribe-Etxebarria*

Main category: cs.LG

TL;DR: 该研究提出了一种基于联邦学习（FL）的新方法，用于解决罕见病诊断中数据稀缺和隐私问题。通过跨国际组织的分布式数据集合作训练ML模型，成功提升了胶原VI相关肌营养不良（COL6-RD）的诊断准确率。


<details>
  <summary>Details</summary>
Motivation: 罕见病（如COL6-RD）的诊断受限于数据稀缺和分散。跨机构数据共享面临隐私、监管和物流障碍。联邦学习（FL）提供了在保持数据本地化和隐私的同时进行协作训练的解决方案。

Method: 使用Sherpa.ai FL平台，在两个国际组织的分布式数据集上实施联邦学习。利用COL6-RD患者成纤维细胞培养物的胶原VI免疫荧光显微镜图像，训练ML模型以分类三种主要致病机制组（外显子跳跃、甘氨酸替换、假外显子插入）。

Result: 该方法在三个致病机制组的分类中实现了0.82的F1分数，显著优于单机构模型（0.57-0.75）。FL方法提高了诊断效用和泛化能力，并可能支持对意义不明确变异的解释和测序策略的优化。

Conclusion: 联邦学习能够有效克服罕见病诊断中的数据壁垒，提升模型的准确性和泛化性。该方法不仅有助于更精确的诊断，还可能推动对未知致病性变异的识别和测序策略的优化。

Abstract: The application of Machine Learning (ML) to the diagnosis of rare diseases, such as collagen VI-related dystrophies (COL6-RD), is fundamentally limited by the scarcity and fragmentation of available data. Attempts to expand sampling across hospitals, institutions, or countries with differing regulations face severe privacy, regulatory, and logistical obstacles that are often difficult to overcome. The Federated Learning (FL) provides a promising solution by enabling collaborative model training across decentralized datasets while keeping patient data local and private. Here, we report a novel global FL initiative using the Sherpa.ai FL platform, which leverages FL across distributed datasets in two international organizations for the diagnosis of COL6-RD, using collagen VI immunofluorescence microscopy images from patient-derived fibroblast cultures. Our solution resulted in an ML model capable of classifying collagen VI patient images into the three primary pathogenic mechanism groups associated with COL6-RD: exon skipping, glycine substitution, and pseudoexon insertion. This new approach achieved an F1-score of 0.82, outperforming single-organization models (0.57-0.75). These results demonstrate that FL substantially improves diagnostic utility and generalizability compared to isolated institutional models. Beyond enabling more accurate diagnosis, we anticipate that this approach will support the interpretation of variants of uncertain significance and guide the prioritization of sequencing strategies to identify novel pathogenic variants.

</details>
